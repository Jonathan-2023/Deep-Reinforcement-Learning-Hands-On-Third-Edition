{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6fb2029",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import ptan\n",
    "import ptan.ignite as ptan_ignite\n",
    "from datetime import datetime, timedelta\n",
    "import argparse\n",
    "import random\n",
    "import warnings\n",
    "import typing as tt\n",
    "from dataclasses import dataclass\n",
    "import torch.multiprocessing as mp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6293fb15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95f6f067",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ignite.engine import Engine\n",
    "from ignite.metrics import RunningAverage\n",
    "from ignite.contrib.handlers import tensorboard_logger as tb_logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5910d68d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib import dqn_model, common, atari_wrappers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a29b23b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "NAME = \"04_wrappers_parallel\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e62db655",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class EpisodeEnded:\n",
    "    reward: float\n",
    "    steps: int\n",
    "    epsilon: float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b24c468",
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_func(params: common.Hyperparams, net: dqn_model.DQN,\n",
    "              dev_name: str, exp_queue: mp.Queue, n_frames: int):\n",
    "    env = gym.make(params.env_name)\n",
    "    env = atari_wrappers.wrap_dqn(env, stack_frames=n_frames)\n",
    "    device = torch.device(dev_name)\n",
    "\n",
    "    selector = ptan.actions.EpsilonGreedyActionSelector(\n",
    "        epsilon=params.epsilon_start)\n",
    "    epsilon_tracker = common.EpsilonTracker(selector, params)\n",
    "    agent = ptan.agent.DQNAgent(net, selector, device=device)\n",
    "    exp_source = ptan.experience.ExperienceSourceFirstLast(\n",
    "        env, agent, gamma=params.gamma, env_seed=common.SEED)\n",
    "\n",
    "    for frame_idx, exp in enumerate(exp_source):\n",
    "        epsilon_tracker.frame(frame_idx//2)\n",
    "        exp_queue.put(exp)\n",
    "        for reward, steps in exp_source.pop_rewards_steps():\n",
    "            ee = EpisodeEnded(\n",
    "                reward=reward, steps=steps,\n",
    "                epsilon=selector.epsilon\n",
    "            )\n",
    "            exp_queue.put(ee)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e201763",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchGenerator:\n",
    "    def __init__(self, buffer_size: int,\n",
    "                 exp_queue: mp.Queue,\n",
    "                 fps_handler: ptan_ignite.EpisodeFPSHandler,\n",
    "                 initial: int, batch_size: int):\n",
    "        self.buffer = ptan.experience.ExperienceReplayBuffer(\n",
    "            experience_source=None, buffer_size=buffer_size)\n",
    "        self.exp_queue = exp_queue\n",
    "        self.fps_handler = fps_handler\n",
    "        self.initial = initial\n",
    "        self.batch_size = batch_size\n",
    "        self._rewards_steps = []\n",
    "        self.epsilon = None\n",
    "\n",
    "    def pop_rewards_steps(self) -> tt.List[tt.Tuple[float, int]]:\n",
    "        res = list(self._rewards_steps)\n",
    "        self._rewards_steps.clear()\n",
    "        return res\n",
    "\n",
    "    def __iter__(self):\n",
    "        while True:\n",
    "            while self.exp_queue.qsize() > 0:\n",
    "                exp = self.exp_queue.get()\n",
    "                if isinstance(exp, EpisodeEnded):\n",
    "                    self._rewards_steps.append(\n",
    "                        (exp.reward, exp.steps))\n",
    "                    self.epsilon = exp.epsilon\n",
    "                else:\n",
    "                    self.buffer._add(exp)\n",
    "                    self.fps_handler.step()\n",
    "            if len(self.buffer) < self.initial:\n",
    "                continue\n",
    "            yield self.buffer.sample(self.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f3ea57a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # get rid of missing metrics warning\n",
    "    warnings.simplefilter(\"ignore\", category=UserWarning)\n",
    "    mp.set_start_method('spawn')\n",
    "\n",
    "    random.seed(common.SEED)\n",
    "    torch.manual_seed(common.SEED)\n",
    "    params = common.GAME_PARAMS['pong']\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--dev\", default=\"cpu\",\n",
    "                        help=\"Device to use, default=cpu\")\n",
    "    parser.add_argument(\"--stack\", type=int, default=2,\n",
    "                        help=\"Number of stacked frames\")\n",
    "    args = parser.parse_args()\n",
    "    device = torch.device(args.dev)\n",
    "\n",
    "    env = gym.make(params.env_name)\n",
    "    env = atari_wrappers.wrap_dqn(env, stack_frames=args.stack)\n",
    "\n",
    "    net = dqn_model.DQN(env.observation_space.shape,\n",
    "                        env.action_space.n).to(device)\n",
    "\n",
    "    tgt_net = ptan.agent.TargetNet(net)\n",
    "    optimizer = optim.Adam(net.parameters(), lr=params.learning_rate)\n",
    "\n",
    "    # start subprocess and experience queue\n",
    "    exp_queue = mp.Queue(maxsize=2)\n",
    "    proc_args = (params, net, args.dev, exp_queue, args.stack)\n",
    "    play_proc = mp.Process(target=play_func, args=proc_args)\n",
    "    play_proc.start()\n",
    "    fps_handler = ptan_ignite.EpisodeFPSHandler()\n",
    "    batch_generator = BatchGenerator(\n",
    "        params.replay_size, exp_queue, fps_handler,\n",
    "        params.replay_initial, params.batch_size)\n",
    "\n",
    "    def process_batch(engine, batch):\n",
    "        optimizer.zero_grad()\n",
    "        loss_v = common.calc_loss_dqn(batch, net, tgt_net.target_model,\n",
    "                                      gamma=params.gamma, device=device)\n",
    "        loss_v.backward()\n",
    "        optimizer.step()\n",
    "        if engine.state.iteration % params.target_net_sync == 0:\n",
    "            tgt_net.sync()\n",
    "        return {\n",
    "            \"loss\": loss_v.item(),\n",
    "            \"epsilon\": batch_generator.epsilon,\n",
    "        }\n",
    "\n",
    "    engine = Engine(process_batch)\n",
    "    ptan_ignite.EndOfEpisodeHandler(batch_generator, bound_avg_reward=18.0).attach(engine)\n",
    "    ptan_ignite.EpisodeFPSHandler().attach(engine)\n",
    "\n",
    "    @engine.on(ptan_ignite.EpisodeEvents.EPISODE_COMPLETED)\n",
    "    def episode_completed(trainer: Engine):\n",
    "        print(\"Episode %d: reward=%s, steps=%s, speed=%.3f frames/s, elapsed=%s\" % (\n",
    "            trainer.state.episode, trainer.state.episode_reward,\n",
    "            trainer.state.episode_steps, trainer.state.metrics.get('avg_fps', 0),\n",
    "            timedelta(seconds=trainer.state.metrics.get('time_passed', 0))))\n",
    "        trainer.should_terminate = trainer.state.episode > 700\n",
    "\n",
    "    @engine.on(ptan_ignite.EpisodeEvents.BOUND_REWARD_REACHED)\n",
    "    def game_solved(trainer: Engine):\n",
    "        print(\"Game solved in %s, after %d episodes and %d iterations!\" % (\n",
    "            timedelta(seconds=trainer.state.metrics['time_passed']),\n",
    "            trainer.state.episode, trainer.state.iteration))\n",
    "        trainer.should_terminate = True\n",
    "\n",
    "    logdir = f\"runs/{datetime.now().isoformat(timespec='minutes')}-{params.run_name}-{NAME}-stack={args.stack}\"\n",
    "    tb = tb_logger.TensorboardLogger(log_dir=logdir)\n",
    "    RunningAverage(output_transform=lambda v: v['loss']).attach(engine, \"avg_loss\")\n",
    "\n",
    "    episode_handler = tb_logger.OutputHandler(tag=\"episodes\", metric_names=['reward', 'steps', 'avg_reward'])\n",
    "    tb.attach(engine, log_handler=episode_handler, event_name=ptan_ignite.EpisodeEvents.EPISODE_COMPLETED)\n",
    "\n",
    "    # write to tensorboard every 100 iterations\n",
    "    ptan_ignite.PeriodicEvents().attach(engine)\n",
    "    handler = tb_logger.OutputHandler(tag=\"train\", metric_names=['avg_loss', 'avg_fps'],\n",
    "                                      output_transform=lambda a: a)\n",
    "    tb.attach(engine, log_handler=handler, event_name=ptan_ignite.PeriodEvents.ITERS_100_COMPLETED)\n",
    "\n",
    "    try:\n",
    "        engine.run(batch_generator)\n",
    "    finally:\n",
    "        play_proc.kill()\n",
    "        play_proc.join()"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "executable": "/usr/bin/env python3",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
