{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d1c2b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import ptan\n",
    "import ptan.ignite as ptan_ignite\n",
    "from datetime import datetime, timedelta\n",
    "import argparse\n",
    "import random\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b550fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "990050a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ignite.engine import Engine\n",
    "from ignite.metrics import RunningAverage\n",
    "from ignite.contrib.handlers import tensorboard_logger as tb_logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c931ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib import dqn_model, common"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7993b8fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "NAME = \"00_slow_grads\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd7cb2f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent(ptan.agent.BaseAgent):\n",
    "    \"\"\"\n",
    "    DQNAgent is a memoryless DQN agent which calculates Q values\n",
    "    from the observations and  converts them into the actions using action_selector\n",
    "    \"\"\"\n",
    "    def __init__(self, dqn_model, action_selector, device:torch.device,\n",
    "                 preprocessor=ptan.agent.default_states_preprocessor):\n",
    "        self.dqn_model = dqn_model\n",
    "        self.action_selector = action_selector\n",
    "        self.preprocessor = preprocessor\n",
    "        self.device = device\n",
    "\n",
    "    def __call__(self, states, agent_states=None):\n",
    "        if agent_states is None:\n",
    "            agent_states = [None] * len(states)\n",
    "        if self.preprocessor is not None:\n",
    "            states = self.preprocessor(states)\n",
    "            if torch.is_tensor(states):\n",
    "                states = states.to(self.device)\n",
    "        q_v = self.dqn_model(states)\n",
    "        q = q_v.data.cpu().numpy()\n",
    "        actions = self.action_selector(q)\n",
    "        return actions, agent_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffb947bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_loss_dqn(batch, net, tgt_net, gamma, device: torch.device, cuda_async=False):\n",
    "    states, actions, rewards, dones, next_states = common.unpack_batch(batch)\n",
    "\n",
    "    states_v = torch.tensor(states).to(device, non_blocking=cuda_async)\n",
    "    next_states_v = torch.tensor(next_states).to(device, non_blocking=cuda_async)\n",
    "    actions_v = torch.tensor(actions).to(device, non_blocking=cuda_async)\n",
    "    rewards_v = torch.tensor(rewards).to(device, non_blocking=cuda_async)\n",
    "    done_mask = torch.BoolTensor(dones).to(device, non_blocking=cuda_async)\n",
    "\n",
    "    state_action_values = net(states_v).gather(1, actions_v.unsqueeze(-1)).squeeze(-1)\n",
    "    next_state_values = tgt_net(next_states_v).max(1)[0]\n",
    "    next_state_values[done_mask] = 0.0\n",
    "\n",
    "    expected_state_action_values = next_state_values.detach() * gamma + rewards_v\n",
    "    return nn.MSELoss()(state_action_values, expected_state_action_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24409af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # get rid of missing metrics warning\n",
    "    warnings.simplefilter(\"ignore\", category=UserWarning)\n",
    "\n",
    "    random.seed(common.SEED)\n",
    "    torch.manual_seed(common.SEED)\n",
    "    params = common.GAME_PARAMS['pong']\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\n",
    "        \"--dev\", default=\"cpu\", help=\"Device to use, default=cpu\")\n",
    "    args = parser.parse_args()\n",
    "    device = torch.device(args.dev)\n",
    "\n",
    "    env = gym.make(params.env_name)\n",
    "    env = ptan.common.wrappers.wrap_dqn(env)\n",
    "\n",
    "    net = dqn_model.DQN(env.observation_space.shape, env.action_space.n).to(device)\n",
    "\n",
    "    tgt_net = ptan.agent.TargetNet(net)\n",
    "    selector = ptan.actions.EpsilonGreedyActionSelector(epsilon=params.epsilon_start)\n",
    "    epsilon_tracker = common.EpsilonTracker(selector, params)\n",
    "    agent = DQNAgent(net, selector, device=device)\n",
    "\n",
    "    exp_source = ptan.experience.ExperienceSourceFirstLast(\n",
    "        env, agent, gamma=params.gamma, steps_count=1,\n",
    "        env_seed=common.SEED)\n",
    "    buffer = ptan.experience.ExperienceReplayBuffer(\n",
    "        exp_source, buffer_size=params.replay_size)\n",
    "    optimizer = optim.Adam(net.parameters(), lr=params.learning_rate)\n",
    "\n",
    "    def process_batch(engine, batch):\n",
    "        optimizer.zero_grad()\n",
    "        loss_v = calc_loss_dqn(batch, net, tgt_net.target_model,\n",
    "                               gamma=params.gamma, device=device)\n",
    "        loss_v.backward()\n",
    "        optimizer.step()\n",
    "        epsilon_tracker.frame(engine.state.iteration)\n",
    "        if engine.state.iteration % params.target_net_sync == 0:\n",
    "            tgt_net.sync()\n",
    "        return {\n",
    "            \"loss\": loss_v.item(),\n",
    "            \"epsilon\": selector.epsilon,\n",
    "        }\n",
    "\n",
    "    engine = Engine(process_batch)\n",
    "    ptan_ignite.EndOfEpisodeHandler(exp_source, bound_avg_reward=18.0).attach(engine)\n",
    "    ptan_ignite.EpisodeFPSHandler().attach(engine)\n",
    "\n",
    "    @engine.on(ptan_ignite.EpisodeEvents.EPISODE_COMPLETED)\n",
    "    def episode_completed(trainer: Engine):\n",
    "        print(\"Episode %d: reward=%s, steps=%s, speed=%.3f frames/s, elapsed=%s\" % (\n",
    "            trainer.state.episode, trainer.state.episode_reward,\n",
    "            trainer.state.episode_steps, trainer.state.metrics.get('fps', 0),\n",
    "            timedelta(seconds=trainer.state.metrics.get('time_passed', 0))))\n",
    "        trainer.should_terminate = trainer.state.episode > 700\n",
    "\n",
    "    @engine.on(ptan_ignite.EpisodeEvents.BOUND_REWARD_REACHED)\n",
    "    def game_solved(trainer: Engine):\n",
    "        print(\"Game solved in %s, after %d episodes and %d iterations!\" % (\n",
    "            timedelta(seconds=trainer.state.metrics['time_passed']),\n",
    "            trainer.state.episode, trainer.state.iteration))\n",
    "        trainer.should_terminate = True\n",
    "\n",
    "    logdir = f\"runs/{datetime.now().isoformat(timespec='minutes')}-{params.run_name}-{NAME}\"\n",
    "    tb = tb_logger.TensorboardLogger(log_dir=logdir)\n",
    "    RunningAverage(output_transform=lambda v: v['loss']).attach(engine, \"avg_loss\")\n",
    "\n",
    "    episode_handler = tb_logger.OutputHandler(tag=\"episodes\", metric_names=['reward', 'steps', 'avg_reward'])\n",
    "    tb.attach(engine, log_handler=episode_handler, event_name=ptan_ignite.EpisodeEvents.EPISODE_COMPLETED)\n",
    "\n",
    "    # write to tensorboard every 100 iterations\n",
    "    ptan_ignite.PeriodicEvents().attach(engine)\n",
    "    handler = tb_logger.OutputHandler(tag=\"train\", metric_names=['avg_loss', 'avg_fps'],\n",
    "                                      output_transform=lambda a: a)\n",
    "    tb.attach(engine, log_handler=handler, event_name=ptan_ignite.PeriodEvents.ITERS_100_COMPLETED)\n",
    "\n",
    "    engine.run(common.batch_generator(buffer, params.replay_initial, params.batch_size))"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "executable": "/usr/bin/env python3",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
