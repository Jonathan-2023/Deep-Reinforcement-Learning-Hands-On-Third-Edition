{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9e04fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import ptan\n",
    "import ptan.ignite as ptan_ignite\n",
    "from datetime import datetime, timedelta\n",
    "import argparse\n",
    "import random\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "304ad88f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2812ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ignite.engine import Engine\n",
    "from ignite.metrics import RunningAverage\n",
    "from ignite.contrib.handlers import tensorboard_logger as tb_logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "447d1a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib import dqn_model, common"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3330c25",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "NAME = \"01_baseline\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5770c32d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # get rid of missing metrics warning\n",
    "    warnings.simplefilter(\"ignore\", category=UserWarning)\n",
    "\n",
    "    random.seed(common.SEED)\n",
    "    torch.manual_seed(common.SEED)\n",
    "    params = common.GAME_PARAMS['pong']\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\n",
    "        \"--dev\", default=\"cpu\", help=\"Device to use, default=cpu\")\n",
    "    args = parser.parse_args()\n",
    "    device = torch.device(args.dev)\n",
    "\n",
    "    env = gym.make(params.env_name)\n",
    "    env = ptan.common.wrappers.wrap_dqn(env)\n",
    "\n",
    "    net = dqn_model.DQN(env.observation_space.shape, env.action_space.n).to(device)\n",
    "\n",
    "    tgt_net = ptan.agent.TargetNet(net)\n",
    "    selector = ptan.actions.EpsilonGreedyActionSelector(epsilon=params.epsilon_start)\n",
    "    epsilon_tracker = common.EpsilonTracker(selector, params)\n",
    "    agent = ptan.agent.DQNAgent(net, selector, device=device)\n",
    "\n",
    "    exp_source = ptan.experience.ExperienceSourceFirstLast(\n",
    "        env, agent, gamma=params.gamma, env_seed=common.SEED)\n",
    "    buffer = ptan.experience.ExperienceReplayBuffer(\n",
    "        exp_source, buffer_size=params.replay_size)\n",
    "    optimizer = optim.Adam(net.parameters(), lr=params.learning_rate)\n",
    "\n",
    "    def process_batch(engine, batch):\n",
    "        optimizer.zero_grad()\n",
    "        loss_v = common.calc_loss_dqn(batch, net, tgt_net.target_model,\n",
    "                                      gamma=params.gamma, device=device)\n",
    "        loss_v.backward()\n",
    "        optimizer.step()\n",
    "        epsilon_tracker.frame(engine.state.iteration)\n",
    "        if engine.state.iteration % params.target_net_sync == 0:\n",
    "            tgt_net.sync()\n",
    "        return {\n",
    "            \"loss\": loss_v.item(),\n",
    "            \"epsilon\": selector.epsilon,\n",
    "        }\n",
    "\n",
    "    engine = Engine(process_batch)\n",
    "    ptan_ignite.EndOfEpisodeHandler(exp_source, bound_avg_reward=18.0).attach(engine)\n",
    "    ptan_ignite.EpisodeFPSHandler().attach(engine)\n",
    "\n",
    "    @engine.on(ptan_ignite.EpisodeEvents.EPISODE_COMPLETED)\n",
    "    def episode_completed(trainer: Engine):\n",
    "        print(\"Episode %d: reward=%s, steps=%s, speed=%.3f frames/s, elapsed=%s\" % (\n",
    "            trainer.state.episode, trainer.state.episode_reward,\n",
    "            trainer.state.episode_steps, trainer.state.metrics.get('avg_fps', 0),\n",
    "            timedelta(seconds=trainer.state.metrics.get('time_passed', 0))))\n",
    "        trainer.should_terminate = trainer.state.episode > 700\n",
    "\n",
    "    @engine.on(ptan_ignite.EpisodeEvents.BOUND_REWARD_REACHED)\n",
    "    def game_solved(trainer: Engine):\n",
    "        print(\"Game solved in %s, after %d episodes and %d iterations!\" % (\n",
    "            timedelta(seconds=trainer.state.metrics['time_passed']),\n",
    "            trainer.state.episode, trainer.state.iteration))\n",
    "        trainer.should_terminate = True\n",
    "\n",
    "    logdir = f\"runs/{datetime.now().isoformat(timespec='minutes')}-{params.run_name}-{NAME}\"\n",
    "    tb = tb_logger.TensorboardLogger(log_dir=logdir)\n",
    "    RunningAverage(output_transform=lambda v: v['loss']).attach(engine, \"avg_loss\")\n",
    "\n",
    "    episode_handler = tb_logger.OutputHandler(tag=\"episodes\", metric_names=['reward', 'steps', 'avg_reward'])\n",
    "    tb.attach(engine, log_handler=episode_handler, event_name=ptan_ignite.EpisodeEvents.EPISODE_COMPLETED)\n",
    "\n",
    "    # write to tensorboard every 100 iterations\n",
    "    ptan_ignite.PeriodicEvents().attach(engine)\n",
    "    handler = tb_logger.OutputHandler(tag=\"train\", metric_names=['avg_loss', 'avg_fps'],\n",
    "                                      output_transform=lambda a: a)\n",
    "    tb.attach(engine, log_handler=handler, event_name=ptan_ignite.PeriodEvents.ITERS_100_COMPLETED)\n",
    "\n",
    "    engine.run(common.batch_generator(buffer, params.replay_initial, params.batch_size))"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "executable": "/usr/bin/env python3",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
