{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e953243",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import ptan\n",
    "import numpy as np\n",
    "import argparse\n",
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4b50043",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.utils as nn_utils\n",
    "import torch.optim as optim\n",
    "from ray import tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a5d11a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib import common"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1211a0c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "GAMMA = 0.99\n",
    "#LEARNING_RATE = 0.0001\n",
    "#ENTROPY_BETA = 0.02\n",
    "BATCH_SIZE = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04f838cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#REWARD_STEPS = 10\n",
    "BASELINE_STEPS = 1000000\n",
    "#GRAD_L2_CLIP = 0.1\n",
    "EVAL_STEPS = 1_000_000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a03887b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "ENV_COUNT = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f96b7c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "PARAMS_SPACE = {\n",
    "    \"lr\": tune.loguniform(1e-5, 1e-3),\n",
    "    \"reward_steps\": tune.choice([3, 5, 7, 9]),\n",
    "    \"grad_clip\": tune.loguniform(1e-2, 1),\n",
    "    \"beta\": tune.loguniform(1e-4, 1e-1),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "310d3649",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_env():\n",
    "    return ptan.common.wrappers.wrap_dqn(gym.make(\"PongNoFrameskip-v4\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d05aa6dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MeanBuffer:\n",
    "    def __init__(self, capacity: int):\n",
    "        self.capacity = capacity\n",
    "        self.deque = collections.deque(maxlen=capacity)\n",
    "        self.sum = 0.0\n",
    "\n",
    "    def add(self, val: float):\n",
    "        if len(self.deque) == self.capacity:\n",
    "            self.sum -= self.deque[0]\n",
    "        self.deque.append(val)\n",
    "        self.sum += val\n",
    "\n",
    "    def mean(self) -> float:\n",
    "        if not self.deque:\n",
    "            return 0.0\n",
    "        return self.sum / len(self.deque)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52a2ddfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(config: dict, device: torch.device) -> dict:\n",
    "    p_lr = config['lr']\n",
    "    p_reward_steps = config['reward_steps']\n",
    "    p_grad_clip = config['grad_clip']\n",
    "    p_beta = config['beta']\n",
    "\n",
    "    envs = [make_env() for _ in range(ENV_COUNT)]\n",
    "\n",
    "    net = common.AtariPGN(envs[0].observation_space.shape, envs[0].action_space.n).to(device)\n",
    "    print(net)\n",
    "\n",
    "    agent = ptan.agent.PolicyAgent(net, apply_softmax=True, device=device)\n",
    "    exp_source = ptan.experience.ExperienceSourceFirstLast(\n",
    "        envs, agent, gamma=GAMMA, steps_count=p_reward_steps)\n",
    "\n",
    "    optimizer = optim.Adam(net.parameters(), lr=p_lr, eps=1e-3)\n",
    "\n",
    "    train_step_idx = 0\n",
    "    baseline_buf = MeanBuffer(BASELINE_STEPS)\n",
    "    reward_buf = MeanBuffer(100)\n",
    "\n",
    "    batch_states, batch_actions, batch_scales = [], [], []\n",
    "    max_reward = None\n",
    "\n",
    "    for step_idx, exp in enumerate(exp_source):\n",
    "        if step_idx > EVAL_STEPS:\n",
    "            break\n",
    "        baseline_buf.add(exp.reward)\n",
    "        baseline = baseline_buf.mean()\n",
    "        batch_states.append(np.asarray(exp.state))\n",
    "        batch_actions.append(int(exp.action))\n",
    "        batch_scales.append(exp.reward - baseline)\n",
    "\n",
    "        # handle new rewards\n",
    "        new_rewards = exp_source.pop_total_rewards()\n",
    "        if new_rewards:\n",
    "            for r in new_rewards:\n",
    "                reward_buf.add(r)\n",
    "            max_rw = reward_buf.mean()\n",
    "            if max_reward is None or max_rw > max_reward:\n",
    "                print(f\"{step_idx}: Max mean reward updated: {max_reward} -> {max_rw:.2f}\")\n",
    "                max_reward = max_rw\n",
    "        if len(batch_states) < BATCH_SIZE:\n",
    "            continue\n",
    "\n",
    "        train_step_idx += 1\n",
    "        states_v = torch.as_tensor(np.asarray(batch_states)).to(device)\n",
    "        batch_actions_t = torch.as_tensor(batch_actions).to(device)\n",
    "        batch_scale_v = torch.as_tensor(batch_scales).to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        logits_v = net(states_v)\n",
    "        log_prob_v = F.log_softmax(logits_v, dim=1)\n",
    "        log_prob_actions_v = batch_scale_v * log_prob_v[range(BATCH_SIZE), batch_actions_t]\n",
    "        loss_policy_v = -log_prob_actions_v.mean()\n",
    "\n",
    "        prob_v = F.softmax(logits_v, dim=1)\n",
    "        entropy_v = -(prob_v * log_prob_v).sum(dim=1).mean()\n",
    "        entropy_loss_v = -p_beta * entropy_v\n",
    "        loss_v = loss_policy_v + entropy_loss_v\n",
    "        loss_v.backward()\n",
    "        nn_utils.clip_grad_norm_(net.parameters(), p_grad_clip)\n",
    "        optimizer.step()\n",
    "\n",
    "        batch_states.clear()\n",
    "        batch_actions.clear()\n",
    "        batch_scales.clear()\n",
    "    for e in envs:\n",
    "        e.close()\n",
    "    return {\"max_reward\": max_reward}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c7ddc40",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\n",
    "        \"--dev\", default=\"cpu\", help=\"Device to use, default=cpu\")\n",
    "    parser.add_argument(\"--samples\", type=int, default=20, help=\"Count of samples to run\")\n",
    "    args = parser.parse_args()\n",
    "    device = torch.device(args.dev)\n",
    "\n",
    "    config = tune.TuneConfig(num_samples=args.samples)\n",
    "    obj = tune.with_parameters(train, device=device)\n",
    "    if device.type == 'cuda':\n",
    "        obj = tune.with_resources(obj, {\"gpu\": 1})\n",
    "    tuner = tune.Tuner(\n",
    "        obj, param_space=PARAMS_SPACE, tune_config=config\n",
    "    )\n",
    "    results = tuner.fit()\n",
    "    best = results.get_best_result(metric=\"max_reward\", mode=\"max\")\n",
    "    print(best.config)\n",
    "    print(best.metrics)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "executable": "/usr/bin/env python3",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
