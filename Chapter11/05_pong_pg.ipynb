{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cf8245f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import ptan\n",
    "import numpy as np\n",
    "import argparse\n",
    "import collections\n",
    "from torch.utils.tensorboard.writer import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f12caff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.utils as nn_utils\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9baac6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib import common"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bae299e",
   "metadata": {},
   "outputs": [],
   "source": [
    "GAMMA = 0.99\n",
    "LEARNING_RATE = 0.00010792\n",
    "ENTROPY_BETA = 0.00010649\n",
    "BATCH_SIZE = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd584aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "REWARD_STEPS = 9\n",
    "BASELINE_STEPS = 1000000\n",
    "GRAD_L2_CLIP = 0.39215"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a964d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "ENV_COUNT = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b54e7c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_env():\n",
    "    return ptan.common.wrappers.wrap_dqn(gym.make(\"PongNoFrameskip-v4\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09e76511",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MeanBuffer:\n",
    "    def __init__(self, capacity: int):\n",
    "        self.capacity = capacity\n",
    "        self.deque = collections.deque(maxlen=capacity)\n",
    "        self.sum = 0.0\n",
    "\n",
    "    def add(self, val: float):\n",
    "        if len(self.deque) == self.capacity:\n",
    "            self.sum -= self.deque[0]\n",
    "        self.deque.append(val)\n",
    "        self.sum += val\n",
    "\n",
    "    def mean(self) -> float:\n",
    "        if not self.deque:\n",
    "            return 0.0\n",
    "        return self.sum / len(self.deque)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc089377",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\n",
    "        \"--dev\", default=\"cpu\", help=\"Device to use, default=cpu\")\n",
    "    parser.add_argument(\"-n\", '--name', required=True, help=\"Name of the run\")\n",
    "    args = parser.parse_args()\n",
    "    device = torch.device(args.dev)\n",
    "\n",
    "    envs = [make_env() for _ in range(ENV_COUNT)]\n",
    "    writer = SummaryWriter(comment=\"-pong-pg-\" + args.name)\n",
    "\n",
    "    net = common.AtariPGN(envs[0].observation_space.shape, envs[0].action_space.n).to(device)\n",
    "    print(net)\n",
    "\n",
    "    agent = ptan.agent.PolicyAgent(net, apply_softmax=True, device=device)\n",
    "    exp_source = ptan.experience.ExperienceSourceFirstLast(envs, agent, gamma=GAMMA, steps_count=REWARD_STEPS)\n",
    "\n",
    "    optimizer = optim.Adam(net.parameters(), lr=LEARNING_RATE, eps=1e-3)\n",
    "\n",
    "    total_rewards = []\n",
    "    step_idx = 0\n",
    "    done_episodes = 0\n",
    "    train_step_idx = 0\n",
    "    baseline_buf = MeanBuffer(BASELINE_STEPS)\n",
    "\n",
    "    batch_states, batch_actions, batch_scales = [], [], []\n",
    "    m_baseline, m_batch_scales, m_loss_entropy, m_loss_policy, m_loss_total = [], [], [], [], []\n",
    "    m_grad_max, m_grad_mean = [], []\n",
    "    sum_reward = 0.0\n",
    "\n",
    "    with common.RewardTracker(writer, stop_reward=18) as tracker:\n",
    "        for step_idx, exp in enumerate(exp_source):\n",
    "            baseline_buf.add(exp.reward)\n",
    "            baseline = baseline_buf.mean()\n",
    "            batch_states.append(np.asarray(exp.state))\n",
    "            batch_actions.append(int(exp.action))\n",
    "            batch_scales.append(exp.reward - baseline)\n",
    "\n",
    "            # handle new rewards\n",
    "            new_rewards = exp_source.pop_total_rewards()\n",
    "            if new_rewards:\n",
    "                if tracker.reward(new_rewards[0], step_idx):\n",
    "                    break\n",
    "\n",
    "            if len(batch_states) < BATCH_SIZE:\n",
    "                continue\n",
    "\n",
    "            train_step_idx += 1\n",
    "            states_v = torch.as_tensor(np.asarray(batch_states)).to(device)\n",
    "            batch_actions_t = torch.as_tensor(batch_actions).to(device)\n",
    "\n",
    "            scale_std = np.std(batch_scales)\n",
    "            batch_scale_v = torch.as_tensor(batch_scales).to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            logits_v = net(states_v)\n",
    "            log_prob_v = F.log_softmax(logits_v, dim=1)\n",
    "            log_prob_actions_v = batch_scale_v * log_prob_v[range(BATCH_SIZE), batch_actions_t]\n",
    "            loss_policy_v = -log_prob_actions_v.mean()\n",
    "\n",
    "            prob_v = F.softmax(logits_v, dim=1)\n",
    "            entropy_v = -(prob_v * log_prob_v).sum(dim=1).mean()\n",
    "            entropy_loss_v = -ENTROPY_BETA * entropy_v\n",
    "            loss_v = loss_policy_v + entropy_loss_v\n",
    "            loss_v.backward()\n",
    "            nn_utils.clip_grad_norm_(net.parameters(), GRAD_L2_CLIP)\n",
    "            optimizer.step()\n",
    "\n",
    "            # calc KL-div\n",
    "            new_logits_v = net(states_v)\n",
    "            new_prob_v = F.softmax(new_logits_v, dim=1)\n",
    "            kl_div_v = -((new_prob_v / prob_v).log() * prob_v).sum(dim=1).mean()\n",
    "            writer.add_scalar(\"kl\", kl_div_v.item(), step_idx)\n",
    "\n",
    "            grad_max = 0.0\n",
    "            grad_means = 0.0\n",
    "            grad_count = 0\n",
    "            for p in net.parameters():\n",
    "                grad_max = max(grad_max, p.grad.abs().max().item())\n",
    "                grad_means += (p.grad ** 2).mean().sqrt().item()\n",
    "                grad_count += 1\n",
    "\n",
    "            writer.add_scalar(\"baseline\", baseline, step_idx)\n",
    "            writer.add_scalar(\"entropy\", entropy_v.item(), step_idx)\n",
    "            writer.add_scalar(\"batch_scales\", np.mean(batch_scales), step_idx)\n",
    "            writer.add_scalar(\"batch_scales_std\", scale_std, step_idx)\n",
    "            writer.add_scalar(\"loss_entropy\", entropy_loss_v.item(), step_idx)\n",
    "            writer.add_scalar(\"loss_policy\", loss_policy_v.item(), step_idx)\n",
    "            writer.add_scalar(\"loss_total\", loss_v.item(), step_idx)\n",
    "            writer.add_scalar(\"grad_l2\", grad_means / grad_count, step_idx)\n",
    "            writer.add_scalar(\"grad_max\", grad_max, step_idx)\n",
    "\n",
    "            batch_states.clear()\n",
    "            batch_actions.clear()\n",
    "            batch_scales.clear()\n",
    "\n",
    "    writer.close()"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "executable": "/usr/bin/env python3",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
