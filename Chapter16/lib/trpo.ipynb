{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bcb2fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a8ac093",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eb460bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_flat_params_from(model):\n",
    "    params = []\n",
    "    for param in model.parameters():\n",
    "        params.append(param.data.view(-1))\n",
    "\n",
    "    flat_params = torch.cat(params)\n",
    "    return flat_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38955ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_flat_params_to(model, flat_params):\n",
    "    prev_ind = 0\n",
    "    for param in model.parameters():\n",
    "        flat_size = int(np.prod(list(param.size())))\n",
    "        param.data.copy_(\n",
    "            flat_params[prev_ind:prev_ind + flat_size].view(param.size()))\n",
    "        prev_ind += flat_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb3d2eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conjugate_gradients(Avp, b, nsteps, residual_tol=1e-10, device=\"cpu\"):\n",
    "    x = torch.zeros(b.size()).to(device)\n",
    "    r = b.clone()\n",
    "    p = b.clone()\n",
    "    rdotr = torch.dot(r, r)\n",
    "    for i in range(nsteps):\n",
    "        _Avp = Avp(p)\n",
    "        alpha = rdotr / torch.dot(p, _Avp)\n",
    "        x += alpha * p\n",
    "        r -= alpha * _Avp\n",
    "        new_rdotr = torch.dot(r, r)\n",
    "        betta = new_rdotr / rdotr\n",
    "        p = r + betta * p\n",
    "        rdotr = new_rdotr\n",
    "        if rdotr < residual_tol:\n",
    "            break\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fae9fa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linesearch(model,\n",
    "               f,\n",
    "               x,\n",
    "               fullstep,\n",
    "               expected_improve_rate,\n",
    "               max_backtracks=10,\n",
    "               accept_ratio=.1):\n",
    "    fval = f().data\n",
    "    for (_n_backtracks, stepfrac) in enumerate(.5**np.arange(max_backtracks)):\n",
    "        xnew = x + fullstep * stepfrac\n",
    "        set_flat_params_to(model, xnew)\n",
    "        newfval = f().data\n",
    "        actual_improve = fval - newfval\n",
    "        expected_improve = expected_improve_rate * stepfrac\n",
    "        ratio = actual_improve / expected_improve\n",
    "\n",
    "        if ratio.item() > accept_ratio and actual_improve.item() > 0:\n",
    "            return True, xnew\n",
    "    return False, x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8a1209e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trpo_step(model, get_loss, get_kl, max_kl, damping, device=\"cpu\"):\n",
    "    loss = get_loss()\n",
    "    grads = torch.autograd.grad(loss, model.parameters())\n",
    "    loss_grad = torch.cat([grad.view(-1) for grad in grads]).data\n",
    "\n",
    "    def Fvp(v):\n",
    "        kl = get_kl()\n",
    "        kl = kl.mean()\n",
    "\n",
    "        grads = torch.autograd.grad(kl, model.parameters(), create_graph=True)\n",
    "        flat_grad_kl = torch.cat([grad.view(-1) for grad in grads])\n",
    "\n",
    "        v_v = v.clone().detach().to(device)\n",
    "        kl_v = (flat_grad_kl * v_v).sum()\n",
    "        grads = torch.autograd.grad(kl_v, model.parameters())\n",
    "        flat_grad_grad_kl = torch.cat([grad.contiguous().view(-1) for grad in grads]).data\n",
    "\n",
    "        return flat_grad_grad_kl + v * damping\n",
    "\n",
    "    stepdir = conjugate_gradients(Fvp, -loss_grad, 10, device=device)\n",
    "\n",
    "    shs = 0.5 * (stepdir * Fvp(stepdir)).sum(0, keepdim=True)\n",
    "\n",
    "    lm = torch.sqrt(shs / max_kl)\n",
    "    fullstep = stepdir / lm[0]\n",
    "\n",
    "    neggdotstepdir = (-loss_grad * stepdir).sum(0, keepdim=True)\n",
    "\n",
    "    prev_params = get_flat_params_from(model)\n",
    "    success, new_params = linesearch(model, get_loss, prev_params, fullstep,\n",
    "                                     neggdotstepdir / lm[0])\n",
    "    set_flat_params_to(model, new_params)\n",
    "\n",
    "    return loss"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
