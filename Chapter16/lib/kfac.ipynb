{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d597cca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code taken here https://github.com/ikostrikov/pytorch-a2c-ppo-acktr/blob/master/kfac.py (with minor modifications)\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c880817",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57e835ad",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "TODO: In order to make this code faster:\n",
    "1) Implement _extract_patches as a single cuda kernel\n",
    "2) Compute QR decomposition in a separate process\n",
    "3) Actually make a general KFAC optimizer so it fits PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13752b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AddBias(nn.Module):\n",
    "    def __init__(self, bias):\n",
    "        super(AddBias, self).__init__()\n",
    "        self._bias = nn.Parameter(bias.unsqueeze(1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        if x.dim() == 2:\n",
    "            bias = self._bias.t().view(1, -1)\n",
    "        else:\n",
    "            bias = self._bias.t().view(1, -1, 1, 1)\n",
    "\n",
    "        return x + bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc208163",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _extract_patches(x, kernel_size, stride, padding):\n",
    "    if padding[0] + padding[1] > 0:\n",
    "        x = F.pad(x, (padding[1], padding[1], padding[0],\n",
    "                      padding[0])).data  # Actually check dims\n",
    "    x = x.unfold(2, kernel_size[0], stride[0])\n",
    "    x = x.unfold(3, kernel_size[1], stride[1])\n",
    "    x = x.transpose_(1, 2).transpose_(2, 3).contiguous()\n",
    "    x = x.view(\n",
    "        x.size(0), x.size(1), x.size(2), x.size(3) * x.size(4) * x.size(5))\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b323c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cov_a(a, classname, layer_info, fast_cnn):\n",
    "    batch_size = a.size(0)\n",
    "\n",
    "    if classname == 'Conv2d':\n",
    "        if fast_cnn:\n",
    "            a = _extract_patches(a, *layer_info)\n",
    "            a = a.view(a.size(0), -1, a.size(-1))\n",
    "            a = a.mean(1)\n",
    "        else:\n",
    "            a = _extract_patches(a, *layer_info)\n",
    "            a = a.view(-1, a.size(-1)).div_(a.size(1)).div_(a.size(2))\n",
    "    elif classname == 'AddBias':\n",
    "        is_cuda = a.is_cuda\n",
    "        a = torch.ones(a.size(0), 1)\n",
    "        if is_cuda:\n",
    "            a = a.cuda()\n",
    "\n",
    "    return a.t() @ (a / batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f82e809e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cov_g(g, classname, layer_info, fast_cnn):\n",
    "    batch_size = g.size(0)\n",
    "\n",
    "    if classname == 'Conv2d':\n",
    "        if fast_cnn:\n",
    "            g = g.view(g.size(0), g.size(1), -1)\n",
    "            g = g.sum(-1)\n",
    "        else:\n",
    "            g = g.transpose(1, 2).transpose(2, 3).contiguous()\n",
    "            g = g.view(-1, g.size(-1)).mul_(g.size(1)).mul_(g.size(2))\n",
    "    elif classname == 'AddBias':\n",
    "        g = g.view(g.size(0), g.size(1), -1)\n",
    "        g = g.sum(-1)\n",
    "\n",
    "    g_ = g * batch_size\n",
    "    return g_.t() @ (g_ / g.size(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef1379cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_running_stat(aa, m_aa, momentum):\n",
    "    # Do the trick to keep aa unchanged and not create any additional tensors\n",
    "    m_aa *= momentum / (1 - momentum)\n",
    "    m_aa += aa\n",
    "    m_aa *= (1 - momentum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e351ec2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SplitBias(nn.Module):\n",
    "    def __init__(self, module):\n",
    "        super(SplitBias, self).__init__()\n",
    "        self.module = module\n",
    "        self.add_bias = AddBias(module.bias.data)\n",
    "        self.module.bias = None\n",
    "\n",
    "    def forward(self, input):\n",
    "        x = self.module(input)\n",
    "        x = self.add_bias(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fa8c619",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KFACOptimizer(optim.Optimizer):\n",
    "    def __init__(self,\n",
    "                 model,\n",
    "                 lr=0.25,\n",
    "                 momentum=0.9,\n",
    "                 stat_decay=0.99,\n",
    "                 kl_clip=0.001,\n",
    "                 damping=1e-2,\n",
    "                 weight_decay=0,\n",
    "                 fast_cnn=False,\n",
    "                 Ts=1,\n",
    "                 Tf=10):\n",
    "        defaults = dict()\n",
    "\n",
    "        def split_bias(module):\n",
    "            for mname, child in module.named_children():\n",
    "                if hasattr(child, 'bias'):\n",
    "                    module._modules[mname] = SplitBias(child)\n",
    "                else:\n",
    "                    split_bias(child)\n",
    "\n",
    "        split_bias(model)\n",
    "\n",
    "        super(KFACOptimizer, self).__init__(model.parameters(), defaults)\n",
    "\n",
    "        self.known_modules = {'Linear', 'Conv2d', 'AddBias'}\n",
    "\n",
    "        self.modules = []\n",
    "        self.grad_outputs = {}\n",
    "\n",
    "        self.model = model\n",
    "        self._prepare_model()\n",
    "\n",
    "        self.steps = 0\n",
    "\n",
    "        self.m_aa, self.m_gg = {}, {}\n",
    "        self.Q_a, self.Q_g = {}, {}\n",
    "        self.d_a, self.d_g = {}, {}\n",
    "\n",
    "        self.momentum = momentum\n",
    "        self.stat_decay = stat_decay\n",
    "\n",
    "        self.lr = lr\n",
    "        self.kl_clip = kl_clip\n",
    "        self.damping = damping\n",
    "        self.weight_decay = weight_decay\n",
    "\n",
    "        self.fast_cnn = fast_cnn\n",
    "\n",
    "        self.Ts = Ts\n",
    "        self.Tf = Tf\n",
    "\n",
    "        self.optim = optim.SGD(\n",
    "            model.parameters(),\n",
    "            lr=self.lr * (1 - self.momentum),\n",
    "            momentum=self.momentum)\n",
    "\n",
    "    def _save_input(self, module, input):\n",
    "        if self.steps % self.Ts == 0:\n",
    "            classname = module.__class__.__name__\n",
    "            layer_info = None\n",
    "            if classname == 'Conv2d':\n",
    "                layer_info = (module.kernel_size, module.stride,\n",
    "                              module.padding)\n",
    "\n",
    "            aa = compute_cov_a(input[0].data, classname, layer_info,\n",
    "                               self.fast_cnn)\n",
    "\n",
    "            # Initialize buffers\n",
    "            if self.steps == 0:\n",
    "                self.m_aa[module] = aa.clone()\n",
    "\n",
    "            update_running_stat(aa, self.m_aa[module], self.stat_decay)\n",
    "\n",
    "    def _save_grad_output(self, module, grad_input, grad_output):\n",
    "        if self.acc_stats:\n",
    "            classname = module.__class__.__name__\n",
    "            layer_info = None\n",
    "            if classname == 'Conv2d':\n",
    "                layer_info = (module.kernel_size, module.stride,\n",
    "                              module.padding)\n",
    "\n",
    "            gg = compute_cov_g(grad_output[0].data, classname,\n",
    "                               layer_info, self.fast_cnn)\n",
    "\n",
    "            # Initialize buffers\n",
    "            if self.steps == 0:\n",
    "                self.m_gg[module] = gg.clone()\n",
    "\n",
    "            update_running_stat(gg, self.m_gg[module], self.stat_decay)\n",
    "\n",
    "    def _prepare_model(self):\n",
    "        for module in self.model.modules():\n",
    "            classname = module.__class__.__name__\n",
    "            if classname in self.known_modules:\n",
    "                assert not ((classname in ['Linear', 'Conv2d']) and module.bias is not None), \\\n",
    "                                    \"You must have a bias as a separate layer\"\n",
    "\n",
    "                self.modules.append(module)\n",
    "                module.register_forward_pre_hook(self._save_input)\n",
    "                module.register_backward_hook(self._save_grad_output)\n",
    "\n",
    "    def step(self):\n",
    "        # Add weight decay\n",
    "        if self.weight_decay > 0:\n",
    "            for p in self.model.parameters():\n",
    "                p.grad.data.add_(self.weight_decay, p.data)\n",
    "\n",
    "        updates = {}\n",
    "        for i, m in enumerate(self.modules):\n",
    "            assert len(list(m.parameters())\n",
    "                       ) == 1, \"Can handle only one parameter at the moment\"\n",
    "            classname = m.__class__.__name__\n",
    "            p = next(m.parameters())\n",
    "\n",
    "            la = self.damping + self.weight_decay\n",
    "\n",
    "            if self.steps % self.Tf == 0:\n",
    "                # My asynchronous implementation exists, I will add it later.\n",
    "                # Experimenting with different ways to this in PyTorch.\n",
    "                self.d_a[m], self.Q_a[m] = torch.linalg.eigh(\n",
    "                    self.m_aa[m], UPLO='U')\n",
    "                self.d_g[m], self.Q_g[m] = torch.linalg.eigh(\n",
    "                    self.m_gg[m], UPLO='U')\n",
    "\n",
    "                self.d_a[m].mul_((self.d_a[m] > 1e-6).float())\n",
    "                self.d_g[m].mul_((self.d_g[m] > 1e-6).float())\n",
    "\n",
    "            if classname == 'Conv2d':\n",
    "                p_grad_mat = p.grad.data.view(p.grad.data.size(0), -1)\n",
    "            else:\n",
    "                p_grad_mat = p.grad.data\n",
    "\n",
    "            v1 = self.Q_g[m].t() @ p_grad_mat @ self.Q_a[m]\n",
    "            v2 = v1 / (\n",
    "                self.d_g[m].unsqueeze(1) * self.d_a[m].unsqueeze(0) + la)\n",
    "            v = self.Q_g[m] @ v2 @ self.Q_a[m].t()\n",
    "\n",
    "            v = v.view(p.grad.data.size())\n",
    "            updates[p] = v\n",
    "\n",
    "        vg_sum = 0\n",
    "        for p in self.model.parameters():\n",
    "            if p not in updates:\n",
    "#                print(\"Not found in updates: %s\" % p)\n",
    "                continue\n",
    "            v = updates[p]\n",
    "            vg_sum += (v * p.grad.data * self.lr * self.lr).sum()\n",
    "\n",
    "        nu = min(1, math.sqrt(self.kl_clip / vg_sum))\n",
    "\n",
    "        for p in self.model.parameters():\n",
    "            if p not in updates:\n",
    "#                print(\"Not found in updates: %s\" % p)\n",
    "                continue\n",
    "            v = updates[p]\n",
    "            p.grad.data.copy_(v)\n",
    "            p.grad.data.mul_(nu)\n",
    "\n",
    "        self.optim.step()\n",
    "        self.steps += 1"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
