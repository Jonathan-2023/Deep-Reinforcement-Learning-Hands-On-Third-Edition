{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a5a1481",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.distributions as distr\n",
    "import gymnasium as gym\n",
    "import typing as tt\n",
    "import ptan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "265cfc2e",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "from lib import model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95e2b76c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ENV_IDS = {\n",
    "    'cheetah': \"HalfCheetahBulletEnv-v0\",\n",
    "    'cheetah-mujoco': \"HalfCheetah-v4\",\n",
    "    'ant': \"AntBulletEnv-v0\",\n",
    "    'ant-mujoco': \"Ant-v4\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a618c042",
   "metadata": {},
   "outputs": [],
   "source": [
    "ENV_PARAMS = {\n",
    "    'cheetah': ('pybullet_envs.gym_locomotion_envs:HalfCheetahBulletEnv', 1000, 3000.0),\n",
    "    'ant': ('pybullet_envs.gym_locomotion_envs:AntBulletEnv', 1000, 2500.0),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b24811f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def register_env(name: str, mujoco: bool) -> str:\n",
    "    if mujoco:\n",
    "        real_id = ENV_IDS[name + \"-mujoco\"]\n",
    "    else:\n",
    "        # register environment in gymnasium registry, not gym's\n",
    "        real_id = ENV_IDS[name]\n",
    "        entry, steps, reward = ENV_PARAMS[name]\n",
    "        gym.register(\n",
    "            real_id, entry_point=entry,\n",
    "            max_episode_steps=steps, reward_threshold=reward,\n",
    "            apply_api_compatibility=True,\n",
    "            disable_env_checker=True,\n",
    "        )\n",
    "    return real_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7246b9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unpack_batch_a2c(\n",
    "        batch: tt.List[ptan.experience.ExperienceFirstLast],\n",
    "        net: model.ModelCritic,\n",
    "        last_val_gamma: float,\n",
    "        device: torch.device):\n",
    "    \"\"\"\n",
    "    Convert batch into training tensors\n",
    "    \"\"\"\n",
    "    states = []\n",
    "    actions = []\n",
    "    rewards = []\n",
    "    not_done_idx = []\n",
    "    last_states = []\n",
    "    for idx, exp in enumerate(batch):\n",
    "        states.append(exp.state)\n",
    "        actions.append(exp.action)\n",
    "        rewards.append(exp.reward)\n",
    "        if exp.last_state is not None:\n",
    "            not_done_idx.append(idx)\n",
    "            last_states.append(exp.last_state)\n",
    "    states_v = ptan.agent.float32_preprocessor(states).to(device)\n",
    "    actions_v = torch.FloatTensor(np.asarray(actions)).to(device)\n",
    "\n",
    "    # handle rewards\n",
    "    rewards_np = np.array(rewards, dtype=np.float32)\n",
    "    if not_done_idx:\n",
    "        last_states_v = ptan.agent.float32_preprocessor(last_states).to(device)\n",
    "        last_vals_v = net(last_states_v)\n",
    "        last_vals_np = last_vals_v.data.cpu().numpy()[:, 0]\n",
    "        rewards_np[not_done_idx] += last_val_gamma * last_vals_np\n",
    "\n",
    "    ref_vals_v = torch.FloatTensor(rewards_np).to(device)\n",
    "    return states_v, actions_v, ref_vals_v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50640548",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def unpack_batch_sac(batch: tt.List[ptan.experience.ExperienceFirstLast],\n",
    "                     val_net: model.ModelCritic, twinq_net: model.ModelSACTwinQ,\n",
    "                     policy_net: model.ModelActor, gamma: float, ent_alpha: float,\n",
    "                     device: torch.device):\n",
    "    \"\"\"\n",
    "    Unpack Soft Actor-Critic batch\n",
    "    \"\"\"\n",
    "    states_v, actions_v, ref_q_v = unpack_batch_a2c(batch, val_net, gamma, device)\n",
    "\n",
    "    # references for the critic network\n",
    "    mu_v = policy_net(states_v)\n",
    "    act_dist = distr.Normal(mu_v, torch.exp(policy_net.logstd))\n",
    "    acts_v = act_dist.sample()\n",
    "    q1_v, q2_v = twinq_net(states_v, acts_v)\n",
    "    # element-wise minimum\n",
    "    ref_vals_v = torch.min(q1_v, q2_v).squeeze() - \\\n",
    "                 ent_alpha * act_dist.log_prob(acts_v).sum(dim=1)\n",
    "    return states_v, actions_v, ref_vals_v, ref_q_v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac681312",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "972d816a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_adv_ref(trajectory: tt.List[ptan.experience.Experience],\n",
    "                 net_crt: model.ModelCritic, states_v: torch.Tensor, gamma: float,\n",
    "                 gae_lambda: float, device: torch.device):\n",
    "    \"\"\"\n",
    "    By trajectory calculate advantage and 1-step ref value\n",
    "    :param trajectory: trajectory list\n",
    "    :param net_crt: critic network\n",
    "    :param states_v: states tensor\n",
    "    :return: tuple with advantage numpy array and reference values\n",
    "    \"\"\"\n",
    "    values_v = net_crt(states_v)\n",
    "    values = values_v.squeeze().data.cpu().numpy()\n",
    "    # generalized advantage estimator: smoothed version of the advantage\n",
    "    last_gae = 0.0\n",
    "    result_adv = []\n",
    "    result_ref = []\n",
    "    for val, next_val, (exp,) in zip(\n",
    "            reversed(values[:-1]), reversed(values[1:]), reversed(trajectory[:-1])):\n",
    "        if exp.done_trunc:\n",
    "            delta = exp.reward - val\n",
    "            last_gae = delta\n",
    "        else:\n",
    "            delta = exp.reward + gamma * next_val - val\n",
    "            last_gae = delta + gamma * gae_lambda * last_gae\n",
    "        result_adv.append(last_gae)\n",
    "        result_ref.append(last_gae + val)\n",
    "\n",
    "    adv_v = torch.FloatTensor(np.asarray(list(reversed(result_adv))))\n",
    "    ref_v = torch.FloatTensor(np.asarray(list(reversed(result_ref))))\n",
    "    return adv_v.to(device), ref_v.to(device)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
