{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a11a277",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import ptan\n",
    "from ptan.experience import VectorExperienceSourceFirstLast\n",
    "from ptan.common.utils import TBMeanTracker\n",
    "import numpy as np\n",
    "import argparse\n",
    "import pathlib\n",
    "import queue\n",
    "import typing as tt\n",
    "from torch.utils.tensorboard.writer import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0027036",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.utils as nn_utils\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1fc1bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib import common, rlhf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3e8c48e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limit of steps in episodes is needed, as sometimes agent leans\n",
    "# just to refill the oxigen and do nothing :)\n",
    "# This limit is applied after all the wrappers (so real game\n",
    "# will be x4 due to frame skip)\n",
    "TIME_LIMIT = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec13449",
   "metadata": {},
   "outputs": [],
   "source": [
    "GAMMA = 0.99\n",
    "LEARNING_RATE          = 0.0007\n",
    "LEARNING_RATE_FINETUNE = 0.00007\n",
    "ENTROPY_BETA = 0.01\n",
    "BATCH_SIZE = 128\n",
    "NUM_ENVS = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec25a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "REWARD_STEPS = 5\n",
    "CLIP_GRAD = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29ff7369",
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_EPISODES = 10\n",
    "TEST_EVERY_STEP = 100*BATCH_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22d7a437",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_env_func(env_idx: int, db_path: tt.Optional[str],\n",
    "                  reward_path: tt.Optional[str],\n",
    "                  dev: torch.device,\n",
    "                  metrics_queue: tt.Optional[queue.Queue]) -> \\\n",
    "        tt.Callable[[], gym.Env]:\n",
    "    def make_env() -> gym.Env:\n",
    "        e = gym.make(\"SeaquestNoFrameskip-v4\")\n",
    "        if reward_path is not None:\n",
    "            p = pathlib.Path(reward_path)\n",
    "            e = rlhf.RewardModelWrapper(e, p, dev=dev, metrics_queue=metrics_queue)\n",
    "        if db_path is not None:\n",
    "            p = pathlib.Path(db_path)\n",
    "            p.mkdir(parents=True, exist_ok=True)\n",
    "            e = rlhf.EpisodeRecorderWrapper(e, p, env_idx=env_idx)\n",
    "        e = ptan.common.wrappers.wrap_dqn(e)\n",
    "        # add time limit after all wrappers\n",
    "        e = gym.wrappers.TimeLimit(e, TIME_LIMIT)\n",
    "        return e\n",
    "    return make_env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2253b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_test_env() -> gym.Env:\n",
    "    e = gym.make(\"SeaquestNoFrameskip-v4\")\n",
    "    e = ptan.common.wrappers.wrap_dqn(e, clip_reward=False)\n",
    "    # add time limit after all wrappers\n",
    "    e = gym.wrappers.TimeLimit(e, TIME_LIMIT)\n",
    "    return e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60377070",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_metrics(step_idx: int, metrics_queue: queue.Queue,\n",
    "                    writer: SummaryWriter):\n",
    "    try:\n",
    "        while True:\n",
    "            key, val = metrics_queue.get(block=False)\n",
    "            writer.add_scalar(key, val, step_idx)\n",
    "    except queue.Empty:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3006dc09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(\n",
    "        env: gym.Env,\n",
    "        dev: torch.device,\n",
    "        net: common.AtariA2C,\n",
    "        episodes: int = TEST_EPISODES\n",
    ") -> tt.Tuple[float, int]:\n",
    "    \"\"\"\n",
    "    Test model for given amount of episodes\n",
    "    :param env: test environment\n",
    "    :param dev: device to use\n",
    "    :param net: model to test\n",
    "    :param episodes: count of episodes\n",
    "    :return: best reward and count of steps\n",
    "    \"\"\"\n",
    "    best_reward = 0.0\n",
    "    best_steps = 0\n",
    "    for _ in range(episodes):\n",
    "        cur_reward, cur_steps = 0, 0.0\n",
    "        obs, _ = env.reset()\n",
    "        while True:\n",
    "            obs_v = torch.FloatTensor(obs).unsqueeze(0).to(dev)\n",
    "            policy_v = net(obs_v)[0]\n",
    "            policy_v = F.softmax(policy_v, dim=1)\n",
    "            probs = policy_v[0].detach().cpu().numpy()\n",
    "            action = np.random.choice(len(probs), p=probs)\n",
    "            obs, reward, done, is_tr, _ = env.step(action)\n",
    "            cur_reward += reward\n",
    "            cur_steps += 1\n",
    "            if done or is_tr:\n",
    "                break\n",
    "        if best_reward < cur_reward:\n",
    "            best_reward = cur_reward\n",
    "        if best_steps < cur_steps:\n",
    "            best_steps = cur_steps\n",
    "    return best_reward, best_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35b5bf49",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--dev\", default=\"cpu\",\n",
    "                        help=\"Device to use, default=cpu\")\n",
    "    parser.add_argument(\"--use-async\", default=False,\n",
    "                        action='store_true',\n",
    "                        help=\"Use async vector env (A3C mode)\")\n",
    "    parser.add_argument(\"-n\", \"--name\", required=True,\n",
    "                        help=\"Name of the run\")\n",
    "    parser.add_argument(\"-r\", \"--reward\",\n",
    "                        help=\"Reward model to load\")\n",
    "    parser.add_argument(\"-m\", \"--model\",\n",
    "                        help=\"Policy model to load\")\n",
    "    parser.add_argument(\"--finetune\", default=False,\n",
    "                        action=\"store_true\", help=\"If given, enable finetune mode\")\n",
    "    parser.add_argument(\"--save\", help=\"If given, dir to save models\")\n",
    "    parser.add_argument(\n",
    "        \"--db-path\", help=\"If given, short episodes will be \"\n",
    "                          \"stored in this path\")\n",
    "    args = parser.parse_args()\n",
    "    device = torch.device(args.dev)\n",
    "    metrics_queue = queue.Queue(maxsize=0)\n",
    "\n",
    "    env_factories = [\n",
    "        make_env_func(env_idx, args.db_path, args.reward,\n",
    "                      device, metrics_queue)\n",
    "        for env_idx in range(NUM_ENVS)\n",
    "    ]\n",
    "    if args.use_async:\n",
    "        env = gym.vector.AsyncVectorEnv(env_factories)\n",
    "    else:\n",
    "        env = gym.vector.SyncVectorEnv(env_factories)\n",
    "    test_env = make_test_env()\n",
    "    writer = SummaryWriter(comment=\"-a2c_\" + args.name)\n",
    "\n",
    "    net = common.AtariA2C(env.single_observation_space.shape,\n",
    "                          env.single_action_space.n).to(device)\n",
    "    print(net)\n",
    "    if args.model is not None:\n",
    "        net.load_state_dict(torch.load(args.model, weights_only=True))\n",
    "        print(\"Loaded model \" + args.model)\n",
    "    agent = ptan.agent.PolicyAgent(\n",
    "        lambda x: net(x)[0], apply_softmax=True, device=device)\n",
    "    exp_source = VectorExperienceSourceFirstLast(\n",
    "        env, agent, gamma=GAMMA, steps_count=REWARD_STEPS)\n",
    "\n",
    "    lr = LEARNING_RATE\n",
    "    if args.finetune:\n",
    "        lr = LEARNING_RATE_FINETUNE\n",
    "        # Freeze convolution layers\n",
    "        net.conv.requires_grad_(False)\n",
    "\n",
    "    save_path = None\n",
    "    if args.save:\n",
    "        save_path = pathlib.Path(args.save)\n",
    "        if not save_path.exists():\n",
    "            save_path.mkdir(parents=True)\n",
    "\n",
    "    optimizer = optim.Adam(\n",
    "        net.parameters(), lr=lr, eps=1e-5)\n",
    "    scheduler = optim.lr_scheduler.PolynomialLR(\n",
    "        optimizer, total_iters=8*(10**7) / BATCH_SIZE)\n",
    "\n",
    "    batch = []\n",
    "    best_test_reward = 0.0\n",
    "    best_test_steps = 0.0\n",
    "\n",
    "    with common.RewardTracker(writer, stop_reward=None) as tracker:\n",
    "        with TBMeanTracker(writer, batch_size=10) as tb_tracker:\n",
    "            for step_idx, exp in enumerate(exp_source):\n",
    "                batch.append(exp)\n",
    "\n",
    "                # handle new rewards\n",
    "                solved = False\n",
    "                for new_reward, new_step in exp_source.pop_rewards_steps():\n",
    "                    if tracker.reward(new_reward, new_step, step_idx):\n",
    "                        solved = True\n",
    "                if solved:\n",
    "                    break\n",
    "\n",
    "                process_metrics(step_idx, metrics_queue, writer)\n",
    "                if step_idx % TEST_EVERY_STEP == 0:\n",
    "                    print(f\"{step_idx}: Testing model...\")\n",
    "                    test_rw, test_steps = test_model(test_env, device, net)\n",
    "                    writer.add_scalar(\"test_reward\", test_rw, step_idx)\n",
    "                    writer.add_scalar(\"test_steps\", test_steps, step_idx)\n",
    "                    print(f\"Got best reward {test_rw:.2f} and steps {test_steps} \"\n",
    "                          f\"in {TEST_EPISODES} episodes\")\n",
    "                    if save_path is not None:\n",
    "                        if test_rw > best_test_reward or test_steps > best_test_steps:\n",
    "                            name = \"model_%06d-rw=%.0f-steps=%d.dat\" % (step_idx, test_rw, test_steps)\n",
    "                            torch.save(net.state_dict(), str(save_path / name))\n",
    "                            best_test_reward = test_rw\n",
    "                            best_test_steps = test_steps\n",
    "\n",
    "                if len(batch) < BATCH_SIZE:\n",
    "                    continue\n",
    "\n",
    "                states_t, actions_t, vals_ref_t = \\\n",
    "                    common.unpack_batch(\n",
    "                        batch, net, device=device,\n",
    "                        gamma=GAMMA, reward_steps=REWARD_STEPS)\n",
    "                batch.clear()\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                logits_t, value_t = net(states_t)\n",
    "                loss_value_t = F.mse_loss(\n",
    "                    value_t.squeeze(-1), vals_ref_t)\n",
    "\n",
    "                log_prob_t = F.log_softmax(logits_t, dim=1)\n",
    "                adv_t = vals_ref_t - value_t.detach()\n",
    "                log_act_t = log_prob_t[range(BATCH_SIZE), actions_t]\n",
    "                log_prob_actions_t = adv_t * log_act_t\n",
    "                loss_policy_t = -log_prob_actions_t.mean()\n",
    "\n",
    "                prob_t = F.softmax(logits_t, dim=1)\n",
    "                entropy_loss_t = ENTROPY_BETA * (\n",
    "                        prob_t * log_prob_t).sum(dim=1).mean()\n",
    "\n",
    "                # calculate policy gradients only\n",
    "                loss_policy_t.backward(retain_graph=True)\n",
    "                grads = np.concatenate([\n",
    "                    p.grad.data.cpu().numpy().flatten()\n",
    "                    for p in net.parameters()\n",
    "                    if p.grad is not None\n",
    "                ])\n",
    "\n",
    "                # apply entropy and value gradients\n",
    "                loss_v = entropy_loss_t + loss_value_t\n",
    "                loss_v.backward()\n",
    "                nn_utils.clip_grad_norm_(\n",
    "                    net.parameters(), CLIP_GRAD)\n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "\n",
    "                # get full loss\n",
    "                loss_v += loss_policy_t\n",
    "\n",
    "                tb_tracker.track(\n",
    "                    \"lr\", scheduler.get_last_lr()[0], step_idx)\n",
    "                tb_tracker.track(\n",
    "                    \"advantage\", adv_t, step_idx)\n",
    "                tb_tracker.track(\n",
    "                    \"values\", value_t, step_idx)\n",
    "                tb_tracker.track(\n",
    "                    \"batch_rewards\", vals_ref_t, step_idx)\n",
    "                tb_tracker.track(\n",
    "                    \"loss_entropy\", entropy_loss_t, step_idx)\n",
    "                tb_tracker.track(\n",
    "                    \"loss_policy\", loss_policy_t, step_idx)\n",
    "                tb_tracker.track(\n",
    "                    \"loss_value\", loss_value_t, step_idx)\n",
    "                tb_tracker.track(\n",
    "                    \"loss_total\", loss_v, step_idx)\n",
    "                tb_tracker.track(\n",
    "                    \"grad_l2\", np.sqrt(np.mean(np.square(grads))),\n",
    "                    step_idx)\n",
    "                tb_tracker.track(\n",
    "                    \"grad_max\", np.max(np.abs(grads)), step_idx)\n",
    "                tb_tracker.track(\n",
    "                    \"grad_var\", np.var(grads), step_idx)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "executable": "/usr/bin/env python3",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
