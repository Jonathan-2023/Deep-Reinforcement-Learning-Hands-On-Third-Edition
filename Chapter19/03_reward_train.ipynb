{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7d9cd33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import pathlib\n",
    "import logging\n",
    "import typing as tt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffd356b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib import rlhf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b6c7d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "from torch.utils.tensorboard.writer import SummaryWriter\n",
    "from torch.utils.data import DataLoader, Dataset, ConcatDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aac9dba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "log = logging.getLogger(\"reward_train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "779bf653",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "SPLIT_SEED = 42\n",
    "TEST_RATIO = 0.2\n",
    "INPUT_SHAPE = (3, 210, 160)\n",
    "TOTAL_ACTIONS = 18\n",
    "BATCH_SIZE = 64\n",
    "LEARNING_RATE = 1e-4\n",
    "MAX_EPOCHES = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28e69118",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LabelsDataset(Dataset):\n",
    "    def __init__(self, db: rlhf.Database,\n",
    "                 labels: tt.List[rlhf.HumanLabel],\n",
    "                 total_actions: int = TOTAL_ACTIONS):\n",
    "        self.db = db\n",
    "        self.labels = labels\n",
    "        self.total_actions = total_actions\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # data loading and transformation into tensors\n",
    "        l = self.labels[idx]\n",
    "        s1_obs, s1_acts = rlhf.steps_to_tensors(\n",
    "            self.db.db_root / l.sample1, self.total_actions)\n",
    "        s2_obs, s2_acts = rlhf.steps_to_tensors(\n",
    "            self.db.db_root / l.sample2, self.total_actions)\n",
    "\n",
    "        if l.label == 1:\n",
    "            vals = [1.0, 0.0]\n",
    "        elif l.label == 2:\n",
    "            vals = [0.0, 1.0]\n",
    "        else:\n",
    "            vals = [0.5, 0.5]\n",
    "        mu = torch.as_tensor(vals)\n",
    "        return s1_obs, s1_acts, s2_obs, s2_acts, mu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07c1927c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_loss(model: rlhf.RewardModel, s1_obs: torch.ByteTensor,\n",
    "              s1_acts: torch.Tensor, s2_obs: torch.ByteTensor,\n",
    "              s2_acts: torch.Tensor, mu: torch.Tensor) -> torch.Tensor:\n",
    "    batch_size, steps = s1_obs.size()[:2]\n",
    "\n",
    "    # combine batch and time sequence dimension into long batch\n",
    "    s1_obs_flat = s1_obs.flatten(0, 1)\n",
    "    s1_acts_flat = s1_acts.flatten(0, 1)\n",
    "    r1_flat = model(s1_obs_flat, s1_acts_flat)\n",
    "    r1 = r1_flat.view((batch_size, steps))\n",
    "    R1 = torch.sum(r1, 1)\n",
    "\n",
    "    s2_obs_flat = s2_obs.flatten(0, 1)\n",
    "    s2_acts_flat = s2_acts.flatten(0, 1)\n",
    "    r2_flat = model(s2_obs_flat, s2_acts_flat)\n",
    "    r2 = r2_flat.view((batch_size, steps))\n",
    "    R2 = torch.sum(r2, 1)\n",
    "    R = torch.hstack((R1.unsqueeze(-1), R2.unsqueeze(-1)))\n",
    "    loss_t = F.binary_cross_entropy_with_logits(R, mu)\n",
    "    return loss_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "508cadbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    logging.basicConfig(format=\"%(asctime)-15s %(levelname)s %(name)s %(message)s\", level=\"INFO\")\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--dev\", default=\"cpu\", help=\"Device to use, default=cpu\")\n",
    "    parser.add_argument(\"-n\", \"--name\", required=True, help=\"Name of the run\")\n",
    "    parser.add_argument(\"-o\", \"--out\", required=True, help=\"Directory to store the model\")\n",
    "    parser.add_argument(\"dbs\", help=\"Path to DB\", nargs='+')\n",
    "\n",
    "    args = parser.parse_args()\n",
    "    print(args)\n",
    "    device = torch.device(args.dev)\n",
    "\n",
    "    databases = []\n",
    "    train_datasets = []\n",
    "    test_datasets = []\n",
    "    for db in args.dbs:\n",
    "        db = rlhf.load_db(db)\n",
    "        databases.append(db)\n",
    "        log.info(\"Loaded DB from %s with %d labels and %d paths\",\n",
    "                 db.db_root, len(db.labels), len(db.paths))\n",
    "        db.shuffle_labels(SPLIT_SEED)\n",
    "        pos = int(len(db.labels) * (1-TEST_RATIO))\n",
    "        train_labels, test_labels = db.labels[:pos], db.labels[pos:]\n",
    "        train_datasets.append(LabelsDataset(db, train_labels))\n",
    "        test_datasets.append(LabelsDataset(db, test_labels))\n",
    "    ds_train = ConcatDataset(train_datasets)\n",
    "    ds_test = ConcatDataset(test_datasets)\n",
    "    data_loader = DataLoader(ds_train, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    data_loader_test = DataLoader(ds_test, batch_size=BATCH_SIZE)\n",
    "\n",
    "    writer = SummaryWriter(comment=\"-reward_\" + args.name)\n",
    "    model = rlhf.RewardModel(INPUT_SHAPE, TOTAL_ACTIONS)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "    print(model)\n",
    "    out_path = pathlib.Path(args.out)\n",
    "    out_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    prev_test_loss = None\n",
    "    count_overfit = 0\n",
    "    for epoch in range(MAX_EPOCHES):\n",
    "        loss = 0.0\n",
    "        size = 0\n",
    "        for s1_obs, s1_acts, s2_obs, s2_acts, mu in data_loader:\n",
    "            optimizer.zero_grad()\n",
    "            loss_t = calc_loss(\n",
    "                model, s1_obs, s1_acts, s2_obs, s2_acts, mu\n",
    "            )\n",
    "            loss_t.backward()\n",
    "            optimizer.step()\n",
    "            loss += loss_t.item()\n",
    "            size += s1_obs.size()[0]\n",
    "        train_loss = loss / size\n",
    "\n",
    "        # test data\n",
    "        loss = 0.0\n",
    "        size = 0\n",
    "        for s1_obs, s1_acts, s2_obs, s2_acts, mu in data_loader_test:\n",
    "            loss_t = calc_loss(\n",
    "                model, s1_obs, s1_acts, s2_obs, s2_acts, mu\n",
    "            )\n",
    "            loss += loss_t.item()\n",
    "            size += s1_obs.size()[0]\n",
    "        test_loss = loss / size\n",
    "\n",
    "        writer.add_scalar(\"loss_train\", train_loss, epoch)\n",
    "        writer.add_scalar(\"loss_test\", test_loss, epoch)\n",
    "\n",
    "        log.info(\"Epoch %d done, train loss %f, test loss %f\",\n",
    "                 epoch, train_loss, test_loss)\n",
    "        if prev_test_loss is None or prev_test_loss > test_loss:\n",
    "            count_overfit = 0\n",
    "            # save model\n",
    "            log.info(f\"Save model for {test_loss:.5f} test loss\")\n",
    "            path = out_path / (\"reward-\" + args.name + \".dat\")\n",
    "            torch.save(model.state_dict(), str(path))\n",
    "            prev_test_loss = test_loss\n",
    "        else:\n",
    "            count_overfit += 1\n",
    "            if count_overfit > 3:\n",
    "                log.info(f\"Prev test loss was less than current for {count_overfit} epoches, stop\")\n",
    "                break"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "executable": "/usr/bin/env python3",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
