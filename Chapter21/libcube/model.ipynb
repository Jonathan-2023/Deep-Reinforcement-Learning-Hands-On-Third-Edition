{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3ee8683",
   "metadata": {},
   "outputs": [],
   "source": [
    "import enum\n",
    "import time\n",
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c835f829",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa16a91a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from . import cubes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5c0115a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, input_shape, actions_count):\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        self.input_size = int(np.prod(input_shape))\n",
    "        self.body = nn.Sequential(\n",
    "            nn.Linear(self.input_size, 4096),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(4096, 2048),\n",
    "            nn.ELU()\n",
    "        )\n",
    "        self.policy = nn.Sequential(\n",
    "            nn.Linear(2048, 512),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(512, actions_count)\n",
    "        )\n",
    "        self.value = nn.Sequential(\n",
    "            nn.Linear(2048, 512),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(512, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, batch, value_only=False):\n",
    "        x = batch.view((-1, self.input_size))\n",
    "        body_out = self.body(x)\n",
    "        value_out = self.value(body_out)\n",
    "        if value_only:\n",
    "            return value_out\n",
    "        policy_out = self.policy(body_out)\n",
    "        return policy_out, value_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7f30a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_states(cube_env, states):\n",
    "    assert isinstance(cube_env, cubes.CubeEnv)\n",
    "    assert isinstance(states, (list, tuple))\n",
    "\n",
    "    # states could be list of lists or just list of states\n",
    "    if isinstance(states[0], list):\n",
    "        encoded = np.zeros((len(states), len(states[0])) + cube_env.encoded_shape, dtype=np.float32)\n",
    "\n",
    "        for i, st_list in enumerate(states):\n",
    "            for j, state in enumerate(st_list):\n",
    "                cube_env.encode_inplace(encoded[i, j], state)\n",
    "    else:\n",
    "        encoded = np.zeros((len(states), ) + cube_env.encoded_shape, dtype=np.float32)\n",
    "        for i, state in enumerate(states):\n",
    "            cube_env.encode_inplace(encoded[i], state)\n",
    "\n",
    "    return encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc0cfe24",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ValueTargetsMethod(enum.Enum):\n",
    "    # method from the paper\n",
    "    Paper = 'paper'\n",
    "    # paper, but value of goal state equals zero\n",
    "    ZeroGoalValue = 'zero_goal_value'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67feb117",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_scramble_buffer(cube_env, buf_size, scramble_depth):\n",
    "    \"\"\"\n",
    "    Create data buffer with scramble states and explored substates\n",
    "    :param cube_env: env to use\n",
    "    :param buf_size: how many states to generate\n",
    "    :param scramble_depth: how deep to scramble\n",
    "    :return: list of tuples\n",
    "    \"\"\"\n",
    "    result = []\n",
    "    data = []\n",
    "    rounds = buf_size // scramble_depth\n",
    "    for _ in range(rounds):\n",
    "        data.extend(cube_env.scramble_cube(scramble_depth, include_initial=True))\n",
    "\n",
    "    # explore each state\n",
    "    for depth, s in data:\n",
    "        states, goals = cube_env.explore_state(s)\n",
    "        enc_s = encode_states(cube_env, [s])\n",
    "        enc_states = encode_states(cube_env, states)\n",
    "        result.append((enc_s, depth, cube_env.is_goal(s), enc_states, goals))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e42ca29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_batch(scramble_buffer, net, device, batch_size, value_targets):\n",
    "    \"\"\"\n",
    "    Sample batch of given size from scramble buffer produced by make_scramble_buffer\n",
    "    :param scramble_buffer: scramble buffer\n",
    "    :param net: network to use to calculate targets\n",
    "    :param device: device to move values\n",
    "    :param batch_size: size of batch to generate\n",
    "    :param value_targets: targets\n",
    "    :return: tensors\n",
    "    \"\"\"\n",
    "    data = random.sample(scramble_buffer, batch_size)\n",
    "    states, depths, is_goals, explored_states, explored_goals = zip(*data)\n",
    "\n",
    "    # handle explored states\n",
    "    explored_states = np.stack(explored_states)\n",
    "    shape = explored_states.shape\n",
    "    explored_states_t = torch.tensor(explored_states).to(device)\n",
    "    explored_states_t = explored_states_t.view(shape[0]*shape[1], *shape[2:])     # shape: (states*actions, encoded_shape)\n",
    "    value_t = net(explored_states_t, value_only=True)\n",
    "    value_t = value_t.squeeze(-1).view(shape[0], shape[1])                  # shape: (states, actions)\n",
    "    if value_targets == ValueTargetsMethod.Paper:\n",
    "        # add reward to the values\n",
    "        goals_mask_t = torch.tensor(explored_goals, dtype=torch.int8).to(device)\n",
    "        goals_mask_t += goals_mask_t - 1                                        # has 1 at final states and -1 elsewhere\n",
    "        value_t += goals_mask_t.type(dtype=torch.float32)\n",
    "        # find target value and target policy\n",
    "        max_val_t, max_act_t = value_t.max(dim=1)\n",
    "    elif value_targets == ValueTargetsMethod.ZeroGoalValue:\n",
    "        value_t -= 1.0\n",
    "        max_val_t, max_act_t = value_t.max(dim=1)\n",
    "        goal_indices = np.nonzero(is_goals)\n",
    "        max_val_t[goal_indices] = 0.0\n",
    "        max_act_t[goal_indices] = 0\n",
    "    else:\n",
    "        assert False, \"Unsupported method of value targets\"\n",
    "\n",
    "    # train input\n",
    "    enc_input = np.stack(states)\n",
    "    enc_input_t = torch.tensor(enc_input).to(device)\n",
    "    depths_t = torch.tensor(depths, dtype=torch.float32).to(device)\n",
    "    weights_t = 1/depths_t\n",
    "    return enc_input_t.detach(), weights_t.detach(), max_act_t.detach(), max_val_t.detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c3f0784",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_train_data(cube_env, net, device, batch_size, scramble_depth, shuffle=False,\n",
    "                    value_targets=ValueTargetsMethod.Paper):\n",
    "    assert isinstance(cube_env, cubes.CubeEnv)\n",
    "    assert isinstance(value_targets, ValueTargetsMethod)\n",
    "\n",
    "    # scramble cube states and their depths\n",
    "    data = []\n",
    "    rounds = batch_size // scramble_depth\n",
    "    for _ in range(rounds):\n",
    "        data.extend(cube_env.scramble_cube(scramble_depth, include_initial=True))\n",
    "    if shuffle:\n",
    "        random.shuffle(data)\n",
    "    cube_depths, cube_states = zip(*data)\n",
    "\n",
    "    # explore each state by doing 1-step BFS search and keep a mask of goal states (for reward calculation)\n",
    "    explored_states, explored_goals = [], []\n",
    "    goal_indices = []\n",
    "    for idx, s in enumerate(cube_states):\n",
    "        states, goals = cube_env.explore_state(s)\n",
    "        explored_states.append(states)\n",
    "        explored_goals.append(goals)\n",
    "        if cube_env.is_goal(s):\n",
    "            goal_indices.append(idx)\n",
    "\n",
    "    # obtain network's values for all explored states\n",
    "    enc_explored = encode_states(cube_env, explored_states)           # shape: (states, actions, encoded_shape)\n",
    "\n",
    "    shape = enc_explored.shape\n",
    "    enc_explored_t = torch.tensor(enc_explored).to(device)\n",
    "    enc_explored_t = enc_explored_t.view(shape[0]*shape[1], *shape[2:])     # shape: (states*actions, encoded_shape)\n",
    "    value_t = net(enc_explored_t, value_only=True)\n",
    "    value_t = value_t.squeeze(-1).view(shape[0], shape[1])                  # shape: (states, actions)\n",
    "    if value_targets == ValueTargetsMethod.Paper:\n",
    "        # add reward to the values\n",
    "        goals_mask_t = torch.tensor(explored_goals, dtype=torch.int8).to(device)\n",
    "        goals_mask_t += goals_mask_t - 1                                        # has 1 at final states and -1 elsewhere\n",
    "        value_t += goals_mask_t.type(dtype=torch.float32)\n",
    "        # find target value and target policy\n",
    "        max_val_t, max_act_t = value_t.max(dim=1)\n",
    "    elif value_targets == ValueTargetsMethod.ZeroGoalValue:\n",
    "        value_t -= 1.0\n",
    "        max_val_t, max_act_t = value_t.max(dim=1)\n",
    "        max_val_t[goal_indices] = 0.0\n",
    "        max_act_t[goal_indices] = 0\n",
    "    else:\n",
    "        assert False, \"Unsupported method of value targets\"\n",
    "\n",
    "    # create train input\n",
    "    enc_input = encode_states(cube_env, cube_states)\n",
    "    enc_input_t = torch.tensor(enc_input).to(device)\n",
    "    cube_depths_t = torch.tensor(cube_depths, dtype=torch.float32).to(device)\n",
    "    weights_t = 1/cube_depths_t\n",
    "    return enc_input_t.detach(), weights_t.detach(), max_act_t.detach(), max_val_t.detach()"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
