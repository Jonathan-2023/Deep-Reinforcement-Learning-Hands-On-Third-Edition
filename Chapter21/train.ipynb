{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "396815ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import argparse\n",
    "import logging\n",
    "import numpy as np\n",
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5261760",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.optim.lr_scheduler as scheduler\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b1acc47",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard.writer import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8feb959",
   "metadata": {},
   "outputs": [],
   "source": [
    "from libcube import cubes\n",
    "from libcube import model\n",
    "from libcube import conf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19d928bc",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "log = logging.getLogger(\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee0d853e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    logging.basicConfig(format=\"%(asctime)-15s %(levelname)s %(message)s\", level=logging.INFO)\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"-i\", \"--ini\", required=True, help=\"Ini file to use for this run\")\n",
    "    parser.add_argument(\"-n\", \"--name\", required=True, help=\"Name of the run\")\n",
    "    args = parser.parse_args()\n",
    "    config = conf.Config(args.ini)\n",
    "    device = torch.device(\"cuda\" if config.train_cuda else \"cpu\")\n",
    "\n",
    "    name = config.train_name(suffix=args.name)\n",
    "    writer = SummaryWriter(comment=\"-\" + name)\n",
    "    save_path = os.path.join(\"saves\", name)\n",
    "    os.makedirs(save_path)\n",
    "\n",
    "    cube_env = cubes.get(config.cube_type)\n",
    "    assert isinstance(cube_env, cubes.CubeEnv)\n",
    "    log.info(\"Selected cube: %s\", cube_env)\n",
    "    value_targets_method = model.ValueTargetsMethod(config.train_value_targets_method)\n",
    "\n",
    "    net = model.Net(cube_env.encoded_shape, len(cube_env.action_enum)).to(device)\n",
    "    print(net)\n",
    "    opt = optim.Adam(net.parameters(), lr=config.train_learning_rate)\n",
    "    sched = scheduler.StepLR(opt, 1, gamma=config.train_lr_decay_gamma) if config.train_lr_decay_enabled else None\n",
    "\n",
    "    step_idx = 0\n",
    "    buf_policy_loss, buf_value_loss, buf_loss = [], [], []\n",
    "    buf_policy_loss_raw, buf_value_loss_raw, buf_loss_raw = [], [], []\n",
    "    buf_mean_values = []\n",
    "    ts = time.time()\n",
    "    best_loss = None\n",
    "\n",
    "    log.info(\"Generate scramble buffer...\")\n",
    "    scramble_buf = collections.deque(maxlen=config.scramble_buffer_batches*config.train_batch_size)\n",
    "    scramble_buf.extend(model.make_scramble_buffer(cube_env, config.train_batch_size*2, config.train_scramble_depth))\n",
    "    log.info(\"Generated buffer of size %d\", len(scramble_buf))\n",
    "\n",
    "    while True:\n",
    "        step_idx += 1\n",
    "        x_t, weights_t, y_policy_t, y_value_t = model.sample_batch(\n",
    "            scramble_buf, net, device, config.train_batch_size, value_targets_method)\n",
    "\n",
    "        opt.zero_grad()\n",
    "        policy_out_t, value_out_t = net(x_t)\n",
    "        value_out_t = value_out_t.squeeze(-1)\n",
    "        value_loss_t = (value_out_t - y_value_t)**2\n",
    "        value_loss_raw_t = value_loss_t.mean()\n",
    "        if config.weight_samples:\n",
    "            value_loss_t *= weights_t\n",
    "        value_loss_t = value_loss_t.mean()\n",
    "        policy_loss_t = F.cross_entropy(policy_out_t, y_policy_t, reduction='none')\n",
    "        policy_loss_raw_t = policy_loss_t.mean()\n",
    "        if config.weight_samples:\n",
    "            policy_loss_t *= weights_t\n",
    "        policy_loss_t = policy_loss_t.mean()\n",
    "        loss_raw_t = policy_loss_raw_t + value_loss_raw_t\n",
    "        loss_t = value_loss_t + policy_loss_t\n",
    "        loss_t.backward()\n",
    "        opt.step()\n",
    "\n",
    "        if config.train_lr_decay_enabled and step_idx % config.train_lr_decay_batches == 0:\n",
    "            sched.step()\n",
    "            log.info(\"LR decrease to %s\", sched.get_last_lr()[0])\n",
    "            writer.add_scalar(\"lr\", sched.get_last_lr()[0], step_idx)\n",
    "\n",
    "        # save data\n",
    "        buf_mean_values.append(value_out_t.mean().item())\n",
    "        buf_policy_loss.append(policy_loss_t.item())\n",
    "        buf_value_loss.append(value_loss_t.item())\n",
    "        buf_loss.append(loss_t.item())\n",
    "        buf_loss_raw.append(loss_raw_t.item())\n",
    "        buf_value_loss_raw.append(value_loss_raw_t.item())\n",
    "        buf_policy_loss_raw.append(policy_loss_raw_t.item())\n",
    "\n",
    "        if config.train_report_batches is not None and step_idx % config.train_report_batches == 0:\n",
    "            m_policy_loss = np.mean(buf_policy_loss)\n",
    "            m_value_loss = np.mean(buf_value_loss)\n",
    "            m_loss = np.mean(buf_loss)\n",
    "            buf_value_loss.clear()\n",
    "            buf_policy_loss.clear()\n",
    "            buf_loss.clear()\n",
    "\n",
    "            m_policy_loss_raw = np.mean(buf_policy_loss_raw)\n",
    "            m_value_loss_raw = np.mean(buf_value_loss_raw)\n",
    "            m_loss_raw = np.mean(buf_loss_raw)\n",
    "            buf_value_loss_raw.clear()\n",
    "            buf_policy_loss_raw.clear()\n",
    "            buf_loss_raw.clear()\n",
    "\n",
    "            m_values = np.mean(buf_mean_values)\n",
    "            buf_mean_values.clear()\n",
    "\n",
    "            dt = time.time() - ts\n",
    "            ts = time.time()\n",
    "            speed = config.train_batch_size * config.train_report_batches / dt\n",
    "            log.info(\"%d: p_loss=%.3e, v_loss=%.3e, loss=%.3e, speed=%.1f cubes/s\",\n",
    "                     step_idx, m_policy_loss, m_value_loss, m_loss, speed)\n",
    "            sum_train_data = 0.0\n",
    "            sum_opt = 0.0\n",
    "            writer.add_scalar(\"loss_policy\", m_policy_loss, step_idx)\n",
    "            writer.add_scalar(\"loss_value\", m_value_loss, step_idx)\n",
    "            writer.add_scalar(\"loss\", m_loss, step_idx)\n",
    "            writer.add_scalar(\"loss_policy_raw\", m_policy_loss_raw, step_idx)\n",
    "            writer.add_scalar(\"loss_value_raw\", m_value_loss_raw, step_idx)\n",
    "            writer.add_scalar(\"loss_raw\", m_loss_raw, step_idx)\n",
    "            writer.add_scalar(\"values\", m_values, step_idx)\n",
    "            writer.add_scalar(\"speed\", speed, step_idx)\n",
    "\n",
    "            if best_loss is None:\n",
    "                best_loss = m_loss\n",
    "            elif best_loss > m_loss:\n",
    "                name = os.path.join(save_path, \"best_%.4e.dat\" % m_loss)\n",
    "                torch.save(net.state_dict(), name)\n",
    "                best_loss = m_loss\n",
    "\n",
    "        if step_idx % config.push_scramble_buffer_iters == 0:\n",
    "            scramble_buf.extend(model.make_scramble_buffer(cube_env, config.train_batch_size,\n",
    "                                                           config.train_scramble_depth))\n",
    "            log.info(\"Pushed new data in scramble buffer, new size = %d\", len(scramble_buf))\n",
    "\n",
    "        if config.train_checkpoint_batches is not None and step_idx % config.train_checkpoint_batches == 0:\n",
    "            name = os.path.join(save_path, \"chpt_%06d.dat\" % step_idx)\n",
    "            torch.save(net.state_dict(), name)\n",
    "\n",
    "        if config.train_max_batches is not None and config.train_max_batches <= step_idx:\n",
    "            log.info(\"Limit of train batches reached, exiting\")\n",
    "            break\n",
    "\n",
    "    writer.close()"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "executable": "/usr/bin/env python3",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
