{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21b43c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import gymnasium as gym\n",
    "import argparse\n",
    "import numpy as np\n",
    "from torch.utils.tensorboard.writer import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "810593cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib import wob, model, common, demos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ed8180f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ptan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcd9e8d3",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.utils as nn_utils\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10bf81ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "ENVS_COUNT = 8\n",
    "ENV_NAME = \"miniwob/click-button-v1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49d4581f",
   "metadata": {},
   "outputs": [],
   "source": [
    "GAMMA = 0.99\n",
    "REWARD_STEPS = 2\n",
    "BATCH_SIZE = 64\n",
    "LEARNING_RATE = 0.0001\n",
    "ENTROPY_BETA = 0.001\n",
    "CLIP_GRAD = 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad710f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# probability to add demo samples to the batch\n",
    "DEMO_PROB = 0.5\n",
    "# For how many initial frames we train on demo batch\n",
    "DEMO_FRAMES = 25000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0695822e",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "SAVES_DIR = \"saves\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e607215",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"-n\", \"--name\", required=True,\n",
    "                        help=\"Name of the run\")\n",
    "    parser.add_argument(\"--dev\", default=\"cpu\",\n",
    "                        help=\"Device to use, default=cpu\")\n",
    "    parser.add_argument(\"--env\", default=ENV_NAME,\n",
    "                        help=\"Environment name to solve, \"\n",
    "                             \"default=\" + ENV_NAME)\n",
    "    parser.add_argument(\"--demo\", help=\"Demo dir to load, \"\n",
    "                                       \"default=no demo\")\n",
    "    args = parser.parse_args()\n",
    "    device = torch.device(args.dev)\n",
    "\n",
    "    env_name = args.env\n",
    "    if not env_name.startswith('miniwob/'):\n",
    "        env_name = \"miniwob/\" + env_name\n",
    "\n",
    "    demo_samples = None\n",
    "    if args.demo:\n",
    "        demo_samples = demos.load_demo_dir(\n",
    "            args.demo, gamma=GAMMA, steps=REWARD_STEPS,\n",
    "            keep_text=True)\n",
    "        print(f\"Loaded {len(demo_samples)} demo samples\")\n",
    "\n",
    "    name = env_name.split('.')[-1] + \"_\" + args.name\n",
    "    writer = SummaryWriter(comment=\"-wob_click_mm_\" + name)\n",
    "    saves_path = os.path.join(SAVES_DIR, name)\n",
    "    os.makedirs(saves_path, exist_ok=True)\n",
    "\n",
    "    envs = [\n",
    "        lambda: wob.MiniWoBClickWrapper.create(env_name, keep_text=True)\n",
    "        for _ in range(ENVS_COUNT)\n",
    "    ]\n",
    "    env = gym.vector.AsyncVectorEnv(envs, shared_memory=False)\n",
    "\n",
    "    net = model.ModelMultimodal(\n",
    "        input_shape=wob.WOB_SHAPE,\n",
    "        n_actions=env.single_action_space.n\n",
    "    ).to(device)\n",
    "    print(net)\n",
    "    optimizer = optim.Adam(net.parameters(), lr=LEARNING_RATE, eps=1e-3)\n",
    "\n",
    "    preprocessor = model.MultimodalPreprocessor(device=device)\n",
    "    agent = ptan.agent.PolicyAgent(\n",
    "        lambda x: net(x)[0], device=device,\n",
    "        apply_softmax=True, preprocessor=preprocessor)\n",
    "    exp_source = ptan.experience.VectorExperienceSourceFirstLast(\n",
    "        env, agent, gamma=GAMMA, steps_count=REWARD_STEPS)\n",
    "\n",
    "    best_reward = None\n",
    "    with common.RewardTracker(writer) as tracker:\n",
    "        with ptan.common.utils.TBMeanTracker(writer, batch_size=10) as tb_tracker:\n",
    "            batch = []\n",
    "            for step_idx, exp in enumerate(exp_source):\n",
    "                rewards_steps = exp_source.pop_rewards_steps()\n",
    "                if rewards_steps:\n",
    "                    rewards, steps = zip(*rewards_steps)\n",
    "                    tb_tracker.track(\"episode_steps\", np.mean(steps), step_idx)\n",
    "\n",
    "                    mean_reward = tracker.reward(np.mean(rewards), step_idx)\n",
    "                    if mean_reward is not None:\n",
    "                        if best_reward is None or mean_reward > best_reward:\n",
    "                            if best_reward is not None:\n",
    "                                name = \"best_%.3f_%d\" % (mean_reward, step_idx)\n",
    "                                fname = os.path.join(saves_path, name)\n",
    "                                torch.save(net.state_dict(), fname + \".dat\")\n",
    "                                preprocessor.save(fname + \".pre\")\n",
    "                                print(\"Best reward updated: %.3f -> %.3f\" % (best_reward, mean_reward))\n",
    "                            best_reward = mean_reward\n",
    "                batch.append(exp)\n",
    "                if len(batch) < BATCH_SIZE:\n",
    "                    continue\n",
    "\n",
    "                if demo_samples and step_idx < DEMO_FRAMES :\n",
    "                    if random.random() < DEMO_PROB:\n",
    "                        random.shuffle(demo_samples)\n",
    "                        demo_batch = demo_samples[:BATCH_SIZE]\n",
    "                        model.train_demo(\n",
    "                            net, optimizer, demo_batch, writer, step_idx, device=device,\n",
    "                            preprocessor=preprocessor\n",
    "                        )\n",
    "\n",
    "                states_v, actions_t, vals_ref_v = \\\n",
    "                    common.unpack_batch(\n",
    "                        batch, net, last_val_gamma=GAMMA ** REWARD_STEPS,\n",
    "                        device=device, states_preprocessor=preprocessor)\n",
    "                batch.clear()\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                logits_v, value_v = net(states_v)\n",
    "\n",
    "                loss_value_v = F.mse_loss(value_v.squeeze(-1), vals_ref_v)\n",
    "\n",
    "                log_prob_v = F.log_softmax(logits_v, dim=1)\n",
    "                adv_v = vals_ref_v - value_v.detach()\n",
    "                log_prob_actions_v = adv_v * log_prob_v[range(BATCH_SIZE), actions_t]\n",
    "                loss_policy_v = -log_prob_actions_v.mean()\n",
    "\n",
    "                prob_v = F.softmax(logits_v, dim=1)\n",
    "                entropy_loss_v = ENTROPY_BETA * (prob_v * log_prob_v).sum(dim=1).mean()\n",
    "\n",
    "                loss_v = entropy_loss_v + loss_value_v + loss_policy_v\n",
    "                loss_v.backward()\n",
    "                nn_utils.clip_grad_norm_(net.parameters(), CLIP_GRAD)\n",
    "                optimizer.step()\n",
    "\n",
    "                tb_tracker.track(\"advantage\", adv_v, step_idx)\n",
    "                tb_tracker.track(\"values\", value_v, step_idx)\n",
    "                tb_tracker.track(\"batch_rewards\", vals_ref_v, step_idx)\n",
    "                tb_tracker.track(\"loss_entropy\", entropy_loss_v, step_idx)\n",
    "                tb_tracker.track(\"loss_policy\", loss_policy_v, step_idx)\n",
    "                tb_tracker.track(\"loss_value\", loss_value_v, step_idx)\n",
    "                tb_tracker.track(\"loss_total\", loss_v, step_idx)\n",
    "                tb_tracker.track(\"dict_size\", len(preprocessor), step_idx)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "executable": "/usr/bin/env python3",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
