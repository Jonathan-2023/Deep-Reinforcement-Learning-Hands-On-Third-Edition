{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "771138d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import time\n",
    "import typing as tt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "740d6b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.tensorboard.writer import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efdb6c9b",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "import ptan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "224fca9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RewardTracker:\n",
    "    def __init__(self, writer: SummaryWriter):\n",
    "        self.writer = writer\n",
    "\n",
    "    def __enter__(self):\n",
    "        self.ts = time.time()\n",
    "        self.ts_frame = 0\n",
    "        self.total_rewards = []\n",
    "        return self\n",
    "\n",
    "    def __exit__(self, *args):\n",
    "        self.writer.close()\n",
    "\n",
    "    def reward(self, reward: float, frame: int,\n",
    "               epsilon: tt.Optional[float] = None):\n",
    "        self.total_rewards.append(reward)\n",
    "        speed = (frame - self.ts_frame) / (time.time() - self.ts)\n",
    "        self.ts_frame = frame\n",
    "        self.ts = time.time()\n",
    "        mean_reward = np.mean(self.total_rewards[-100:])\n",
    "        epsilon_str = \"\"\n",
    "        if epsilon is not None:\n",
    "            epsilon_str = f\", eps {epsilon:.2f}\"\n",
    "        print(f\"{frame}: done {len(self.total_rewards)} games, \"\n",
    "              f\"mean reward {mean_reward:.3f}, \"\n",
    "              f\"speed {speed:.2f} f/s{epsilon_str}\")\n",
    "        sys.stdout.flush()\n",
    "        if epsilon is not None:\n",
    "            self.writer.add_scalar(\"epsilon\", epsilon, frame)\n",
    "        self.writer.add_scalar(\"speed\", speed, frame)\n",
    "        self.writer.add_scalar(\"reward_100\", mean_reward, frame)\n",
    "        self.writer.add_scalar(\"reward\", reward, frame)\n",
    "        return mean_reward if len(self.total_rewards) > 30 else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4ea08f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unpack_batch(batch, net, last_val_gamma, device=\"cpu\", states_preprocessor=ptan.agent.default_states_preprocessor):\n",
    "    \"\"\"\n",
    "    Convert batch into training tensors\n",
    "    :param batch:\n",
    "    :param net:\n",
    "    :return: states variable, actions tensor, reference values variable\n",
    "    \"\"\"\n",
    "    states = []\n",
    "    actions = []\n",
    "    rewards = []\n",
    "    not_done_idx = []\n",
    "    last_states = []\n",
    "    for idx, exp in enumerate(batch):\n",
    "        states.append(exp.state)\n",
    "        actions.append(int(exp.action))\n",
    "        rewards.append(exp.reward)\n",
    "        if exp.last_state is not None:\n",
    "            not_done_idx.append(idx)\n",
    "            last_states.append(exp.last_state)\n",
    "    states_v = states_preprocessor(states)\n",
    "    if torch.is_tensor(states_v):\n",
    "        states_v = states_v.to(device)\n",
    "    actions_t = torch.LongTensor(actions).to(device)\n",
    "\n",
    "    # handle rewards\n",
    "    rewards_np = np.array(rewards, dtype=np.float32)\n",
    "    if not_done_idx:\n",
    "        last_states_v = states_preprocessor(last_states)\n",
    "        if torch.is_tensor(last_states_v):\n",
    "            last_states_v = last_states_v.to(device)\n",
    "        last_vals_v = net(last_states_v)[1]\n",
    "        last_vals_np = last_vals_v.data.cpu().numpy()[:, 0]\n",
    "        rewards_np[not_done_idx] += last_val_gamma * last_vals_np\n",
    "\n",
    "    ref_vals_v = torch.FloatTensor(rewards_np).to(device)\n",
    "    return states_v, actions_t, ref_vals_v"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
