{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb75120f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ptan\n",
    "import logging\n",
    "import pickle\n",
    "import numpy as np\n",
    "import typing as tt\n",
    "from nltk.tokenize import TweetTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b67c7247",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.utils.rnn as rnn_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a25afa25",
   "metadata": {},
   "outputs": [],
   "source": [
    "MM_EMBEDDINGS_DIM = 50\n",
    "MM_HIDDEN_SIZE = 128\n",
    "MM_MAX_DICT_SIZE = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2023b10e",
   "metadata": {},
   "outputs": [],
   "source": [
    "TOKEN_UNK = \"#unk\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f94eb38",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, input_shape: tt.Tuple[int, ...], n_actions: int):\n",
    "        super(Model, self).__init__()\n",
    "\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(input_shape[0], 64, 5, stride=5),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, 3, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "        )\n",
    "        size = self.conv(torch.zeros(1, *input_shape)).size()[-1]\n",
    "        self.policy = nn.Linear(size, n_actions)\n",
    "        self.value = nn.Linear(size, 1)\n",
    "\n",
    "    def forward(self, x: torch.ByteTensor) -> tt.Tuple[torch.Tensor, torch.Tensor]:\n",
    "        xx = x / 255.0\n",
    "        conv_out = self.conv(xx)\n",
    "        return self.policy(conv_out), self.value(conv_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9bd7f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelMultimodal(nn.Module):\n",
    "    def __init__(self, input_shape: tt.Tuple[int, ...], n_actions: int,\n",
    "                 max_dict_size: int = MM_MAX_DICT_SIZE):\n",
    "        super(ModelMultimodal, self).__init__()\n",
    "\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(input_shape[0], 64, 5, stride=5),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, 3, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "        )\n",
    "        size = self.conv(torch.zeros(1, *input_shape)).size()[-1]\n",
    "\n",
    "        self.emb = nn.Embedding(max_dict_size, MM_EMBEDDINGS_DIM)\n",
    "        self.rnn = nn.LSTM(MM_EMBEDDINGS_DIM, MM_HIDDEN_SIZE, batch_first=True)\n",
    "        self.policy = nn.Linear(size + MM_HIDDEN_SIZE*2, n_actions)\n",
    "        self.value = nn.Linear(size + MM_HIDDEN_SIZE*2, 1)\n",
    "\n",
    "    def _concat_features(self, img_out: torch.Tensor,\n",
    "                         rnn_hidden: torch.Tensor | tt.Tuple[torch.Tensor, ...]):\n",
    "        batch_size = img_out.size()[0]\n",
    "        if isinstance(rnn_hidden, tuple):\n",
    "            flat_h = list(map(lambda t: t.view(batch_size, -1), rnn_hidden))\n",
    "            rnn_h = torch.cat(flat_h, dim=1)\n",
    "        else:\n",
    "            rnn_h = rnn_hidden.view(batch_size, -1)\n",
    "        return torch.cat((img_out, rnn_h), dim=1)\n",
    "\n",
    "    def forward(self, x: tt.Tuple[torch.Tensor, rnn_utils.PackedSequence]):\n",
    "        x_img, x_text = x\n",
    "\n",
    "        # deal with text data\n",
    "        emb_out = self.emb(x_text.data)\n",
    "        emb_out_seq = rnn_utils.PackedSequence(emb_out, x_text.batch_sizes)\n",
    "        rnn_out, rnn_h = self.rnn(emb_out_seq)\n",
    "\n",
    "        # extract image features\n",
    "        xx = x_img / 255.0\n",
    "        conv_out = self.conv(xx)\n",
    "        feats = self._concat_features(conv_out, rnn_h)\n",
    "        return self.policy(feats), self.value(feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a10e00b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultimodalPreprocessor:\n",
    "    log = logging.getLogger(\"MulitmodalPreprocessor\")\n",
    "\n",
    "    def __init__(self, max_dict_size: int = MM_MAX_DICT_SIZE,\n",
    "                 device: torch.device = torch.device('cpu')):\n",
    "        self.max_dict_size = max_dict_size\n",
    "        self.token_to_id = {TOKEN_UNK: 0}\n",
    "        self.next_id = 1\n",
    "        self.tokenizer = TweetTokenizer(preserve_case=True)\n",
    "        self.device = device\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.token_to_id)\n",
    "\n",
    "    def __call__(self, batch: tt.Tuple[tt.Any, ...] | tt.List[tt.Tuple[tt.Any, ...]]):\n",
    "        \"\"\"\n",
    "        Convert list of multimodel observations (tuples with image and text string) into the form suitable\n",
    "        for ModelMultimodal to disgest\n",
    "        :param batch: either tuple of vectorized observations or list of individual observations\n",
    "        \"\"\"\n",
    "        tokens_batch = []\n",
    "\n",
    "        if isinstance(batch, tuple):\n",
    "            batch_iter = zip(*batch)\n",
    "        else:\n",
    "            batch_iter = batch\n",
    "        for img_obs, txt_obs in batch_iter:\n",
    "            tokens = self.tokenizer.tokenize(txt_obs)\n",
    "            idx_obs = self.tokens_to_idx(tokens)\n",
    "            tokens_batch.append((img_obs, idx_obs))\n",
    "        # sort batch decreasing to seq len\n",
    "        tokens_batch.sort(key=lambda p: len(p[1]), reverse=True)\n",
    "        img_batch, seq_batch = zip(*tokens_batch)\n",
    "        lens = list(map(len, seq_batch))\n",
    "\n",
    "        # convert data into the target form\n",
    "        # images\n",
    "        img_v = torch.FloatTensor(np.asarray(img_batch)).to(self.device)\n",
    "        # sequences\n",
    "        seq_arr = np.zeros(\n",
    "            shape=(len(seq_batch), max(len(seq_batch[0]), 1)), dtype=np.int64)\n",
    "        for idx, seq in enumerate(seq_batch):\n",
    "            seq_arr[idx, :len(seq)] = seq\n",
    "            # Map empty sequences into single #UNK token\n",
    "            if len(seq) == 0:\n",
    "                lens[idx] = 1\n",
    "        seq_v = torch.LongTensor(seq_arr).to(self.device)\n",
    "        seq_p = rnn_utils.pack_padded_sequence(seq_v, lens, batch_first=True)\n",
    "        return img_v, seq_p\n",
    "\n",
    "    def tokens_to_idx(self, tokens):\n",
    "        res = []\n",
    "        for token in tokens:\n",
    "            idx = self.token_to_id.get(token)\n",
    "            if idx is None:\n",
    "                if self.next_id == self.max_dict_size:\n",
    "                    self.log.warning(\"Maximum size of dict reached, token \"\n",
    "                                     \"'%s' converted to #UNK token\", token)\n",
    "                    idx = 0\n",
    "                else:\n",
    "                    idx = self.next_id\n",
    "                    self.next_id += 1\n",
    "                    self.token_to_id[token] = idx\n",
    "            res.append(idx)\n",
    "        return res\n",
    "\n",
    "    def save(self, file_name):\n",
    "        with open(file_name, 'wb') as fd:\n",
    "            pickle.dump(self.token_to_id, fd)\n",
    "            pickle.dump(self.max_dict_size, fd)\n",
    "            pickle.dump(self.next_id, fd)\n",
    "\n",
    "    @classmethod\n",
    "    def load(cls, file_name):\n",
    "        with open(file_name, \"rb\") as fd:\n",
    "            token_to_id = pickle.load(fd)\n",
    "            max_dict_size = pickle.load(fd)\n",
    "            next_id = pickle.load(fd)\n",
    "\n",
    "            res = MultimodalPreprocessor(max_dict_size)\n",
    "            res.token_to_id = token_to_id\n",
    "            res.next_id = next_id\n",
    "            return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b68eb33",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "def train_demo(net: Model, optimizer: torch.optim.Optimizer,\n",
    "               batch: tt.List[ptan.experience.ExperienceFirstLast], writer, step_idx: int,\n",
    "               preprocessor=ptan.agent.default_states_preprocessor,\n",
    "               device: torch.device = torch.device(\"cpu\")):\n",
    "    \"\"\"\n",
    "    Train net on demonstration batch\n",
    "    \"\"\"\n",
    "    batch_obs, batch_act = [], []\n",
    "    for e in batch:\n",
    "        batch_obs.append(e.state)\n",
    "        batch_act.append(e.action)\n",
    "    batch_v = preprocessor(batch_obs)\n",
    "    if torch.is_tensor(batch_v):\n",
    "        batch_v = batch_v.to(device)\n",
    "    optimizer.zero_grad()\n",
    "    ref_actions_v = torch.LongTensor(batch_act).to(device)\n",
    "    policy_v = net(batch_v)[0]\n",
    "    loss_v = F.cross_entropy(policy_v, ref_actions_v)\n",
    "    loss_v.backward()\n",
    "    optimizer.step()\n",
    "    writer.add_scalar(\"demo_loss\", loss_v.item(), step_idx)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
