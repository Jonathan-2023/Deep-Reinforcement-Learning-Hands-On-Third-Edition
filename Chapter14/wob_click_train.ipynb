{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0b668f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pathlib\n",
    "import random\n",
    "import argparse\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "from torch.utils.tensorboard.writer import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3585b899",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib import wob, model, common, demos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e969b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ptan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb5b5726",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.utils as nn_utils\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae71bb28",
   "metadata": {},
   "outputs": [],
   "source": [
    "ENVS_COUNT = 8\n",
    "ENV_NAME = 'miniwob/click-dialog-v1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9250d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "GAMMA = 0.99\n",
    "REWARD_STEPS = 2\n",
    "BATCH_SIZE = 16\n",
    "LEARNING_RATE = 0.0001\n",
    "ENTROPY_BETA = 0.001\n",
    "CLIP_GRAD = 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebe86daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVES_DIR = \"saves\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2011545e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# probability to add demo samples to the batch\n",
    "DEMO_PROB = 0.5\n",
    "# For how many initial frames we train on demo batch\n",
    "DEMO_FRAMES = 25000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5b5c690",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "DUMP_INTERVAL = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "042c4f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"-n\", \"--name\", required=True,\n",
    "                        help=\"Name of the run\")\n",
    "    parser.add_argument(\"--dev\", default=\"cpu\",\n",
    "                        help=\"Device to use, default=cpu\")\n",
    "    parser.add_argument(\"--env\", default=ENV_NAME,\n",
    "                        help=\"Environment name to solve, \"\n",
    "                             \"default=\" + ENV_NAME)\n",
    "    parser.add_argument(\"--demo\", help=\"Demo dir to load, \"\n",
    "                                       \"default=no demo\")\n",
    "    parser.add_argument(\"--dump\",\n",
    "                        help=\"If given save models and screenshots \"\n",
    "                             \"every 1k training steps using this dir\")\n",
    "    args = parser.parse_args()\n",
    "    device = torch.device(args.dev)\n",
    "\n",
    "    env_name = args.env\n",
    "    if not env_name.startswith('miniwob/'):\n",
    "        env_name = \"miniwob/\" + env_name\n",
    "\n",
    "    demo_samples = None\n",
    "    if args.demo:\n",
    "        demo_samples = demos.load_demo_dir(args.demo, gamma=GAMMA, steps=REWARD_STEPS)\n",
    "        print(f\"Loaded {len(demo_samples)} demo samples\")\n",
    "\n",
    "    dump_dir = None\n",
    "    if args.dump is not None:\n",
    "        dump_dir = pathlib.Path(args.dump)\n",
    "        dump_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    name = env_name.split('/')[-1] + \"_\" + args.name\n",
    "    writer = SummaryWriter(comment=\"-wob_click_\" + name)\n",
    "    saves_path = os.path.join(SAVES_DIR, name)\n",
    "    os.makedirs(saves_path, exist_ok=True)\n",
    "\n",
    "    envs = [\n",
    "        lambda: wob.MiniWoBClickWrapper.create(env_name)\n",
    "        for _ in range(ENVS_COUNT)\n",
    "    ]\n",
    "    env = gym.vector.AsyncVectorEnv(envs)\n",
    "\n",
    "    net = model.Model(input_shape=wob.WOB_SHAPE,\n",
    "                      n_actions=env.single_action_space.n).to(device)\n",
    "    print(net)\n",
    "    optimizer = optim.Adam(net.parameters(),\n",
    "                           lr=LEARNING_RATE, eps=1e-3)\n",
    "\n",
    "    agent = ptan.agent.PolicyAgent(\n",
    "        lambda x: net(x)[0], device=device, apply_softmax=True)\n",
    "    exp_source = ptan.experience.VectorExperienceSourceFirstLast(\n",
    "        env, agent, gamma=GAMMA, steps_count=REWARD_STEPS)\n",
    "\n",
    "    best_reward = None\n",
    "    with common.RewardTracker(writer) as tracker:\n",
    "        with ptan.common.utils.TBMeanTracker(\n",
    "                writer, batch_size=10) as tb_tracker:\n",
    "            batch = []\n",
    "            for step_idx, exp in enumerate(exp_source):\n",
    "                rewards_steps = exp_source.pop_rewards_steps()\n",
    "                if rewards_steps:\n",
    "                    rewards, steps = zip(*rewards_steps)\n",
    "                    tb_tracker.track(\"episode_steps\",\n",
    "                                     np.mean(steps), step_idx)\n",
    "\n",
    "                    mean_reward = tracker.reward(\n",
    "                        float(np.mean(rewards)), step_idx)\n",
    "                    if mean_reward is not None:\n",
    "                        if best_reward is None or \\\n",
    "                                mean_reward > best_reward:\n",
    "                            if best_reward is not None:\n",
    "                                name = \"best_%.3f_%d.dat\" % (\n",
    "                                    mean_reward, step_idx)\n",
    "                                fname = os.path.join(\n",
    "                                    saves_path, name)\n",
    "                                torch.save(net.state_dict(), fname)\n",
    "                                print(\"Best reward updated: %.3f \"\n",
    "                                      \"-> %.3f\" % (\n",
    "                                    best_reward, mean_reward))\n",
    "                            best_reward = mean_reward\n",
    "                batch.append(exp)\n",
    "                if dump_dir is not None and step_idx % DUMP_INTERVAL == 0:\n",
    "                    print(\"Dumping model and screenshots from the batch\")\n",
    "                    p = dump_dir / f\"model_{step_idx:06d}.dat\"\n",
    "                    torch.save(net.state_dict(), str(p))\n",
    "                    p = dump_dir / f\"scr_{step_idx:06d}_act={exp.action}_r={exp.reward:.2f}.png\"\n",
    "                    demos.save_obs_image(exp.state, exp.action, str(p))\n",
    "                if len(batch) < BATCH_SIZE:\n",
    "                    continue\n",
    "\n",
    "                if demo_samples and step_idx < DEMO_FRAMES:\n",
    "                    if random.random() < DEMO_PROB:\n",
    "                        random.shuffle(demo_samples)\n",
    "                        demo_batch = demo_samples[:BATCH_SIZE]\n",
    "                        model.train_demo(net, optimizer, demo_batch, writer,\n",
    "                                         step_idx, device=device)\n",
    "\n",
    "                states_v, actions_t, vals_ref_v = common.unpack_batch(\n",
    "                    batch, net, device=device, last_val_gamma=GAMMA ** REWARD_STEPS)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                logits_v, value_v = net(states_v)\n",
    "\n",
    "                loss_value_v = F.mse_loss(\n",
    "                    value_v.squeeze(-1), vals_ref_v)\n",
    "\n",
    "                log_prob_v = F.log_softmax(logits_v, dim=1)\n",
    "                adv_v = vals_ref_v - value_v.detach()\n",
    "                lpa = log_prob_v[range(len(batch)), actions_t]\n",
    "                log_prob_actions_v = adv_v * lpa\n",
    "                loss_policy_v = -log_prob_actions_v.mean()\n",
    "\n",
    "                prob_v = F.softmax(logits_v, dim=1)\n",
    "                ent_v = prob_v * log_prob_v\n",
    "                entropy_loss_v = ENTROPY_BETA * ent_v\n",
    "                entropy_loss_v = entropy_loss_v.sum(dim=1).mean()\n",
    "\n",
    "                loss_v = loss_policy_v + entropy_loss_v + \\\n",
    "                         loss_value_v\n",
    "                loss_v.backward()\n",
    "                nn_utils.clip_grad_norm_(\n",
    "                    net.parameters(), CLIP_GRAD)\n",
    "                optimizer.step()\n",
    "                batch.clear()\n",
    "\n",
    "                tb_tracker.track(\"advantage\", adv_v, step_idx)\n",
    "                tb_tracker.track(\"values\", value_v, step_idx)\n",
    "                tb_tracker.track(\"batch_rewards\", vals_ref_v,\n",
    "                                 step_idx)\n",
    "                tb_tracker.track(\"loss_entropy\", entropy_loss_v,\n",
    "                                 step_idx)\n",
    "                tb_tracker.track(\"loss_policy\", loss_policy_v,\n",
    "                                 step_idx)\n",
    "                tb_tracker.track(\"loss_value\", loss_value_v,\n",
    "                                 step_idx)\n",
    "                tb_tracker.track(\"loss_total\", loss_v, step_idx)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "executable": "/usr/bin/env python3",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
