{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5946b652",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb3730f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ptan\n",
    "import torch\n",
    "import argparse\n",
    "from typing import Tuple, List\n",
    "import ptan.ignite as ptan_ignite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0543d6b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "from types import SimpleNamespace\n",
    "from lib import data, model, common\n",
    "from ignite.engine import Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46f77e05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# As both deer and tigers are in the same replay buffer, need to increase the sampled batch\n",
    "TRAIN_BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbc16afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "PARAMS = SimpleNamespace(**{\n",
    "    'run_name':         'tigers-deers',\n",
    "    'stop_reward':      None,\n",
    "    'replay_size':      3000000,\n",
    "    'replay_initial':   300,\n",
    "    'target_net_sync':  1000,\n",
    "    'epsilon_frames':   5*10**5,\n",
    "    'epsilon_start':    1.0,\n",
    "    'epsilon_final':    0.02,\n",
    "    'learning_rate':    1e-4,\n",
    "    'gamma':            0.99,\n",
    "    'batch_size':       TRAIN_BATCH_SIZE * 10\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb6b81e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_env(args: argparse.Namespace) -> data.magent_parallel_env:\n",
    "    if args.mode == \"forest\":\n",
    "        env = data.ForestEnv(\n",
    "            map_size=args.map_size,\n",
    "            count_walls=args.walls,\n",
    "            count_tigers=args.tigers,\n",
    "            count_deer=args.deer,\n",
    "        )\n",
    "    elif args.mode == 'double_attack':\n",
    "        env = data.DoubleAttackEnv(\n",
    "            map_size=args.map_size,\n",
    "            count_walls=args.walls,\n",
    "            count_tigers=args.tigers,\n",
    "            count_deer=args.deer,\n",
    "        )\n",
    "    else:\n",
    "        raise RuntimeError(\"Wrong mode\")\n",
    "    return env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1884fa18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(tiger_net: model.DQNModel, deer_net: model.DQNModel, device: torch.device, args: argparse.Namespace) -> Tuple[float, float, int]:\n",
    "    env = make_env(args)\n",
    "    tiger_agent = ptan.agent.DQNAgent(\n",
    "        tiger_net, ptan.actions.ArgmaxActionSelector(),\n",
    "        device)\n",
    "    deer_agent = ptan.agent.DQNAgent(\n",
    "        deer_net, ptan.actions.ArgmaxActionSelector(),\n",
    "        device)\n",
    "\n",
    "    obs = env.reset()\n",
    "    total_steps = 0\n",
    "    tiger_rewards = 0.0\n",
    "    deer_rewards = 0.0\n",
    "\n",
    "    while env.agents:\n",
    "        actions = {}\n",
    "        tiger_obs = [\n",
    "            obs[agent_id]\n",
    "            for agent_id in env.agents\n",
    "            if agent_id.startswith(\"tiger\")\n",
    "        ]\n",
    "        tiger_acts, _ = tiger_agent(tiger_obs)\n",
    "        ofs = 0\n",
    "        for agent_id in env.agents:\n",
    "            if agent_id.startswith(\"tiger\"):\n",
    "                actions[agent_id] = tiger_acts[ofs]\n",
    "                ofs += 1\n",
    "\n",
    "        deer_obs = [\n",
    "            obs[agent_id]\n",
    "            for agent_id in env.agents\n",
    "            if agent_id.startswith(\"deer\")\n",
    "        ]\n",
    "        deer_acts, _ = deer_agent(deer_obs)\n",
    "        ofs = 0\n",
    "        for agent_id in env.agents:\n",
    "            if agent_id.startswith(\"deer\"):\n",
    "                actions[agent_id] = deer_acts[ofs]\n",
    "                ofs += 1\n",
    "\n",
    "        obs, rewards, dones, _, _ = env.step(actions)\n",
    "        total_steps += 1\n",
    "        for agent_id, reward in rewards.items():\n",
    "            if agent_id.startswith(\"tiger\"):\n",
    "                tiger_rewards += reward\n",
    "            elif agent_id.startswith(\"deer\"):\n",
    "                deer_rewards += reward\n",
    "\n",
    "    return tiger_rewards / env.count_tigers, deer_rewards / env.count_deer, total_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a014828c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_batch(batch: List[data.ExperienceFirstLastMARL],\n",
    "                 group: str, batch_size: int) -> List[data.ExperienceFirstLastMARL]:\n",
    "    res = []\n",
    "    for sample in batch:\n",
    "        if sample.group != group:\n",
    "            continue\n",
    "        res.append(sample)\n",
    "        if len(res) == batch_size:\n",
    "            break\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59f6b386",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--dev\", default=\"cpu\", help=\"Device to train\")\n",
    "    parser.add_argument(\"-n\", \"--name\", required=True, help=\"Run name\")\n",
    "    parser.add_argument(\"--mode\", default='forest', choices=['forest', 'double_attack'],\n",
    "                        help=\"GridWorld mode, could be 'forest' or 'double_attack' default='forest'\")\n",
    "    parser.add_argument(\"--map-size\", type=int, default=data.MAP_SIZE,\n",
    "                        help=\"Size of the map, default=\" + str(data.MAP_SIZE))\n",
    "    parser.add_argument(\"--walls\", type=int, default=data.COUNT_WALLS,\n",
    "                        help=\"Count of walls, default=\" + str(data.COUNT_WALLS))\n",
    "    parser.add_argument(\"--tigers\", type=int, default=data.COUNT_TIGERS,\n",
    "                        help=\"Count of tigers, default=\" + str(data.COUNT_TIGERS))\n",
    "    parser.add_argument(\"--deer\", type=int, default=data.COUNT_DEER,\n",
    "                        help=\"Count of deer, default=\" + str(data.COUNT_DEER))\n",
    "    args = parser.parse_args()\n",
    "    device = torch.device(args.dev)\n",
    "\n",
    "    env = make_env(args)\n",
    "    saves_path = os.path.join(\"saves\", args.name)\n",
    "    os.makedirs(saves_path, exist_ok=True)\n",
    "\n",
    "    tiger_net = model.DQNModel(\n",
    "        env.observation_spaces['tiger_0'].shape,\n",
    "        env.action_spaces['tiger_0'].n,\n",
    "    ).to(device)\n",
    "    tiger_tgt_net = ptan.agent.TargetNet(tiger_net)\n",
    "    print(tiger_net)\n",
    "\n",
    "    deer_net = model.DQNModel(\n",
    "        env.observation_spaces['deer_0'].shape,\n",
    "        env.action_spaces['deer_0'].n,\n",
    "    ).to(device)\n",
    "    deer_tgt_net = ptan.agent.TargetNet(deer_net)\n",
    "\n",
    "    action_selector = ptan.actions.EpsilonGreedyActionSelector(\n",
    "        epsilon=PARAMS.epsilon_start)\n",
    "    epsilon_tracker = common.EpsilonTracker(action_selector, PARAMS)\n",
    "    tiger_agent = ptan.agent.DQNAgent(tiger_net, action_selector, device)\n",
    "    deer_agent = ptan.agent.DQNAgent(deer_net, action_selector, device)\n",
    "    exp_source = data.MAgentExperienceSourceFirstLast(\n",
    "        env,\n",
    "        agents_by_group={\n",
    "            'deer': deer_agent, 'tiger': tiger_agent\n",
    "        },\n",
    "        track_reward_group=\"tiger\",\n",
    "    )\n",
    "    buffer = ptan.experience.ExperienceReplayBuffer(\n",
    "        exp_source, PARAMS.replay_size)\n",
    "    tiger_optimizer = optim.Adam(tiger_net.parameters(), lr=PARAMS.learning_rate)\n",
    "    deer_optimizer = optim.Adam(deer_net.parameters(), lr=PARAMS.learning_rate)\n",
    "\n",
    "    def process_batch(engine, batch):\n",
    "        res = {}\n",
    "        tiger_batch = filter_batch(batch, group=\"tiger\", batch_size=TRAIN_BATCH_SIZE)\n",
    "        if tiger_batch:\n",
    "            tiger_optimizer.zero_grad()\n",
    "            tiger_loss_v = model.calc_loss_dqn(\n",
    "                tiger_batch, tiger_net, tiger_tgt_net.target_model,\n",
    "                ptan.agent.default_states_preprocessor,\n",
    "                gamma=PARAMS.gamma, device=device)\n",
    "            tiger_loss_v.backward()\n",
    "            tiger_optimizer.step()\n",
    "            res['tiger_loss'] = tiger_loss_v.item()\n",
    "\n",
    "        deer_batch = filter_batch(batch, group=\"deer\", batch_size=TRAIN_BATCH_SIZE)\n",
    "        if deer_batch:\n",
    "            deer_optimizer.zero_grad()\n",
    "            deer_loss_v = model.calc_loss_dqn(\n",
    "                deer_batch, deer_net, deer_tgt_net.target_model,\n",
    "                ptan.agent.default_states_preprocessor,\n",
    "                gamma=PARAMS.gamma, device=device)\n",
    "            deer_loss_v.backward()\n",
    "            deer_optimizer.step()\n",
    "            res['deer_loss'] = deer_loss_v.item()\n",
    "\n",
    "        if epsilon_tracker is not None:\n",
    "            epsilon_tracker.frame(engine.state.iteration)\n",
    "            res['epsilon'] = action_selector.epsilon\n",
    "        if engine.state.iteration % PARAMS.target_net_sync == 0:\n",
    "            tiger_tgt_net.sync()\n",
    "            deer_tgt_net.sync()\n",
    "        return res\n",
    "\n",
    "    engine = Engine(process_batch)\n",
    "    common.setup_ignite(engine, PARAMS, exp_source, args.name,\n",
    "                        extra_metrics=('test_tiger_reward', 'test_deer_reward', 'test_steps'),\n",
    "                        loss_metrics=(\"tiger_loss\", \"deer_loss\"))\n",
    "    best_test_tiger_reward = None\n",
    "    best_test_deer_reward = None\n",
    "\n",
    "    @engine.on(ptan_ignite.PeriodEvents.ITERS_10000_COMPLETED)\n",
    "    def test_network(engine):\n",
    "        tiger_net.train(False)\n",
    "        deer_net.train(False)\n",
    "        tiger_reward, deer_reward, steps = test_model(tiger_net, deer_net, device, args)\n",
    "        tiger_net.train(True)\n",
    "        deer_net.train(True)\n",
    "        engine.state.metrics['test_tiger_reward'] = tiger_reward\n",
    "        engine.state.metrics['test_deer_reward'] = deer_reward\n",
    "        engine.state.metrics['test_steps'] = steps\n",
    "        print(\"Test done: got %.3f tiger and %.3f deer reward after %.2f steps\" % (\n",
    "            tiger_reward, deer_reward, steps\n",
    "        ))\n",
    "\n",
    "        global best_test_tiger_reward, best_test_deer_reward\n",
    "        if best_test_tiger_reward is None:\n",
    "            best_test_tiger_reward = tiger_reward\n",
    "        elif best_test_tiger_reward < tiger_reward:\n",
    "            print(\"Best test tiger reward updated %.3f -> %.3f, save model\" % (\n",
    "                best_test_tiger_reward, tiger_reward\n",
    "            ))\n",
    "            best_test_tiger_reward = tiger_reward\n",
    "            torch.save(tiger_net.state_dict(), os.path.join(saves_path, \"best_tiger_%.3f.dat\" % tiger_reward))\n",
    "\n",
    "        if best_test_deer_reward is None:\n",
    "            best_test_deer_reward = deer_reward\n",
    "        elif best_test_deer_reward < deer_reward:\n",
    "            print(\"Best test deer reward updated %.3f -> %.3f, save model\" % (\n",
    "                best_test_deer_reward, deer_reward\n",
    "            ))\n",
    "            best_test_deer_reward = deer_reward\n",
    "            torch.save(deer_net.state_dict(), os.path.join(saves_path, \"best_deer_%.3f.dat\" % deer_reward))\n",
    "\n",
    "    engine.run(common.batch_generator(buffer, PARAMS.replay_initial,\n",
    "                                      PARAMS.batch_size))"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "executable": "/usr/bin/env python3",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
