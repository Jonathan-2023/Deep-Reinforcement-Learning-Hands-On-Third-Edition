{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d7fd20c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cc178ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ptan\n",
    "import torch\n",
    "import argparse\n",
    "from typing import Tuple\n",
    "import ptan.ignite as ptan_ignite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d68d31b1",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "from types import SimpleNamespace\n",
    "from lib import data, model, common\n",
    "from ignite.engine import Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf536934",
   "metadata": {},
   "outputs": [],
   "source": [
    "PARAMS = SimpleNamespace(**{\n",
    "    'run_name':         'tigers',\n",
    "    'stop_reward':      None,\n",
    "    'replay_size':      1000000, # check 5M\n",
    "    'replay_initial':   100,\n",
    "    'target_net_sync':  1000,\n",
    "    'epsilon_frames':   5*10**5,\n",
    "    'epsilon_start':    1.0,\n",
    "    'epsilon_final':    0.02,\n",
    "    'learning_rate':    1e-4,\n",
    "    'gamma':            0.99,\n",
    "    'batch_size':       32\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1c3236b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_env(args: argparse.Namespace) -> data.magent_parallel_env:\n",
    "    if args.mode == \"forest\":\n",
    "        env = data.ForestEnv(\n",
    "            map_size=args.map_size,\n",
    "            count_walls=args.walls,\n",
    "            count_tigers=args.tigers,\n",
    "            count_deer=args.deer,\n",
    "        )\n",
    "    elif args.mode == 'double_attack':\n",
    "        env = data.DoubleAttackEnv(\n",
    "            map_size=args.map_size,\n",
    "            count_walls=args.walls,\n",
    "            count_tigers=args.tigers,\n",
    "            count_deer=args.deer,\n",
    "        )\n",
    "    else:\n",
    "        raise RuntimeError(\"Wrong mode\")\n",
    "    return env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69c2016b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(net: model.DQNModel, device: torch.device, args: argparse.Namespace) -> Tuple[float, float]:\n",
    "    env = make_env(args)\n",
    "    tiger_agent = ptan.agent.DQNAgent(\n",
    "        net, ptan.actions.ArgmaxActionSelector(),\n",
    "        device)\n",
    "    deer_agent = data.RandomMAgent(env, env.handles[0])\n",
    "\n",
    "    obs = env.reset()\n",
    "    sum_steps = 0\n",
    "    sum_rewards = 0.0\n",
    "\n",
    "    while env.agents:\n",
    "        actions = {}\n",
    "        tiger_obs = [\n",
    "            obs[agent_id]\n",
    "            for agent_id in env.agents\n",
    "            if agent_id.startswith(\"tiger\")\n",
    "        ]\n",
    "        tiger_acts, _ = tiger_agent(tiger_obs)\n",
    "        ofs = 0\n",
    "        for agent_id in env.agents:\n",
    "            if agent_id.startswith(\"tiger\"):\n",
    "                actions[agent_id] = tiger_acts[ofs]\n",
    "                ofs += 1\n",
    "\n",
    "        deer_obs = [\n",
    "            obs[agent_id]\n",
    "            for agent_id in env.agents\n",
    "            if agent_id.startswith(\"deer\")\n",
    "        ]\n",
    "        deer_acts, _ = deer_agent(deer_obs)\n",
    "        ofs = 0\n",
    "        for agent_id in env.agents:\n",
    "            if agent_id.startswith(\"deer\"):\n",
    "                actions[agent_id] = deer_acts[ofs]\n",
    "                ofs += 1\n",
    "\n",
    "        obs, rewards, dones, _, _ = env.step(actions)\n",
    "        sum_steps += 1\n",
    "        for agent_id, reward in rewards.items():\n",
    "            if agent_id.startswith(\"tiger\"):\n",
    "                sum_rewards += reward\n",
    "\n",
    "    return sum_rewards / env.count_tigers, sum_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db17e5f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--dev\", default=\"cpu\", help=\"Device to train\")\n",
    "    parser.add_argument(\"-n\", \"--name\", required=True, help=\"Run name\")\n",
    "    parser.add_argument(\"--mode\", default='forest', choices=['forest', 'double_attack'],\n",
    "                        help=\"GridWorld mode, could be 'forest' or 'double_attack' default='forest'\")\n",
    "    parser.add_argument(\"--map-size\", type=int, default=data.MAP_SIZE,\n",
    "                        help=\"Size of the map, default=\" + str(data.MAP_SIZE))\n",
    "    parser.add_argument(\"--walls\", type=int, default=data.COUNT_WALLS,\n",
    "                        help=\"Count of walls, default=\" + str(data.COUNT_WALLS))\n",
    "    parser.add_argument(\"--tigers\", type=int, default=data.COUNT_TIGERS,\n",
    "                        help=\"Count of tigers, default=\" + str(data.COUNT_TIGERS))\n",
    "    parser.add_argument(\"--deer\", type=int, default=data.COUNT_DEER,\n",
    "                        help=\"Count of deer, default=\" + str(data.COUNT_DEER))\n",
    "    args = parser.parse_args()\n",
    "    device = torch.device(args.dev)\n",
    "\n",
    "    env = make_env(args)\n",
    "    saves_path = os.path.join(\"saves\", args.name)\n",
    "    os.makedirs(saves_path, exist_ok=True)\n",
    "\n",
    "    net = model.DQNModel(\n",
    "        env.observation_spaces['tiger_0'].shape,\n",
    "        env.action_spaces['tiger_0'].n,\n",
    "    ).to(device)\n",
    "    tgt_net = ptan.agent.TargetNet(net)\n",
    "    print(net)\n",
    "\n",
    "    action_selector = ptan.actions.EpsilonGreedyActionSelector(epsilon=PARAMS.epsilon_start)\n",
    "    epsilon_tracker = common.EpsilonTracker(action_selector, PARAMS)\n",
    "    tiger_agent = ptan.agent.DQNAgent(net, action_selector, device)\n",
    "    deer_agent = data.RandomMAgent(env, env.handles[0])\n",
    "    exp_source = data.MAgentExperienceSourceFirstLast(\n",
    "        env,\n",
    "        agents_by_group={'deer': deer_agent, 'tiger': tiger_agent},\n",
    "        track_reward_group=\"tiger\",\n",
    "        filter_group=\"tiger\",\n",
    "    )\n",
    "    buffer = ptan.experience.ExperienceReplayBuffer(exp_source, PARAMS.replay_size)\n",
    "    optimizer = optim.Adam(net.parameters(), lr=PARAMS.learning_rate)\n",
    "\n",
    "    def process_batch(engine, batch):\n",
    "        res = {}\n",
    "        optimizer.zero_grad()\n",
    "        loss_v = model.calc_loss_dqn(\n",
    "            batch, net, tgt_net.target_model,\n",
    "            ptan.agent.default_states_preprocessor,\n",
    "            gamma=PARAMS.gamma, device=device)\n",
    "        loss_v.backward()\n",
    "        optimizer.step()\n",
    "        if epsilon_tracker is not None:\n",
    "            epsilon_tracker.frame(engine.state.iteration)\n",
    "            res['epsilon'] = action_selector.epsilon\n",
    "        if engine.state.iteration % PARAMS.target_net_sync == 0:\n",
    "            tgt_net.sync()\n",
    "        res['loss'] = loss_v.item()\n",
    "        return res\n",
    "\n",
    "    engine = Engine(process_batch)\n",
    "    common.setup_ignite(engine, PARAMS, exp_source, args.name,\n",
    "                        extra_metrics=('test_reward', 'test_steps'))\n",
    "    best_test_reward = None\n",
    "\n",
    "    @engine.on(ptan_ignite.PeriodEvents.ITERS_10000_COMPLETED)\n",
    "    def test_network(engine):\n",
    "        net.train(False)\n",
    "        reward, steps = test_model(net, device, args)\n",
    "        net.train(True)\n",
    "        engine.state.metrics['test_reward'] = reward\n",
    "        engine.state.metrics['test_steps'] = steps\n",
    "        print(\"Test done: got %.3f reward after %.2f steps\" % (\n",
    "            reward, steps\n",
    "        ))\n",
    "\n",
    "        global best_test_reward\n",
    "        if best_test_reward is None:\n",
    "            best_test_reward = reward\n",
    "        elif best_test_reward < reward:\n",
    "            print(\"Best test reward updated %.3f -> %.3f, save model\" % (\n",
    "                best_test_reward, reward\n",
    "            ))\n",
    "            best_test_reward = reward\n",
    "            torch.save(net.state_dict(), os.path.join(saves_path, \"best_%.3f.dat\" % reward))\n",
    "\n",
    "    engine.run(common.batch_generator(buffer, PARAMS.replay_initial,\n",
    "                                      PARAMS.batch_size))"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "executable": "/usr/bin/env python3",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
