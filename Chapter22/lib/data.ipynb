{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4a7ddc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import typing as tt\n",
    "import collections\n",
    "from dataclasses import dataclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc4305f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from magent2.builtin.config.forest import get_config as forest_config\n",
    "from magent2.builtin.config.double_attack import get_config as double_attack_config\n",
    "from magent2.builtin.config.battle import get_config as battle_config\n",
    "from magent2.gridworld import GridWorld"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1af6ba78",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gymnasium.utils import EzPickle\n",
    "from magent2.environments.magent_env import magent_parallel_env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa104b89",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "from ptan.experience import ExperienceFirstLast\n",
    "from ptan.agent import BaseAgent, States, AgentStates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b3e1ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAP_SIZE = 64\n",
    "COUNT_WALLS = int(MAP_SIZE * MAP_SIZE * 0.04)\n",
    "COUNT_DEER = int(MAP_SIZE * MAP_SIZE * 0.05)\n",
    "COUNT_TIGERS = int(MAP_SIZE * MAP_SIZE * 0.01)\n",
    "COUNT_BATTLERS = int(MAP_SIZE * MAP_SIZE * 0.02)\n",
    "MAX_CYCLES = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4adc20a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ForestEnv(magent_parallel_env, EzPickle):\n",
    "    metadata = {\n",
    "        \"render_modes\": [\"human\", \"rgb_array\"],\n",
    "        \"name\": \"forest_v4\",\n",
    "        \"render_fps\": 5,\n",
    "    }\n",
    "\n",
    "    def __init__(self, map_size: int = MAP_SIZE, max_cycles: int = MAX_CYCLES,\n",
    "                 extra_features: bool = False, render_mode: tt.Optional[str] = None,\n",
    "                 seed: tt.Optional[int] = None, count_walls: int = COUNT_WALLS,\n",
    "                 count_deer: int = COUNT_DEER, count_tigers: int = COUNT_TIGERS):\n",
    "        EzPickle.__init__(self, map_size, max_cycles, extra_features, render_mode, seed)\n",
    "        env = GridWorld(self.get_config(map_size), map_size=map_size)\n",
    "\n",
    "        handles = env.get_handles()\n",
    "        self.count_walls = count_walls\n",
    "        self.count_deer = count_deer\n",
    "        self.count_tigers = count_tigers\n",
    "\n",
    "        names = [\"deer\", \"tiger\"]\n",
    "        super().__init__(env, handles, names, map_size, max_cycles, [-1, 1],\n",
    "                         False, extra_features, render_mode)\n",
    "\n",
    "    @classmethod\n",
    "    def get_config(cls, map_size: int):\n",
    "        # Standard forest config, but deer get reward after every step\n",
    "        cfg = forest_config(map_size)\n",
    "        cfg.agent_type_dict[\"deer\"][\"step_reward\"] = 1\n",
    "        return cfg\n",
    "\n",
    "    def generate_map(self):\n",
    "        env, map_size = self.env, self.map_size\n",
    "        handles = env.get_handles()\n",
    "\n",
    "        env.add_walls(method=\"random\", n=self.count_walls)\n",
    "        env.add_agents(handles[0], method=\"random\", n=self.count_deer)\n",
    "        env.add_agents(handles[1], method=\"random\", n=self.count_tigers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b44e6a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DoubleAttackEnv(magent_parallel_env, EzPickle):\n",
    "    metadata = {\n",
    "        \"render_modes\": [\"human\", \"rgb_array\"],\n",
    "        \"name\": \"tiger_deer_v4\",\n",
    "        \"render_fps\": 5,\n",
    "    }\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        map_size: int = MAP_SIZE,\n",
    "        max_cycles: int = MAX_CYCLES,\n",
    "        extra_features: bool = False,\n",
    "        render_mode: tt.Optional[str] = None,\n",
    "        seed: tt.Optional[int] = None,\n",
    "        count_walls: int = COUNT_WALLS,\n",
    "        count_deer: int = COUNT_DEER,\n",
    "        count_tigers: int = COUNT_TIGERS,\n",
    "    ):\n",
    "        EzPickle.__init__(\n",
    "            self, map_size, max_cycles,\n",
    "            extra_features, render_mode, seed,\n",
    "        )\n",
    "        env = GridWorld(\n",
    "            self.get_config(map_size), map_size=map_size\n",
    "        )\n",
    "\n",
    "        handles = env.get_handles()\n",
    "        self.count_walls = count_walls\n",
    "        self.count_deer = count_deer\n",
    "        self.count_tigers = count_tigers\n",
    "\n",
    "        names = [\"deer\", \"tiger\"]\n",
    "        super().__init__(\n",
    "            env, handles, names, map_size, max_cycles,\n",
    "            [-1, 1], False, extra_features, render_mode,\n",
    "        )\n",
    "\n",
    "    @classmethod\n",
    "    def get_config(cls, map_size: int):\n",
    "        # Standard forest config, but deer get reward after every step\n",
    "        cfg = double_attack_config(map_size)\n",
    "        cfg.agent_type_dict[\"deer\"][\"step_reward\"] = 1\n",
    "        return cfg\n",
    "\n",
    "    def generate_map(self):\n",
    "        env, map_size = self.env, self.map_size\n",
    "        handles = env.get_handles()\n",
    "\n",
    "        env.add_walls(method=\"random\", n=self.count_walls)\n",
    "        env.add_agents(handles[0], method=\"random\", n=self.count_deer)\n",
    "        env.add_agents(handles[1], method=\"random\", n=self.count_tigers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8efa9a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BattleEnv(magent_parallel_env, EzPickle):\n",
    "    metadata = {\n",
    "        \"render_modes\": [\"human\", \"rgb_array\"],\n",
    "        \"name\": \"battle_v4\",\n",
    "        \"render_fps\": 5,\n",
    "    }\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        map_size: int = MAP_SIZE,\n",
    "        max_cycles: int = MAX_CYCLES,\n",
    "        extra_features: bool = False,\n",
    "        render_mode: tt.Optional[str] = None,\n",
    "        seed: tt.Optional[int] = None,\n",
    "        count_walls: int = COUNT_WALLS,\n",
    "        count_a: int = COUNT_BATTLERS,\n",
    "        count_b: int = COUNT_BATTLERS,\n",
    "    ):\n",
    "        EzPickle.__init__(\n",
    "            self, map_size, max_cycles,\n",
    "            extra_features, render_mode, seed,\n",
    "        )\n",
    "        env = GridWorld(\n",
    "            self.get_config(map_size), map_size=map_size\n",
    "        )\n",
    "\n",
    "        handles = env.get_handles()\n",
    "        self.count_walls = count_walls\n",
    "        self.count_a = count_a\n",
    "        self.count_b = count_b\n",
    "\n",
    "        names = [\"a\", \"b\"]\n",
    "        super().__init__(\n",
    "            env, handles, names, map_size, max_cycles,\n",
    "            [-1, 1], False, extra_features, render_mode,\n",
    "        )\n",
    "\n",
    "    @classmethod\n",
    "    def get_config(cls, map_size: int):\n",
    "        cfg = battle_config(map_size)\n",
    "        return cfg\n",
    "\n",
    "    def generate_map(self):\n",
    "        env, map_size = self.env, self.map_size\n",
    "        handles = env.get_handles()\n",
    "\n",
    "        env.add_walls(method=\"random\", n=self.count_walls)\n",
    "        env.add_agents(handles[0], method=\"random\", n=self.count_a)\n",
    "        env.add_agents(handles[1], method=\"random\", n=self.count_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "464cb264",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass(frozen=True)\n",
    "class ExperienceFirstLastMARL(ExperienceFirstLast):\n",
    "    group: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53931a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MAgentExperienceSourceFirstLast:\n",
    "    \"\"\"\n",
    "    2-step experience source for MAgent parallel environment\n",
    "    \"\"\"\n",
    "    def __init__(self, env: magent_parallel_env, agents_by_group: tt.Dict[str, BaseAgent],\n",
    "                 track_reward_group: str, env_seed: tt.Optional[int] = None,\n",
    "                 filter_group: tt.Optional[str] = None):\n",
    "        self.env = env\n",
    "        self.agents_by_group = agents_by_group\n",
    "        self.track_reward_group = track_reward_group\n",
    "        self.env_seed = env_seed\n",
    "        self.filter_group = filter_group\n",
    "        self.total_rewards = []\n",
    "        self.total_steps = []\n",
    "\n",
    "        # forward and inverse map of agent_id -> group\n",
    "        self.agent_groups = {\n",
    "            agent_id: self.agent_group(agent_id)\n",
    "            for agent_id in self.env.agents\n",
    "        }\n",
    "        self.group_agents = collections.defaultdict(list)\n",
    "        for agent_id, group in self.agent_groups.items():\n",
    "            self.group_agents[group].append(agent_id)\n",
    "\n",
    "    @classmethod\n",
    "    def agent_group(cls, agent_id: str) -> str:\n",
    "        a, _ = agent_id.split(\"_\", maxsplit=1)\n",
    "        return a\n",
    "\n",
    "    def __iter__(self) -> tt.Generator[ExperienceFirstLastMARL, None, None]:\n",
    "        # iterate episodes\n",
    "        while True:\n",
    "            # initial observation\n",
    "            cur_obs = self.env.reset(self.env_seed)\n",
    "\n",
    "            # agent states are kept in groups\n",
    "            agent_states = {\n",
    "                prefix: [self.agents_by_group[prefix].initial_state() for _ in group]\n",
    "                for prefix, group in self.group_agents.items()\n",
    "            }\n",
    "\n",
    "            episode_steps = 0\n",
    "            episode_rewards = 0.0\n",
    "            # steps while we have alive agents\n",
    "            while self.env.agents:\n",
    "                # calculate actions for the whole group and unpack\n",
    "                actions = {}\n",
    "                for prefix, group in self.group_agents.items():\n",
    "                    gr_obs = [\n",
    "                        cur_obs[agent_id]\n",
    "                        for agent_id in group if agent_id in cur_obs\n",
    "                    ]\n",
    "                    gr_actions, gr_states = self.agents_by_group[prefix](\n",
    "                        gr_obs, agent_states[prefix])\n",
    "                    agent_states[prefix] = gr_states\n",
    "                    idx = 0\n",
    "                    for agent_id in group:\n",
    "                        if agent_id not in cur_obs:\n",
    "                            continue\n",
    "                        actions[agent_id] = gr_actions[idx]\n",
    "                        idx += 1\n",
    "                # perform the action\n",
    "                new_obs, rewards, dones, truncs, _ = self.env.step(actions)\n",
    "\n",
    "                # compute and yeld experience items\n",
    "                # list of agents was updated (deads cleared),\n",
    "                # need to process returned data for all agents\n",
    "                for agent_id, reward in rewards.items():\n",
    "                    group = self.agent_groups[agent_id]\n",
    "                    if group == self.track_reward_group:\n",
    "                        episode_rewards += reward\n",
    "                    if self.filter_group is not None:\n",
    "                        if group != self.filter_group:\n",
    "                            continue\n",
    "                    last_state = new_obs[agent_id]\n",
    "                    if dones[agent_id] or truncs[agent_id]:\n",
    "                        last_state = None\n",
    "                    yield ExperienceFirstLastMARL(\n",
    "                        state=cur_obs[agent_id], action=actions[agent_id],\n",
    "                        reward=reward, last_state=last_state, group=group\n",
    "                    )\n",
    "                # update observations\n",
    "                cur_obs = new_obs\n",
    "                episode_steps += 1\n",
    "            # episode ended\n",
    "            self.total_steps.append(episode_steps)\n",
    "            tr_group = self.group_agents[self.track_reward_group]\n",
    "            self.total_rewards.append(episode_rewards / len(tr_group))\n",
    "\n",
    "    def pop_total_rewards(self) -> tt.List[float]:\n",
    "        r = self.total_rewards\n",
    "        if r:\n",
    "            self.total_rewards = []\n",
    "            self.total_steps = []\n",
    "        return r\n",
    "\n",
    "    def pop_rewards_steps(self) -> tt.List[tt.Tuple[float, int]]:\n",
    "        res = list(zip(self.total_rewards, self.total_steps))\n",
    "        if res:\n",
    "            self.total_rewards, self.total_steps = [], []\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8f0e23c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomMAgent(BaseAgent):\n",
    "    def __init__(self, env: magent_parallel_env, handle):\n",
    "        self.env = env.env\n",
    "        self.handle = handle\n",
    "\n",
    "    def __call__(\n",
    "            self, states: States,\n",
    "            agent_states: AgentStates = None,\n",
    "    ) -> tt.Tuple[np.ndarray, AgentStates]:\n",
    "        n_actions = self.env.get_action_space(self.handle)[0]\n",
    "        if isinstance(states, list):\n",
    "            size = len(states)\n",
    "        else:\n",
    "            size = states.shape[0]\n",
    "        res = np.random.randint(n_actions, size=size)\n",
    "        return res, agent_states"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
