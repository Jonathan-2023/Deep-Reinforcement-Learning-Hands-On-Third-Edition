{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bf89815",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "244051bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ptan\n",
    "import torch\n",
    "import argparse\n",
    "from typing import Tuple\n",
    "import ptan.ignite as ptan_ignite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31cb5155",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "from types import SimpleNamespace\n",
    "from lib import data, model, common\n",
    "from ignite.engine import Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0495e008",
   "metadata": {},
   "outputs": [],
   "source": [
    "PARAMS = SimpleNamespace(**{\n",
    "    'run_name':         'battle',\n",
    "    'stop_reward':      None,\n",
    "    'replay_size':      1000000,\n",
    "    'replay_initial':   100,\n",
    "    'target_net_sync':  1000,\n",
    "    'epsilon_frames':   5*10**5,\n",
    "    'epsilon_start':    1.0,\n",
    "    'epsilon_final':    0.02,\n",
    "    'learning_rate':    1e-4,\n",
    "    'gamma':            0.99,\n",
    "    'batch_size':       32\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8e0c811",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_env(args: argparse.Namespace) -> data.magent_parallel_env:\n",
    "    env = data.BattleEnv(\n",
    "        map_size=args.map_size,\n",
    "        count_walls=args.walls,\n",
    "        count_a=args.a,\n",
    "        count_b=args.b,\n",
    "    )\n",
    "    return env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd8279c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(a_net: model.DQNModel, device: torch.device,\n",
    "               args: argparse.Namespace) -> Tuple[float, float, int]:\n",
    "    env = make_env(args)\n",
    "    a_agent = ptan.agent.DQNAgent(\n",
    "        a_net, ptan.actions.ArgmaxActionSelector(),\n",
    "        device)\n",
    "    b_agent = data.RandomMAgent(env, env.handles[1])\n",
    "\n",
    "    obs = env.reset()\n",
    "    sum_steps = 0\n",
    "    a_rewards = 0.0\n",
    "    b_rewards = 0.0\n",
    "\n",
    "    while env.agents:\n",
    "        actions = {}\n",
    "        a_obs = [\n",
    "            obs[agent_id]\n",
    "            for agent_id in env.agents\n",
    "            if agent_id.startswith(\"a\")\n",
    "        ]\n",
    "        a_acts, _ = a_agent(a_obs)\n",
    "        ofs = 0\n",
    "        for agent_id in env.agents:\n",
    "            if agent_id.startswith(\"a\"):\n",
    "                actions[agent_id] = a_acts[ofs]\n",
    "                ofs += 1\n",
    "        b_obs = [\n",
    "            obs[agent_id]\n",
    "            for agent_id in env.agents\n",
    "            if agent_id.startswith(\"b\")\n",
    "        ]\n",
    "        b_acts, _ = b_agent(b_obs)\n",
    "        ofs = 0\n",
    "        for agent_id in env.agents:\n",
    "            if agent_id.startswith(\"b\"):\n",
    "                actions[agent_id] = b_acts[ofs]\n",
    "                ofs += 1\n",
    "\n",
    "        obs, rewards, dones, _, _ = env.step(actions)\n",
    "        sum_steps += 1\n",
    "        for agent_id, reward in rewards.items():\n",
    "            if agent_id.startswith(\"a\"):\n",
    "                a_rewards += reward\n",
    "            if agent_id.startswith(\"b\"):\n",
    "                b_rewards += reward\n",
    "\n",
    "    return a_rewards / env.count_a, b_rewards / env.count_b, sum_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a100dd6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--dev\", default=\"cpu\", help=\"Device to train\")\n",
    "    parser.add_argument(\"-n\", \"--name\", required=True, help=\"Run name\")\n",
    "    parser.add_argument(\"--map-size\", type=int, default=data.MAP_SIZE,\n",
    "                        help=\"Size of the map, default=\" + str(data.MAP_SIZE))\n",
    "    parser.add_argument(\"--walls\", type=int, default=data.COUNT_WALLS,\n",
    "                        help=\"Count of walls, default=\" + str(data.COUNT_WALLS))\n",
    "    parser.add_argument(\"--a\", type=int, default=data.COUNT_BATTLERS,\n",
    "                        help=\"Count of tigers, default=\" + str(data.COUNT_BATTLERS))\n",
    "    parser.add_argument(\"--b\", type=int, default=data.COUNT_BATTLERS,\n",
    "                        help=\"Count of deer, default=\" + str(data.COUNT_BATTLERS))\n",
    "    args = parser.parse_args()\n",
    "    device = torch.device(args.dev)\n",
    "\n",
    "    env = make_env(args)\n",
    "    saves_path = os.path.join(\"saves\", args.name)\n",
    "    os.makedirs(saves_path, exist_ok=True)\n",
    "\n",
    "    net = model.DQNModel(\n",
    "        env.observation_spaces['a_0'].shape,\n",
    "        env.action_spaces['a_0'].n,\n",
    "    ).to(device)\n",
    "    tgt_net = ptan.agent.TargetNet(net)\n",
    "    print(net)\n",
    "\n",
    "    action_selector = ptan.actions.EpsilonGreedyActionSelector(\n",
    "        epsilon=PARAMS.epsilon_start)\n",
    "    epsilon_tracker = common.EpsilonTracker(action_selector, PARAMS)\n",
    "    a_agent = ptan.agent.DQNAgent(net, action_selector, device)\n",
    "    b_agent = data.RandomMAgent(env, env.handles[1])\n",
    "    exp_source = data.MAgentExperienceSourceFirstLast(\n",
    "        env,\n",
    "        agents_by_group={\n",
    "            'a': a_agent, 'b': b_agent\n",
    "        },\n",
    "        track_reward_group=\"a\",\n",
    "        filter_group=\"a\",\n",
    "    )\n",
    "    buffer = ptan.experience.ExperienceReplayBuffer(\n",
    "        exp_source, PARAMS.replay_size)\n",
    "    optimizer = optim.Adam(net.parameters(), lr=PARAMS.learning_rate)\n",
    "\n",
    "    def process_batch(engine, batch):\n",
    "        res = {}\n",
    "        optimizer.zero_grad()\n",
    "        loss_v = model.calc_loss_dqn(\n",
    "            batch, net, tgt_net.target_model,\n",
    "            ptan.agent.default_states_preprocessor,\n",
    "            gamma=PARAMS.gamma, device=device)\n",
    "        loss_v.backward()\n",
    "        optimizer.step()\n",
    "        if epsilon_tracker is not None:\n",
    "            epsilon_tracker.frame(engine.state.iteration)\n",
    "            res['epsilon'] = action_selector.epsilon\n",
    "        if engine.state.iteration % PARAMS.target_net_sync == 0:\n",
    "            tgt_net.sync()\n",
    "        res['loss'] = loss_v.item()\n",
    "        return res\n",
    "\n",
    "    engine = Engine(process_batch)\n",
    "    common.setup_ignite(engine, PARAMS, exp_source, args.name,\n",
    "                        extra_metrics=('test_a_reward', 'test_b_reward', 'test_steps'))\n",
    "    best_test_reward = None\n",
    "\n",
    "    @engine.on(ptan_ignite.PeriodEvents.ITERS_10000_COMPLETED)\n",
    "    def test_network(engine):\n",
    "        net.train(False)\n",
    "        a_reward, b_reward, steps = test_model(net, device, args)\n",
    "        net.train(True)\n",
    "        engine.state.metrics['test_a_reward'] = a_reward\n",
    "        engine.state.metrics['test_b_reward'] = b_reward\n",
    "        engine.state.metrics['test_steps'] = steps\n",
    "        print(\"Test done: got %.3f reward (a) vs %.3f reward (b) after %.2f steps\" % (\n",
    "            a_reward, b_reward, steps\n",
    "        ))\n",
    "\n",
    "        global best_test_reward\n",
    "        if best_test_reward is None:\n",
    "            best_test_reward = a_reward\n",
    "        elif best_test_reward < a_reward:\n",
    "            print(\"Best test reward updated %.3f -> %.3f, save model\" % (\n",
    "                best_test_reward, a_reward\n",
    "            ))\n",
    "            best_test_reward = a_reward\n",
    "            torch.save(net.state_dict(), os.path.join(saves_path, \"best_%.3f.dat\" % a_reward))\n",
    "\n",
    "    engine.run(common.batch_generator(buffer, PARAMS.replay_initial,\n",
    "                                      PARAMS.batch_size))"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "executable": "/usr/bin/env python3",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
