{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf403e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import time\n",
    "import numpy as np\n",
    "import typing as tt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d4f08a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from ptan.experience import ExperienceFirstLast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7d8fc5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RewardTracker:\n",
    "    def __init__(self, writer, stop_reward):\n",
    "        self.writer = writer\n",
    "        self.stop_reward = stop_reward\n",
    "\n",
    "    def __enter__(self):\n",
    "        self.ts = time.time()\n",
    "        self.ts_frame = 0\n",
    "        self.total_rewards = []\n",
    "        return self\n",
    "\n",
    "    def __exit__(self, *args):\n",
    "        self.writer.close()\n",
    "\n",
    "    def reward(self, reward, frame, epsilon=None):\n",
    "        self.total_rewards.append(reward)\n",
    "        speed = (frame - self.ts_frame) / (time.time() - self.ts)\n",
    "        self.ts_frame = frame\n",
    "        self.ts = time.time()\n",
    "        mean_reward = np.mean(self.total_rewards[-100:])\n",
    "        epsilon_str = \"\" if epsilon is None else \", eps %.2f\" % epsilon\n",
    "        print(\"%d: done %d games, mean reward %.3f, speed %.2f f/s%s\" % (\n",
    "            frame, len(self.total_rewards), mean_reward, speed, epsilon_str\n",
    "        ))\n",
    "        sys.stdout.flush()\n",
    "        if epsilon is not None:\n",
    "            self.writer.add_scalar(\"epsilon\", epsilon, frame)\n",
    "        self.writer.add_scalar(\"speed\", speed, frame)\n",
    "        self.writer.add_scalar(\"reward_100\", mean_reward, frame)\n",
    "        self.writer.add_scalar(\"reward\", reward, frame)\n",
    "        if mean_reward > self.stop_reward:\n",
    "            print(\"Solved in %d frames!\" % frame)\n",
    "            return True\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b47db8fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AtariA2C(nn.Module):\n",
    "    def __init__(self, input_shape: tt.Tuple[int, ...], n_actions: int):\n",
    "        super(AtariA2C, self).__init__()\n",
    "\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(input_shape[0], 32, kernel_size=8, stride=4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "        )\n",
    "\n",
    "        size = self.conv(torch.zeros(1, *input_shape)).size()[-1]\n",
    "        self.policy = nn.Sequential(\n",
    "            nn.Linear(size, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, n_actions)\n",
    "        )\n",
    "        self.value = nn.Sequential(\n",
    "            nn.Linear(size, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.ByteTensor) -> tt.Tuple[torch.Tensor, torch.Tensor]:\n",
    "        xx = x / 255\n",
    "        conv_out = self.conv(xx)\n",
    "        return self.policy(conv_out), self.value(conv_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4b98743",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unpack_batch(batch: tt.List[ExperienceFirstLast], net: AtariA2C,\n",
    "                 device: torch.device, gamma: float, reward_steps: int):\n",
    "    \"\"\"\n",
    "    Convert batch into training tensors\n",
    "    :param batch: batch to process\n",
    "    :param net: network to use√ü\n",
    "    :param gamma: gamma value\n",
    "    :param reward_steps: steps of reward\n",
    "    :return: states variable, actions tensor, reference values variable\n",
    "    \"\"\"\n",
    "    states = []\n",
    "    actions = []\n",
    "    rewards = []\n",
    "    not_done_idx = []\n",
    "    last_states = []\n",
    "    for idx, exp in enumerate(batch):\n",
    "        states.append(np.asarray(exp.state))\n",
    "        actions.append(int(exp.action))\n",
    "        rewards.append(exp.reward)\n",
    "        if exp.last_state is not None:\n",
    "            not_done_idx.append(idx)\n",
    "            last_states.append(np.asarray(exp.last_state))\n",
    "\n",
    "    states_t = torch.FloatTensor(np.asarray(states)).to(device)\n",
    "    actions_t = torch.LongTensor(actions).to(device)\n",
    "\n",
    "    # handle rewards\n",
    "    rewards_np = np.array(rewards, dtype=np.float32)\n",
    "    if not_done_idx:\n",
    "        last_states_t = torch.FloatTensor(np.asarray(last_states)).to(device)\n",
    "        last_vals_t = net(last_states_t)[1]\n",
    "        last_vals_np = last_vals_t.data.cpu().numpy()[:, 0]\n",
    "        last_vals_np *= gamma ** reward_steps\n",
    "        rewards_np[not_done_idx] += last_vals_np\n",
    "\n",
    "    ref_vals_t = torch.FloatTensor(rewards_np).to(device)\n",
    "\n",
    "    return states_t, actions_t, ref_vals_t"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
