{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24048701",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import ptan\n",
    "import collections\n",
    "from ptan.experience import VectorExperienceSourceFirstLast\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b7e0f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.utils as nn_utils\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from ray import tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "803201be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib import common"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9763c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_STEPS_TUNE = 4_000_000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8612eee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "GAMMA = 0.99\n",
    "LEARNING_RATE = 0.001\n",
    "ENTROPY_BETA = 0.01\n",
    "BATCH_SIZE = 128\n",
    "NUM_ENVS = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af0e0971",
   "metadata": {},
   "outputs": [],
   "source": [
    "REWARD_STEPS = 4\n",
    "CLIP_GRAD = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "550e8774",
   "metadata": {},
   "outputs": [],
   "source": [
    "PARAMS_SPACE = {\n",
    "    \"entropy_beta\": tune.loguniform(0.001, 0.1),\n",
    "    \"lr\": tune.loguniform(1e-5, 1e-2),\n",
    "    \"reward_steps\": tune.choice([2, 4, 6, 8]),\n",
    "    \"clip_grad\": tune.loguniform(1e-2, 1),\n",
    "    \"batch_size\": tune.choice([4, 8, 16, 32, 64, 128, 256]),\n",
    "    \"num_envs\": tune.choice([4, 8, 16, 32, 64]),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bb45e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MeanBuffer:\n",
    "    def __init__(self, capacity: int):\n",
    "        self.capacity = capacity\n",
    "        self.deque = collections.deque(maxlen=capacity)\n",
    "        self.sum = 0.0\n",
    "\n",
    "    def add(self, val: float):\n",
    "        if len(self.deque) == self.capacity:\n",
    "            self.sum -= self.deque[0]\n",
    "        self.deque.append(val)\n",
    "        self.sum += val\n",
    "\n",
    "    def mean(self) -> float:\n",
    "        if not self.deque:\n",
    "            return 0.0\n",
    "        return self.sum / len(self.deque)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47d3d910",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(config: dict, device: torch.device) -> dict:\n",
    "    LEARNING_RATE = config['lr']\n",
    "    BATCH_SIZE = config['batch_size']\n",
    "    NUM_ENVS = config['num_envs']\n",
    "    REWARD_STEPS = config['reward_steps']\n",
    "    CLIP_GRAD = config['clip_grad']\n",
    "    ENTROPY_BETA = config['entropy_beta']\n",
    "\n",
    "    env_factories = [\n",
    "        lambda: ptan.common.wrappers.wrap_dqn(\n",
    "            gym.make(\"PongNoFrameskip-v4\"))\n",
    "        for _ in range(NUM_ENVS)\n",
    "    ]\n",
    "    env = gym.vector.SyncVectorEnv(env_factories)\n",
    "    net = common.AtariA2C(env.single_observation_space.shape,\n",
    "                          env.single_action_space.n).to(device)\n",
    "\n",
    "    agent = ptan.agent.PolicyAgent(\n",
    "        lambda x: net(x)[0], apply_softmax=True, device=device)\n",
    "    exp_source = VectorExperienceSourceFirstLast(\n",
    "        env, agent, gamma=GAMMA, steps_count=REWARD_STEPS)\n",
    "\n",
    "    mean_buf = MeanBuffer(100)\n",
    "    max_mean_reward = None\n",
    "    optimizer = optim.Adam(\n",
    "        net.parameters(), lr=LEARNING_RATE, eps=1e-3)\n",
    "    batch = []\n",
    "\n",
    "    for step_idx, exp in enumerate(exp_source):\n",
    "        if step_idx > MAX_STEPS_TUNE:\n",
    "            break\n",
    "        batch.append(exp)\n",
    "\n",
    "        # handle new rewards\n",
    "        new_rewards = exp_source.pop_total_rewards()\n",
    "        if new_rewards:\n",
    "            for r in new_rewards:\n",
    "                mean_buf.add(r)\n",
    "            m = mean_buf.mean()\n",
    "            if max_mean_reward is None:\n",
    "                max_mean_reward = m\n",
    "            elif max_mean_reward < m:\n",
    "                print(f\"{step_idx}: Mean reward \"\n",
    "                      f\"updated {max_mean_reward} -> {m}\")\n",
    "                max_mean_reward = m\n",
    "\n",
    "        if len(batch) < BATCH_SIZE:\n",
    "            continue\n",
    "\n",
    "        states_t, actions_t, vals_ref_t = \\\n",
    "            common.unpack_batch(\n",
    "                batch, net, device=device,\n",
    "                gamma=GAMMA, reward_steps=REWARD_STEPS)\n",
    "        batch.clear()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        logits_t, value_t = net(states_t)\n",
    "        loss_value_t = F.mse_loss(\n",
    "            value_t.squeeze(-1), vals_ref_t)\n",
    "\n",
    "        log_prob_t = F.log_softmax(logits_t, dim=1)\n",
    "        adv_t = vals_ref_t - value_t.detach()\n",
    "        log_act_t = log_prob_t[range(BATCH_SIZE), actions_t]\n",
    "        log_prob_actions_t = adv_t * log_act_t\n",
    "        loss_policy_t = -log_prob_actions_t.mean()\n",
    "\n",
    "        prob_t = F.softmax(logits_t, dim=1)\n",
    "        entropy_loss_t = ENTROPY_BETA * (\n",
    "                prob_t * log_prob_t).sum(dim=1).mean()\n",
    "\n",
    "        # calculate policy gradients only\n",
    "        loss_policy_t.backward(retain_graph=True)\n",
    "\n",
    "        # apply entropy and value gradients\n",
    "        loss_v = entropy_loss_t + loss_value_t\n",
    "        loss_v.backward()\n",
    "        nn_utils.clip_grad_norm_(\n",
    "            net.parameters(), CLIP_GRAD)\n",
    "        optimizer.step()\n",
    "    env.close()\n",
    "    return {\"max_reward\": max_mean_reward}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ce1a16b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--dev\", default=\"cpu\",\n",
    "                        help=\"Device to use, default=cpu\")\n",
    "    parser.add_argument(\"--samples\", type=int, default=20,\n",
    "                        help=\"Count of samples to run\")\n",
    "    args = parser.parse_args()\n",
    "    device = torch.device(args.dev)\n",
    "\n",
    "    config = tune.TuneConfig(num_samples=args.samples)\n",
    "    obj = tune.with_parameters(train, device=device)\n",
    "    if device.type == 'cuda':\n",
    "        obj = tune.with_resources(obj, {\"gpu\": 1})\n",
    "    tuner = tune.Tuner(\n",
    "        obj, param_space=PARAMS_SPACE, tune_config=config\n",
    "    )\n",
    "    results = tuner.fit()\n",
    "    best = results.get_best_result(metric=\"max_reward\", mode=\"max\")\n",
    "    print(best.config)\n",
    "    print(best.metrics)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "executable": "/usr/bin/env python3",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
