{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b11d6c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ptan\n",
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "686bafab",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNNet(nn.Module):\n",
    "    def __init__(self, actions: int):\n",
    "        super(DQNNet, self).__init__()\n",
    "        self.actions = actions\n",
    "\n",
    "    def forward(self, x):\n",
    "        # we always produce diagonal tensor of shape\n",
    "        # (batch_size, actions)\n",
    "        return torch.eye(x.size()[0], self.actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dad8a3da",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyNet(nn.Module):\n",
    "    def __init__(self, actions: int):\n",
    "        super(PolicyNet, self).__init__()\n",
    "        self.actions = actions\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Now we produce the tensor with first two actions\n",
    "        # having the same logit scores\n",
    "        shape = (x.size()[0], self.actions)\n",
    "        res = torch.zeros(shape, dtype=torch.float32)\n",
    "        res[:, 0] = 1\n",
    "        res[:, 1] = 1\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46819599",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    net = DQNNet(actions=3)\n",
    "    net_out = net(torch.zeros(2, 10))\n",
    "    print(\"dqn_net:\")\n",
    "    print(net_out)\n",
    "\n",
    "    selector = ptan.actions.ArgmaxActionSelector()\n",
    "    agent = ptan.agent.DQNAgent(model=net, action_selector=selector)\n",
    "    ag_out = agent(np.zeros(shape=(2, 5)))\n",
    "    print(\"Argmax:\", ag_out)\n",
    "\n",
    "    selector = ptan.actions.EpsilonGreedyActionSelector(epsilon=1.0)\n",
    "    agent = ptan.agent.DQNAgent(model=net, action_selector=selector)\n",
    "    ag_out = agent(torch.zeros(10, 5))[0]\n",
    "    print(\"eps=1.0:\", ag_out)\n",
    "\n",
    "    selector.epsilon = 0.5\n",
    "    ag_out = agent(torch.zeros(10, 5))[0]\n",
    "    print(\"eps=0.5:\", ag_out)\n",
    "\n",
    "    selector.epsilon = 0.1\n",
    "    ag_out = agent(torch.zeros(10, 5))[0]\n",
    "    print(\"eps=0.1:\", ag_out)\n",
    "\n",
    "    net = PolicyNet(actions=5)\n",
    "    net_out = net(torch.zeros(6, 10))\n",
    "    print(\"policy_net:\")\n",
    "    print(net_out)\n",
    "\n",
    "    selector = ptan.actions.ProbabilityActionSelector()\n",
    "    agent = ptan.agent.PolicyAgent(model=net, action_selector=selector, apply_softmax=True)\n",
    "    ag_out = agent(torch.zeros(6, 5))[0]\n",
    "    print(ag_out)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
