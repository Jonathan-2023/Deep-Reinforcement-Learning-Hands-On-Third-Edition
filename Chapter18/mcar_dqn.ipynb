{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae642c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import ptan\n",
    "import ptan.ignite as ptan_ignite\n",
    "import gymnasium as gym\n",
    "import argparse\n",
    "import random\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from types import SimpleNamespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bc0d0a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ignite.engine import Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54a23d18",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "from lib import common, dqn_extra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44bfd98c",
   "metadata": {},
   "outputs": [],
   "source": [
    "HYPERPARAMS = {\n",
    "    'egreedy': SimpleNamespace(**{\n",
    "        'env_name':         \"MountainCar-v0\",\n",
    "        'stop_reward':      None,\n",
    "        'stop_test_reward': -130.0,\n",
    "        'run_name':         'egreedy',\n",
    "        'replay_size':      100000,\n",
    "        'replay_initial':   100,\n",
    "        'target_net_sync':  100,\n",
    "        'epsilon_frames':   10**5,\n",
    "        'epsilon_start':    1.0,\n",
    "        'epsilon_final':    0.02,\n",
    "        'learning_rate':    0.0001,\n",
    "        'gamma':            0.99,\n",
    "        'batch_size':       32,\n",
    "        'eps_decay_trigger': False,\n",
    "    }),\n",
    "    'egreedy-long': SimpleNamespace(**{\n",
    "        'env_name':         \"MountainCar-v0\",\n",
    "        'stop_reward':      None,\n",
    "        'stop_test_reward': -130.0,\n",
    "        'run_name':         'egreedy-long',\n",
    "        'replay_size':      100000,\n",
    "        'replay_initial':   1000,\n",
    "        'target_net_sync':  100,\n",
    "        'epsilon_frames':   10 ** 6,\n",
    "        'epsilon_start':    1.0,\n",
    "        'epsilon_final':    0.02,\n",
    "        'learning_rate':    0.0001,\n",
    "        'gamma':            0.99,\n",
    "        'batch_size':       32,\n",
    "        'eps_decay_trigger': True,\n",
    "    }),\n",
    "    'noisynet': SimpleNamespace(**{\n",
    "        'env_name':         \"MountainCar-v0\",\n",
    "        'stop_reward':      None,\n",
    "        'stop_test_reward': -130.0,\n",
    "        'run_name':         'noisynet',\n",
    "        'replay_size':      100000,\n",
    "        'replay_initial':   1000,\n",
    "        'target_net_sync':  1000,\n",
    "        'learning_rate':    0.0001,\n",
    "        'gamma':            0.99,\n",
    "        'batch_size':       32,\n",
    "        'eps_decay_trigger': False,\n",
    "    }),\n",
    "    'counts': SimpleNamespace(**{\n",
    "        'env_name':         \"MountainCar-v0\",\n",
    "        'stop_reward':      None,\n",
    "        'stop_test_reward': -130.0,\n",
    "        'run_name':         'counts',\n",
    "        'replay_size':      100000,\n",
    "        'replay_initial':   1000,\n",
    "        'target_net_sync':  1000,\n",
    "        'learning_rate':    0.00005,\n",
    "        'gamma':            0.99,\n",
    "        'batch_size':       32,\n",
    "        'counts_reward_scale': 0.5,\n",
    "        'eps_decay_trigger': False,\n",
    "    }),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbfa9bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_STEPS = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ee59ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def counts_hash(obs: np.ndarray):\n",
    "    r = obs.tolist()\n",
    "    return tuple(map(lambda v: round(v, 3), r))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0408bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    random.seed(common.SEED)\n",
    "    torch.manual_seed(common.SEED)\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"-n\", \"--name\", required=True, help=\"Run name\")\n",
    "    parser.add_argument(\"-p\", \"--params\", default='egreedy', choices=list(HYPERPARAMS.keys()),\n",
    "                        help=\"Parameters, default=egreedy\")\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    params = HYPERPARAMS[args.params]\n",
    "\n",
    "    env = gym.make(params.env_name)\n",
    "    test_env = gym.make(params.env_name)\n",
    "    if args.params == 'counts':\n",
    "        env = common.PseudoCountRewardWrapper(env, reward_scale=params.counts_reward_scale, hash_function=counts_hash)\n",
    "    if args.params.startswith(\"egreedy\") or args.params == 'counts':\n",
    "        net = dqn_extra.MountainCarBaseDQN(env.observation_space.shape[0], env.action_space.n)\n",
    "    elif args.params == 'noisynet':\n",
    "        net = dqn_extra.MountainCarNoisyNetDQN(env.observation_space.shape[0], env.action_space.n)\n",
    "    tgt_net = ptan.agent.TargetNet(net)\n",
    "    print(net)\n",
    "\n",
    "    if args.params.startswith('egreedy'):\n",
    "        selector = ptan.actions.EpsilonGreedyActionSelector(epsilon=params.epsilon_start)\n",
    "        epsilon_tracker = common.EpsilonTracker(selector, params)\n",
    "        training_enabled = not params.eps_decay_trigger\n",
    "        epsilon_tracker_frame = 0\n",
    "    else:\n",
    "        selector = ptan.actions.ArgmaxActionSelector()\n",
    "        training_enabled = True\n",
    "\n",
    "    agent = ptan.agent.DQNAgent(net, selector, preprocessor=ptan.agent.float32_preprocessor)\n",
    "\n",
    "    exp_source = ptan.experience.ExperienceSourceFirstLast(\n",
    "        env, agent, gamma=params.gamma, steps_count=N_STEPS,\n",
    "        env_seed=common.SEED\n",
    "    )\n",
    "    buffer = ptan.experience.ExperienceReplayBuffer(\n",
    "        exp_source, buffer_size=params.replay_size)\n",
    "    optimizer = optim.Adam(net.parameters(), lr=params.learning_rate)\n",
    "\n",
    "    def process_batch(engine, batch):\n",
    "        if not training_enabled:\n",
    "            return {\n",
    "                \"loss\": 0.0,\n",
    "                \"epsilon\": selector.epsilon\n",
    "            }\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss_v = common.calc_loss_double_dqn(batch, net, tgt_net.target_model,\n",
    "                                             gamma=params.gamma**N_STEPS)\n",
    "        loss_v.backward()\n",
    "        optimizer.step()\n",
    "        res = {\n",
    "            \"loss\": loss_v.item(),\n",
    "            \"epsilon\": 0.0,\n",
    "        }\n",
    "        if engine.state.iteration % params.target_net_sync == 0:\n",
    "            tgt_net.sync()\n",
    "\n",
    "        if args.params.startswith(\"egreedy\"):\n",
    "            epsilon_tracker.frame(engine.state.iteration - epsilon_tracker_frame)\n",
    "            res['epsilon'] = selector.epsilon\n",
    "        # reset noise every training step, this is fine in off-policy method\n",
    "        if args.params == 'noisynet':\n",
    "            net.reset_noise()\n",
    "        return res\n",
    "\n",
    "    engine = Engine(process_batch)\n",
    "    common.setup_ignite(engine, params, exp_source, args.name, extra_metrics=(\n",
    "        'test_reward', 'avg_test_reward', 'test_steps'))\n",
    "\n",
    "    @engine.on(ptan_ignite.EpisodeEvents.EPISODE_COMPLETED)\n",
    "    def check_reward_trigger(trainer: Engine):\n",
    "        global training_enabled, epsilon_tracker_frame\n",
    "        if training_enabled:\n",
    "            return\n",
    "        # check trigger condition to enable epsilon decay\n",
    "        if trainer.state.episode_reward > -200:\n",
    "            training_enabled = True\n",
    "            epsilon_tracker_frame = trainer.state.iteration\n",
    "            print(\"Epsilon decay triggered!\")\n",
    "\n",
    "    @engine.on(ptan_ignite.PeriodEvents.ITERS_1000_COMPLETED)\n",
    "    def test_network(engine):\n",
    "        net.train(False)\n",
    "        obs, _ = test_env.reset()\n",
    "        reward = 0.0\n",
    "        steps = 0\n",
    "\n",
    "        while True:\n",
    "            acts, _ = agent([obs])\n",
    "            obs, r, is_done, is_tr, _ = test_env.step(acts[0])\n",
    "            reward += r\n",
    "            steps += 1\n",
    "            if is_done or is_tr:\n",
    "                break\n",
    "        test_reward_avg = getattr(engine.state, \"test_reward_avg\", None)\n",
    "        if test_reward_avg is None:\n",
    "            test_reward_avg = reward\n",
    "        else:\n",
    "            test_reward_avg = test_reward_avg * 0.95 + 0.05 * reward\n",
    "        engine.state.test_reward_avg = test_reward_avg\n",
    "        print(\"Test done: got %.3f reward after %d steps, avg reward %.3f\" % (\n",
    "            reward, steps, test_reward_avg\n",
    "        ))\n",
    "        engine.state.metrics['test_reward'] = reward\n",
    "        engine.state.metrics['avg_test_reward'] = test_reward_avg\n",
    "        engine.state.metrics['test_steps'] = steps\n",
    "\n",
    "        if test_reward_avg > params.stop_test_reward:\n",
    "            print(\"Reward boundary has crossed, stopping training. Contgrats!\")\n",
    "            engine.should_terminate = True\n",
    "        net.train(True)\n",
    "\n",
    "    engine.run(common.batch_generator(buffer, params.replay_initial, params.batch_size))"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "executable": "/usr/bin/env python3",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
