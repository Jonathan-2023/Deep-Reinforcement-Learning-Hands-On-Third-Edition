{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c706ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import collections\n",
    "import numpy as np\n",
    "import warnings\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from datetime import timedelta, datetime\n",
    "from types import SimpleNamespace\n",
    "from typing import Iterable, List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22d8e36c",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "import ptan\n",
    "import ptan.ignite as ptan_ignite\n",
    "from ignite.engine import Engine\n",
    "from ignite.metrics import RunningAverage\n",
    "from ignite.contrib.handlers import tensorboard_logger as tb_logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5108c862",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 123"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02ea1b71",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75f92548",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unpack_batch(batch: List[ptan.experience.ExperienceFirstLast]):\n",
    "    states, actions, rewards, dones, last_states = [],[],[],[],[]\n",
    "    for exp in batch:\n",
    "        state = np.asarray(exp.state)\n",
    "        states.append(state)\n",
    "        actions.append(exp.action)\n",
    "        rewards.append(exp.reward)\n",
    "        dones.append(exp.last_state is None)\n",
    "        if exp.last_state is None:\n",
    "            lstate = state  # the result will be masked anyway\n",
    "        else:\n",
    "            lstate = np.asarray(exp.last_state)\n",
    "        last_states.append(lstate)\n",
    "    return np.asarray(states, dtype=np.float32), \\\n",
    "           np.array(actions), \\\n",
    "           np.array(rewards, dtype=np.float32), \\\n",
    "           np.array(dones, dtype=bool), \\\n",
    "           np.asarray(last_states, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3616bf32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_loss_dqn(\n",
    "        batch: List[ptan.experience.ExperienceFirstLast],\n",
    "        net: nn.Module,\n",
    "        tgt_net: nn.Module,\n",
    "        gamma: float,\n",
    "        device: torch.device = torch.device(\"cpu\")\n",
    "):\n",
    "    states, actions, rewards, dones, next_states = \\\n",
    "        unpack_batch(batch)\n",
    "\n",
    "    states_v = torch.tensor(states).to(device)\n",
    "    next_states_v = torch.tensor(next_states).to(device)\n",
    "    actions_v = torch.tensor(actions).to(device)\n",
    "    rewards_v = torch.tensor(rewards).to(device)\n",
    "    done_mask = torch.BoolTensor(dones).to(device)\n",
    "\n",
    "    actions_v = actions_v.unsqueeze(-1)\n",
    "    state_action_vals = net(states_v).gather(1, actions_v)\n",
    "    state_action_vals = state_action_vals.squeeze(-1)\n",
    "    with torch.no_grad():\n",
    "        next_state_vals = tgt_net(next_states_v).max(1)[0]\n",
    "        next_state_vals[done_mask] = 0.0\n",
    "\n",
    "    bellman_vals = next_state_vals.detach() * gamma + rewards_v\n",
    "    return nn.MSELoss()(state_action_vals, bellman_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c0ff077",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_loss_double_dqn(\n",
    "        batch: List[ptan.experience.ExperienceFirstLast],\n",
    "        net: nn.Module,\n",
    "        tgt_net: nn.Module,\n",
    "        gamma: float,\n",
    "        device: torch.device = torch.device(\"cpu\")\n",
    "):\n",
    "    states, actions, rewards, dones, next_states = unpack_batch(batch)\n",
    "\n",
    "    states_v = torch.tensor(states).to(device)\n",
    "    actions_v = torch.tensor(actions).to(device)\n",
    "    rewards_v = torch.tensor(rewards).to(device)\n",
    "    done_mask = torch.BoolTensor(dones).to(device)\n",
    "\n",
    "    actions_v = actions_v.unsqueeze(-1)\n",
    "    state_action_vals = net(states_v).gather(1, actions_v)\n",
    "    state_action_vals = state_action_vals.squeeze(-1)\n",
    "    with torch.no_grad():\n",
    "        next_states_v = torch.tensor(next_states).to(device)\n",
    "        next_state_acts = net(next_states_v).max(1)[1]\n",
    "        next_state_acts = next_state_acts.unsqueeze(-1)\n",
    "        next_state_vals = tgt_net(next_states_v).gather(\n",
    "                1, next_state_acts).squeeze(-1)\n",
    "        next_state_vals[done_mask] = 0.0\n",
    "        exp_sa_vals = next_state_vals.detach() * gamma + rewards_v\n",
    "    return nn.MSELoss()(state_action_vals, exp_sa_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e68b5560",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EpsilonTracker:\n",
    "    def __init__(self, selector: ptan.actions.EpsilonGreedyActionSelector,\n",
    "                 params: SimpleNamespace):\n",
    "        self.selector = selector\n",
    "        self.params = params\n",
    "        self.frame(0)\n",
    "\n",
    "    def frame(self, frame_idx: int):\n",
    "        eps = self.params.epsilon_start - \\\n",
    "              frame_idx / self.params.epsilon_frames\n",
    "        self.selector.epsilon = max(self.params.epsilon_final, eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d44b3d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_generator(buffer: ptan.experience.ExperienceReplayBuffer,\n",
    "                    initial: int, batch_size: int):\n",
    "    buffer.populate(initial)\n",
    "    while True:\n",
    "        buffer.populate(1)\n",
    "        yield buffer.sample(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "598ed9d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def calc_values_of_states(states: np.ndarray, net: nn.Module, device: torch.device):\n",
    "    mean_vals = []\n",
    "    for batch in np.array_split(states, 64):\n",
    "        states_v = torch.tensor(batch).to(device)\n",
    "        action_values_v = net(states_v)\n",
    "        best_action_values_v = action_values_v.max(1)[0]\n",
    "        mean_vals.append(best_action_values_v.mean().item())\n",
    "    return np.mean(mean_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aa6a0f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_ignite(engine: Engine, params: SimpleNamespace,\n",
    "                 exp_source, run_name: str,\n",
    "                 extra_metrics: Iterable[str] = ()):\n",
    "    warnings.simplefilter(\"ignore\", category=UserWarning)\n",
    "    handler = ptan_ignite.EndOfEpisodeHandler(\n",
    "        exp_source, bound_avg_reward=params.stop_reward)\n",
    "    handler.attach(engine)\n",
    "    ptan_ignite.EpisodeFPSHandler().attach(engine)\n",
    "\n",
    "    @engine.on(ptan_ignite.EpisodeEvents.EPISODE_COMPLETED)\n",
    "    def episode_completed(trainer: Engine):\n",
    "        passed = trainer.state.metrics.get('time_passed', 0)\n",
    "        print(\"Episode %d: reward=%.2f, steps=%s, \"\n",
    "              \"speed=%.1f f/s, elapsed=%s\" % (\n",
    "            trainer.state.episode, trainer.state.episode_reward,\n",
    "            trainer.state.episode_steps,\n",
    "            trainer.state.metrics.get('avg_fps', 0),\n",
    "            timedelta(seconds=int(passed))))\n",
    "\n",
    "    @engine.on(ptan_ignite.EpisodeEvents.BOUND_REWARD_REACHED)\n",
    "    def game_solved(trainer: Engine):\n",
    "        passed = trainer.state.metrics['time_passed']\n",
    "        print(\"Game solved in %s, after %d episodes \"\n",
    "              \"and %d iterations!\" % (\n",
    "            timedelta(seconds=int(passed)),\n",
    "            trainer.state.episode, trainer.state.iteration))\n",
    "        trainer.should_terminate = True\n",
    "\n",
    "    now = datetime.now().isoformat(timespec='minutes')\n",
    "    logdir = f\"runs/{now}-{params.run_name}-{run_name}\"\n",
    "    tb = tb_logger.TensorboardLogger(log_dir=logdir)\n",
    "    run_avg = RunningAverage(output_transform=lambda v: v['loss'])\n",
    "    run_avg.attach(engine, \"avg_loss\")\n",
    "\n",
    "    metrics = ['reward', 'steps', 'avg_reward']\n",
    "    handler = tb_logger.OutputHandler(\n",
    "        tag=\"episodes\", metric_names=metrics)\n",
    "    event = ptan_ignite.EpisodeEvents.EPISODE_COMPLETED\n",
    "    tb.attach(engine, log_handler=handler, event_name=event)\n",
    "\n",
    "    # write to tensorboard every 100 iterations\n",
    "    ptan_ignite.PeriodicEvents().attach(engine)\n",
    "    metrics = ['avg_loss', 'avg_fps']\n",
    "    metrics.extend(extra_metrics)\n",
    "    handler = tb_logger.OutputHandler(\n",
    "        tag=\"train\", metric_names=metrics,\n",
    "        output_transform=lambda a: a)\n",
    "    event = ptan_ignite.PeriodEvents.ITERS_100_COMPLETED\n",
    "    tb.attach(engine, log_handler=handler, event_name=event)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20ba20af",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PseudoCountRewardWrapper(gym.Wrapper):\n",
    "    def __init__(self, env: gym.Env, hash_function = lambda o: o,\n",
    "                 reward_scale: float = 1.0):\n",
    "        super(PseudoCountRewardWrapper, self).__init__(env)\n",
    "        self.hash_function = hash_function\n",
    "        self.reward_scale = reward_scale\n",
    "        self.counts = collections.Counter()\n",
    "\n",
    "    def step(self, action):\n",
    "        obs, reward, done, is_tr, info = self.env.step(action)\n",
    "        extra_reward = self._count_observation(obs)\n",
    "        return obs, reward + self.reward_scale * extra_reward, done, is_tr, info\n",
    "\n",
    "    def _count_observation(self, obs) -> float:\n",
    "        \"\"\"\n",
    "        Increments observation counter and returns pseudo-count reward\n",
    "        :param obs: observation\n",
    "        :return: extra reward\n",
    "        \"\"\"\n",
    "        h = self.hash_function(obs)\n",
    "        self.counts[h] += 1\n",
    "        return np.sqrt(1/self.counts[h])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5577fc45",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetworkDistillationRewardWrapper(gym.Wrapper):\n",
    "    def __init__(self, env, reward_callable, reward_scale: float = 1.0, sum_rewards: bool = True):\n",
    "        super(NetworkDistillationRewardWrapper, self).__init__(env)\n",
    "        self.reward_scale = reward_scale\n",
    "        self.reward_callable = reward_callable\n",
    "        self.sum_rewards = sum_rewards\n",
    "\n",
    "    def step(self, action):\n",
    "        obs, reward, done, is_tr, info = self.env.step(action)\n",
    "        extra_reward = self.reward_callable(obs)\n",
    "        if self.sum_rewards:\n",
    "            res_rewards = reward + self.reward_scale * extra_reward\n",
    "        else:\n",
    "            res_rewards = np.array([reward, extra_reward * self.reward_scale])\n",
    "        return obs, res_rewards, done, is_tr, info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e87378",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DistillExperienceSource(ptan.experience.ExperienceSource):\n",
    "    \"\"\"\n",
    "    Tweaked version of experience source which sums up reward\n",
    "    \"\"\"\n",
    "    def pop_rewards_steps(self):\n",
    "        res = []\n",
    "        for rewards, steps in super(DistillExperienceSource, self).pop_rewards_steps():\n",
    "            res.append((rewards.sum(), steps))\n",
    "        return res"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
