{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f68416ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ptan\n",
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from typing import Union, Callable, Optional, Tuple, List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8bf672b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from . import dqn_extra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e699e596",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MountainCarBasePPO(nn.Module):\n",
    "    def __init__(self, obs_size: int, n_actions: int, hid_size: int = 64):\n",
    "        super(MountainCarBasePPO, self).__init__()\n",
    "\n",
    "        self.actor = nn.Sequential(\n",
    "            nn.Linear(obs_size, hid_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hid_size, n_actions),\n",
    "        )\n",
    "\n",
    "        self.critic = nn.Sequential(\n",
    "            nn.Linear(obs_size, hid_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hid_size, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        return self.actor(x), self.critic(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "080f686d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MountainCarNoisyNetsPPO(nn.Module):\n",
    "    def __init__(self, obs_size: int, n_actions: int, hid_size: int = 128):\n",
    "        super(MountainCarNoisyNetsPPO, self).__init__()\n",
    "\n",
    "        self.noisy_layers = [\n",
    "            dqn_extra.NoisyLinear(hid_size, n_actions)\n",
    "        ]\n",
    "\n",
    "        self.actor = nn.Sequential(\n",
    "            nn.Linear(obs_size, hid_size),\n",
    "            nn.ReLU(),\n",
    "            self.noisy_layers[0],\n",
    "        )\n",
    "\n",
    "        self.critic = nn.Sequential(\n",
    "            nn.Linear(obs_size, hid_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hid_size, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        return self.actor(x), self.critic(x)\n",
    "\n",
    "    def reset_noise(self):\n",
    "        for l in self.noisy_layers:\n",
    "            l.reset_noise()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0028beee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_adv_ref(values, dones, rewards, gamma, gae_lambda):\n",
    "    last_gae = 0.0\n",
    "    adv, ref = [], []\n",
    "\n",
    "    for val, next_val, done, reward in zip(reversed(values[:-1]), reversed(values[1:]),\n",
    "                                           reversed(dones[:-1]), reversed(rewards[:-1])):\n",
    "        if done:\n",
    "            delta = reward - val\n",
    "            last_gae = delta\n",
    "        else:\n",
    "            delta = reward + gamma * next_val - val\n",
    "            last_gae = delta + gamma * gae_lambda * last_gae\n",
    "        adv.append(last_gae)\n",
    "        ref.append(last_gae + val)\n",
    "    adv = list(reversed(adv))\n",
    "    ref = list(reversed(ref))\n",
    "    return torch.FloatTensor(adv), torch.FloatTensor(ref)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8a95c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_generator(exp_source: ptan.experience.ExperienceSource,\n",
    "                    net: nn.Module,\n",
    "                    trajectory_size: int, ppo_epoches: int,\n",
    "                    batch_size: int, gamma: float, gae_lambda: float,\n",
    "                    device: Union[torch.device, str] = \"cpu\", trim_trajectory: bool = True,\n",
    "                    new_batch_callable: Optional[Callable] = None):\n",
    "    trj_states = []\n",
    "    trj_actions = []\n",
    "    trj_rewards = []\n",
    "    trj_dones = []\n",
    "    last_done_index = None\n",
    "    for (exp,) in exp_source:\n",
    "        trj_states.append(exp.state)\n",
    "        trj_actions.append(exp.action)\n",
    "        trj_rewards.append(exp.reward)\n",
    "        trj_dones.append(exp.done_trunc)\n",
    "        if exp.done_trunc:\n",
    "            last_done_index = len(trj_states)-1\n",
    "        if len(trj_states) < trajectory_size:\n",
    "            continue\n",
    "        # ensure that we have at least one full episode in the trajectory\n",
    "        if last_done_index is None or last_done_index == len(trj_states)-1:\n",
    "            continue\n",
    "\n",
    "        if new_batch_callable is not None:\n",
    "            new_batch_callable()\n",
    "\n",
    "        # trim the trajectory till the last done plus one step (which will be discarded).\n",
    "        # This increases convergence speed and stability\n",
    "        if trim_trajectory:\n",
    "            trj_states = trj_states[:last_done_index+2]\n",
    "            trj_actions = trj_actions[:last_done_index + 2]\n",
    "            trj_rewards = trj_rewards[:last_done_index + 2]\n",
    "            trj_dones = trj_dones[:last_done_index + 2]\n",
    "\n",
    "        trj_states_t = torch.FloatTensor(trj_states).to(device)\n",
    "        trj_actions_t = torch.tensor(trj_actions).to(device)\n",
    "        policy_t, trj_values_t = net(trj_states_t)\n",
    "        trj_values_t = trj_values_t.squeeze()\n",
    "\n",
    "        adv_t, ref_t = calc_adv_ref(trj_values_t.data.cpu().numpy(),\n",
    "                                    trj_dones, trj_rewards, gamma, gae_lambda)\n",
    "        adv_t = adv_t.to(device)\n",
    "        ref_t = ref_t.to(device)\n",
    "\n",
    "        logpolicy_t = F.log_softmax(policy_t, dim=1)\n",
    "        old_logprob_t = logpolicy_t.gather(1, trj_actions_t.unsqueeze(-1)).squeeze(-1)\n",
    "        adv_t = (adv_t - torch.mean(adv_t)) / torch.std(adv_t)\n",
    "        old_logprob_t = old_logprob_t.detach()\n",
    "\n",
    "        # make our trajectory splittable on even batch chunks\n",
    "        trj_len = len(trj_states) - 1\n",
    "        trj_len -= trj_len % batch_size\n",
    "        trj_len += 1\n",
    "        indices = np.arange(0, trj_len-1)\n",
    "\n",
    "        # generate needed amount of batches\n",
    "        for _ in range(ppo_epoches):\n",
    "            np.random.shuffle(indices)\n",
    "            for batch_indices in np.split(indices, trj_len // batch_size):\n",
    "                yield (\n",
    "                    trj_states_t[batch_indices],\n",
    "                    trj_actions_t[batch_indices],\n",
    "                    adv_t[batch_indices],\n",
    "                    ref_t[batch_indices],\n",
    "                    old_logprob_t[batch_indices],\n",
    "                )\n",
    "\n",
    "        trj_states.clear()\n",
    "        trj_actions.clear()\n",
    "        trj_rewards.clear()\n",
    "        trj_dones.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d076b120",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_generator_distill(exp_source: ptan.experience.ExperienceSource,\n",
    "                            net: nn.Module, trajectory_size: int, ppo_epoches: int,\n",
    "                            batch_size: int, gamma: float, gae_lambda: float,\n",
    "                            device: Union[torch.device, str] = \"cpu\", trim_trajectory: bool = True,\n",
    "                            new_batch_callable: Optional[Callable] = None):\n",
    "    \"\"\"\n",
    "    Same logic as batch_generator, but with distillery networks\n",
    "    \"\"\"\n",
    "    trj_states = []\n",
    "    trj_actions = []\n",
    "    trj_rewards = []\n",
    "    trj_rewards_ext = []\n",
    "    trj_rewards_int = []\n",
    "    trj_dones = []\n",
    "    last_done_index = None\n",
    "    trj_time = time.time()\n",
    "    for (exp,) in exp_source:\n",
    "        trj_states.append(exp.state)\n",
    "        trj_actions.append(exp.action)\n",
    "        trj_rewards_ext.append(exp.reward[0])\n",
    "        trj_rewards_int.append(exp.reward[1])\n",
    "        trj_rewards.append(exp.reward.sum())\n",
    "        trj_dones.append(exp.done_trunc)\n",
    "        if exp.done_trunc:\n",
    "            last_done_index = len(trj_states)-1\n",
    "        if len(trj_states) < trajectory_size:\n",
    "            continue\n",
    "        # ensure that we have at least one full episode in the trajectory\n",
    "        if last_done_index is None or last_done_index == len(trj_states)-1:\n",
    "            continue\n",
    "\n",
    "        trj_dt = time.time() - trj_time\n",
    "\n",
    "        if new_batch_callable is not None:\n",
    "            new_batch_callable()\n",
    "\n",
    "        prep_ts = time.time()\n",
    "        # trim the trajectory till the last done plus one step (which will be discarded).\n",
    "        # This increases convergence speed and stability\n",
    "        if trim_trajectory:\n",
    "            trj_states = trj_states[:last_done_index+2]\n",
    "            trj_actions = trj_actions[:last_done_index + 2]\n",
    "            trj_rewards_ext = trj_rewards_ext[:last_done_index + 2]\n",
    "            trj_rewards_int = trj_rewards_int[:last_done_index + 2]\n",
    "            trj_rewards = trj_rewards[:last_done_index + 2]\n",
    "            trj_dones = trj_dones[:last_done_index + 2]\n",
    "\n",
    "        trj_states_t = torch.FloatTensor(trj_states).to(device)\n",
    "        trj_actions_t = torch.tensor(trj_actions).to(device)\n",
    "        policy_t, trj_values_ext_t, trj_values_int_t = net(trj_states_t)\n",
    "        trj_values_ext_t = trj_values_ext_t.squeeze()\n",
    "        trj_values_int_t = trj_values_int_t.squeeze()\n",
    "\n",
    "        # calculate combined rewards advantage\n",
    "        adv_t, _ = calc_adv_ref((trj_values_ext_t + trj_values_int_t).data.cpu().numpy(),\n",
    "                                trj_dones, trj_rewards, gamma, gae_lambda)\n",
    "        adv_t = adv_t.to(device)\n",
    "\n",
    "        # intrinistic and extrinistic reference values\n",
    "        _, ref_ext_t = calc_adv_ref(trj_values_ext_t.data.cpu().numpy(),\n",
    "                                    trj_dones, trj_rewards_ext, gamma, gae_lambda)\n",
    "        ref_ext_t = ref_ext_t.to(device)\n",
    "\n",
    "        _, ref_int_t = calc_adv_ref(trj_values_int_t.data.cpu().numpy(),\n",
    "                                    trj_dones, trj_rewards_int, gamma, gae_lambda)\n",
    "        ref_int_t = ref_int_t.to(device)\n",
    "\n",
    "        logpolicy_t = F.log_softmax(policy_t, dim=1)\n",
    "        old_logprob_t = logpolicy_t.gather(1, trj_actions_t.unsqueeze(-1)).squeeze(-1)\n",
    "        adv_t = (adv_t - torch.mean(adv_t)) / torch.std(adv_t)\n",
    "        old_logprob_t = old_logprob_t.detach()\n",
    "\n",
    "        # make our trajectory splittable on even batch chunks\n",
    "        trj_len = len(trj_states) - 1\n",
    "        trj_len -= trj_len % batch_size\n",
    "        trj_len += 1\n",
    "        indices = np.arange(0, trj_len-1)\n",
    "        prep_dt = time.time() - prep_ts\n",
    "\n",
    "        # generate needed amount of batches\n",
    "        for _ in range(ppo_epoches):\n",
    "            np.random.shuffle(indices)\n",
    "            for batch_indices in np.split(indices, trj_len // batch_size):\n",
    "                yield (\n",
    "                    trj_states_t[batch_indices],\n",
    "                    trj_actions_t[batch_indices],\n",
    "                    adv_t[batch_indices],\n",
    "                    ref_ext_t[batch_indices],\n",
    "                    ref_int_t[batch_indices],\n",
    "                    old_logprob_t[batch_indices],\n",
    "                    trj_dt,\n",
    "                    prep_dt,\n",
    "                )\n",
    "\n",
    "        trj_states.clear()\n",
    "        trj_actions.clear()\n",
    "        trj_rewards.clear()\n",
    "        trj_rewards_ext.clear()\n",
    "        trj_rewards_int.clear()\n",
    "        trj_dones.clear()\n",
    "        trj_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a841d0bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MountainCarNetDistillery(nn.Module):\n",
    "    def __init__(self, obs_size: int, hid_size: int = 128):\n",
    "        super(MountainCarNetDistillery, self).__init__()\n",
    "\n",
    "        self.ref_net = nn.Sequential(\n",
    "            nn.Linear(obs_size, hid_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hid_size, hid_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hid_size, 1),\n",
    "        )\n",
    "        self.ref_net.train(False)\n",
    "\n",
    "        self.trn_net = nn.Sequential(\n",
    "            nn.Linear(obs_size, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.ref_net(x), self.trn_net(x)\n",
    "\n",
    "    def extra_reward(self, obs):\n",
    "        r1, r2 = self.forward(torch.FloatTensor([obs]))\n",
    "        return (r1 - r2).abs().detach().numpy()[0][0]\n",
    "\n",
    "    def loss(self, obs_t):\n",
    "        r1_t, r2_t = self.forward(obs_t)\n",
    "        return F.mse_loss(r2_t, r1_t).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fbb8921",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AtariBasePPO(nn.Module):\n",
    "    \"\"\"\n",
    "    Dueling net\n",
    "    \"\"\"\n",
    "    def __init__(self, input_shape, n_actions):\n",
    "        super(AtariBasePPO, self).__init__()\n",
    "\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(input_shape[0], 32,\n",
    "                      kernel_size=8, stride=4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        conv_out_size = self._get_conv_out(input_shape)\n",
    "        self.actor = nn.Sequential(\n",
    "            nn.Linear(conv_out_size, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, n_actions)\n",
    "        )\n",
    "        self.critic = nn.Sequential(\n",
    "            nn.Linear(conv_out_size, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 1)\n",
    "        )\n",
    "\n",
    "    def _get_conv_out(self, shape):\n",
    "        o = self.conv(torch.zeros(1, *shape))\n",
    "        return int(np.prod(o.size()))\n",
    "\n",
    "    def forward(self, x):\n",
    "        fx = x.float() / 256\n",
    "        conv_out = self.conv(fx).view(fx.size()[0], -1)\n",
    "        return self.actor(conv_out), self.critic(conv_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d99b1454",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AtariNoisyNetsPPO(nn.Module):\n",
    "    \"\"\"\n",
    "    Dueling net\n",
    "    \"\"\"\n",
    "    def __init__(self, input_shape, n_actions):\n",
    "        super(AtariNoisyNetsPPO, self).__init__()\n",
    "\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(input_shape[0], 32,\n",
    "                      kernel_size=8, stride=4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        conv_out_size = self._get_conv_out(input_shape)\n",
    "        self.noisy_layers = [\n",
    "            dqn_extra.NoisyLinear(conv_out_size, 256),\n",
    "            dqn_extra.NoisyLinear(256, n_actions),\n",
    "        ]\n",
    "\n",
    "        self.actor = nn.Sequential(\n",
    "            self.noisy_layers[0],\n",
    "            nn.ReLU(),\n",
    "            self.noisy_layers[1],\n",
    "        )\n",
    "        self.critic = nn.Sequential(\n",
    "            nn.Linear(conv_out_size, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 1)\n",
    "        )\n",
    "\n",
    "    def _get_conv_out(self, shape):\n",
    "        o = self.conv(torch.zeros(1, *shape))\n",
    "        return int(np.prod(o.size()))\n",
    "\n",
    "    def forward(self, x):\n",
    "        fx = x.float() / 256\n",
    "        conv_out = self.conv(fx).view(fx.size()[0], -1)\n",
    "        return self.actor(conv_out), self.critic(conv_out)\n",
    "\n",
    "    def reset_noise(self):\n",
    "        for l in self.noisy_layers:\n",
    "            l.reset_noise()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25347270",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AtariDistill(nn.Module):\n",
    "    \"\"\"\n",
    "    Network to be distilled\n",
    "    \"\"\"\n",
    "    def __init__(self, input_shape):\n",
    "        super(AtariDistill, self).__init__()\n",
    "\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(input_shape[0], 32,\n",
    "                      kernel_size=8, stride=4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        conv_out_size = self._get_conv_out(input_shape)\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(conv_out_size, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 1)\n",
    "        )\n",
    "\n",
    "    def _get_conv_out(self, shape):\n",
    "        o = self.conv(torch.zeros(1, *shape))\n",
    "        return int(np.prod(o.size()))\n",
    "\n",
    "    def forward(self, x):\n",
    "        fx = x.float() / 256\n",
    "        conv_out = self.conv(fx).view(fx.size()[0], -1)\n",
    "        return self.ff(conv_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1c14816",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AtariDistillPPO(nn.Module):\n",
    "    \"\"\"\n",
    "    Dueling net\n",
    "    \"\"\"\n",
    "    def __init__(self, input_shape, n_actions):\n",
    "        super(AtariDistillPPO, self).__init__()\n",
    "\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(input_shape[0], 32,\n",
    "                      kernel_size=8, stride=4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        conv_out_size = self._get_conv_out(input_shape)\n",
    "        self.actor = nn.Sequential(\n",
    "            nn.Linear(conv_out_size, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, n_actions)\n",
    "        )\n",
    "        self.critic_ext = nn.Sequential(\n",
    "            nn.Linear(conv_out_size, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 1)\n",
    "        )\n",
    "        self.critic_int = nn.Sequential(\n",
    "            nn.Linear(conv_out_size, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 1)\n",
    "        )\n",
    "\n",
    "    def _get_conv_out(self, shape):\n",
    "        o = self.conv(torch.zeros(1, *shape))\n",
    "        return int(np.prod(o.size()))\n",
    "\n",
    "    def forward(self, x):\n",
    "        fx = x.float() / 256\n",
    "        conv_out = self.conv(fx).view(fx.size()[0], -1)\n",
    "        return self.actor(conv_out), self.critic_ext(conv_out), self.critic_int(conv_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd1740b1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
