{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11ee76b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ptan\n",
    "import ptan.ignite as ptan_ignite\n",
    "import gymnasium as gym\n",
    "import argparse\n",
    "import random\n",
    "import time\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ca8ea3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ignite.engine import Engine\n",
    "from types import SimpleNamespace\n",
    "from lib import common, ppo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14f2b530",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_ENVS = 8\n",
    "NAME = \"atari\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff6cdae6",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "HYPERPARAMS = {\n",
    "    'ppo': SimpleNamespace(**{\n",
    "        'env_name':         \"SeaquestNoFrameskip-v4\",\n",
    "        'stop_reward':      None,\n",
    "        'stop_test_reward': 10000,\n",
    "        'run_name':         'ppo',\n",
    "        'lr':               1e-5,\n",
    "        'gamma':            0.99,\n",
    "        'ppo_trajectory':   1025,\n",
    "        'ppo_epoches':      4,\n",
    "        'ppo_eps':          0.2,\n",
    "        'batch_size':       64,\n",
    "        'gae_lambda':       0.95,\n",
    "        'entropy_beta':     0.1,\n",
    "    }),\n",
    "    'noisynet': SimpleNamespace(**{\n",
    "        'env_name':         \"SeaquestNoFrameskip-v4\",\n",
    "        'stop_reward':      None,\n",
    "        'stop_test_reward': 10000,\n",
    "        'run_name':         'noisynet',\n",
    "        'lr':               1e-5,\n",
    "        'gamma':            0.99,\n",
    "        'ppo_trajectory':   1025,\n",
    "        'ppo_epoches':      2,\n",
    "        'ppo_eps':          0.2,\n",
    "        'batch_size':       64,\n",
    "        'gae_lambda':       0.95,\n",
    "        'entropy_beta':     0.01,\n",
    "    }),\n",
    "    'distill': SimpleNamespace(**{\n",
    "        'env_name':         \"SeaquestNoFrameskip-v4\",\n",
    "        'stop_reward':      None,\n",
    "        'stop_test_reward': 10000,\n",
    "        'run_name':         'distill',\n",
    "        'lr':               5e-5,\n",
    "        'gamma':            0.99,\n",
    "        'ppo_trajectory':   1025,\n",
    "        'ppo_epoches':      4,\n",
    "        'ppo_eps':          0.2,\n",
    "        'batch_size':       64,\n",
    "        'gae_lambda':       0.95,\n",
    "        'entropy_beta':     0.1,\n",
    "        'lr_distill':       1e-6,\n",
    "        'distill_scale':    100.0,\n",
    "    }),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c41043a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    random.seed(common.SEED)\n",
    "    torch.manual_seed(common.SEED)\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--dev\", default=\"cpu\", help=\"Device to use\")\n",
    "    parser.add_argument(\"-n\", \"--name\", required=True, help=\"Run name\")\n",
    "    parser.add_argument(\"-p\", \"--params\", default='ppo', choices=list(HYPERPARAMS.keys()),\n",
    "                        help=\"Parameters, default=ppo\")\n",
    "    args = parser.parse_args()\n",
    "    params = HYPERPARAMS[args.params]\n",
    "    device = torch.device(args.dev)\n",
    "\n",
    "    test_env = gym.make(params.env_name)\n",
    "    test_env = ptan.common.wrappers.wrap_dqn(test_env)\n",
    "\n",
    "    do_distill = False\n",
    "    dist_ref = dist_trn = None\n",
    "\n",
    "    if args.params == 'noisynet':\n",
    "        net = ppo.AtariNoisyNetsPPO(test_env.observation_space.shape, test_env.action_space.n).to(device)\n",
    "    elif args.params == 'distill':\n",
    "        net = ppo.AtariDistillPPO(test_env.observation_space.shape, test_env.action_space.n).to(device)\n",
    "        do_distill = True\n",
    "        dist_ref = ppo.AtariDistill(test_env.observation_space.shape).to(device)\n",
    "        dist_ref.train(False)\n",
    "        dist_trn = ppo.AtariDistill(test_env.observation_space.shape).to(device)\n",
    "    else:\n",
    "        net = ppo.AtariBasePPO(test_env.observation_space.shape, test_env.action_space.n).to(device)\n",
    "    print(net)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def get_distill_reward(obs) -> float:\n",
    "        obs_t = torch.FloatTensor([obs]).to(device)\n",
    "        res = (dist_ref(obs_t) - dist_trn(obs_t)).abs()[0][0].item()\n",
    "        return res\n",
    "\n",
    "    envs = []\n",
    "    for _ in range(N_ENVS):\n",
    "        env = gym.make(params.env_name)\n",
    "        env = ptan.common.wrappers.wrap_dqn(env)\n",
    "        if do_distill:\n",
    "            env = common.NetworkDistillationRewardWrapper(\n",
    "                env, reward_callable=get_distill_reward,\n",
    "                reward_scale=params.distill_scale, sum_rewards=False)\n",
    "        envs.append(env)\n",
    "\n",
    "    agent = ptan.agent.PolicyAgent(lambda x: net(x)[0], apply_softmax=True, preprocessor=ptan.agent.float32_preprocessor,\n",
    "                                   device=device)\n",
    "    if do_distill:\n",
    "        exp_source = common.DistillExperienceSource(envs, agent, steps_count=1)\n",
    "    else:\n",
    "        exp_source = ptan.experience.ExperienceSource(envs, agent, steps_count=1)\n",
    "\n",
    "    optimizer = optim.Adam(net.parameters(), lr=params.lr)\n",
    "    if do_distill:\n",
    "        distill_optimizer = optim.Adam(dist_trn.parameters(), lr=params.lr_distill)\n",
    "\n",
    "    def process_batch(engine, batch):\n",
    "        start_ts = time.time()\n",
    "        optimizer.zero_grad()\n",
    "        res = {}\n",
    "\n",
    "        if do_distill:\n",
    "            states_t, actions_t, adv_t, ref_ext_t, ref_int_t, old_logprob_t, trj_dt, prep_dt = batch\n",
    "            res['time_traj'] = trj_dt\n",
    "            res['time_prep'] = prep_dt\n",
    "            policy_t, value_ext_t, value_int_t = net(states_t)\n",
    "            loss_value_ext_t = F.mse_loss(value_ext_t.squeeze(-1), ref_ext_t)\n",
    "            loss_value_int_t = F.mse_loss(value_int_t.squeeze(-1), ref_int_t)\n",
    "            res['loss_value_ext'] = loss_value_ext_t.item()\n",
    "            res['loss_value_int'] = loss_value_int_t.item()\n",
    "            loss_value_t = loss_value_ext_t + loss_value_int_t\n",
    "            res['ref_ext'] = ref_ext_t.mean().item()\n",
    "            res['ref_int'] = ref_int_t.mean().item()\n",
    "        else:\n",
    "            states_t, actions_t, adv_t, ref_t, old_logprob_t = batch\n",
    "            policy_t, value_t = net(states_t)\n",
    "            loss_value_t = F.mse_loss(value_t.squeeze(-1), ref_t)\n",
    "            res['ref'] = ref_t.mean().item()\n",
    "\n",
    "        logpolicy_t = F.log_softmax(policy_t, dim=1)\n",
    "\n",
    "        prob_t = F.softmax(policy_t, dim=1)\n",
    "        loss_entropy_t = (prob_t * logpolicy_t).sum(dim=1).mean()\n",
    "\n",
    "        logprob_t = logpolicy_t.gather(1, actions_t.unsqueeze(-1)).squeeze(-1)\n",
    "        ratio_t = torch.exp(logprob_t - old_logprob_t)\n",
    "        surr_obj_t = adv_t * ratio_t\n",
    "        clipped_surr_t = adv_t * torch.clamp(ratio_t, 1.0 - params.ppo_eps, 1.0 + params.ppo_eps)\n",
    "        loss_policy_t = -torch.min(surr_obj_t, clipped_surr_t).mean()\n",
    "\n",
    "        loss_t = params.entropy_beta * loss_entropy_t + loss_policy_t + loss_value_t\n",
    "        loss_t.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # perform distillation training\n",
    "        if do_distill:\n",
    "            distill_optimizer.zero_grad()\n",
    "            trn_out_t = dist_trn(states_t)\n",
    "            ref_out_t = dist_ref(states_t)\n",
    "            dist_loss_t = F.mse_loss(ref_out_t, trn_out_t)\n",
    "            dist_loss_t.backward()\n",
    "            distill_optimizer.step()\n",
    "            res[\"loss_distill\"] = dist_loss_t.item()\n",
    "\n",
    "        res.update({\n",
    "            \"loss\": loss_t.item(),\n",
    "            \"loss_value\": loss_value_t.item(),\n",
    "            \"loss_policy\": loss_policy_t.item(),\n",
    "            \"adv\": adv_t.mean().item(),\n",
    "            \"loss_entropy\": loss_entropy_t.item(),\n",
    "            \"time_batch\": time.time() - start_ts,\n",
    "        })\n",
    "\n",
    "        return res\n",
    "\n",
    "\n",
    "    engine = Engine(process_batch)\n",
    "    common.setup_ignite(engine, params, exp_source, NAME + \"_\" + args.name, extra_metrics=(\n",
    "        'test_reward', 'avg_test_reward', 'test_steps'))\n",
    "\n",
    "    @engine.on(ptan_ignite.PeriodEvents.ITERS_10000_COMPLETED)\n",
    "    def test_network(engine):\n",
    "        net.actor.train(False)\n",
    "        obs, _ = test_env.reset()\n",
    "        reward = 0.0\n",
    "        steps = 0\n",
    "\n",
    "        while True:\n",
    "            acts, _ = agent([obs])\n",
    "            obs, r, is_done, is_tr, _ = test_env.step(acts[0])\n",
    "            reward += r\n",
    "            steps += 1\n",
    "            if is_done or is_tr:\n",
    "                break\n",
    "        test_reward_avg = getattr(engine.state, \"test_reward_avg\", None)\n",
    "        if test_reward_avg is None:\n",
    "            test_reward_avg = reward\n",
    "        else:\n",
    "            test_reward_avg = test_reward_avg * 0.95 + 0.05 * reward\n",
    "        engine.state.test_reward_avg = test_reward_avg\n",
    "        print(\"Test done: got %.3f reward after %d steps, avg reward %.3f\" % (\n",
    "            reward, steps, test_reward_avg\n",
    "        ))\n",
    "        engine.state.metrics['test_reward'] = reward\n",
    "        engine.state.metrics['avg_test_reward'] = test_reward_avg\n",
    "        engine.state.metrics['test_steps'] = steps\n",
    "\n",
    "        if test_reward_avg > params.stop_test_reward:\n",
    "            print(\"Reward boundary has crossed, stopping training. Contgrats!\")\n",
    "            engine.should_terminate = True\n",
    "        net.actor.train(True)\n",
    "\n",
    "    def new_ppo_batch():\n",
    "        # In noisy networks we need to reset the noise\n",
    "        if args.params == 'noisynet':\n",
    "            net.reset_noise()\n",
    "\n",
    "    if do_distill:\n",
    "        engine.run(ppo.batch_generator_distill(exp_source, net, params.ppo_trajectory,\n",
    "                                               params.ppo_epoches, params.batch_size,\n",
    "                                               params.gamma, params.gae_lambda, device=device,\n",
    "                                               trim_trajectory=False, new_batch_callable=new_ppo_batch))\n",
    "    else:\n",
    "        engine.run(ppo.batch_generator(exp_source, net, params.ppo_trajectory,\n",
    "                                       params.ppo_epoches, params.batch_size,\n",
    "                                       params.gamma, params.gae_lambda, device=device,\n",
    "                                       trim_trajectory=False, new_batch_callable=new_ppo_batch))"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "executable": "/usr/bin/env python3",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
