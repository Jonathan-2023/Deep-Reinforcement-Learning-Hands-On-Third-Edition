{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65b54398",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ptan\n",
    "import ptan.ignite as ptan_ignite\n",
    "import gymnasium as gym\n",
    "import argparse\n",
    "import random\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d362c2aa",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "from ignite.engine import Engine\n",
    "from types import SimpleNamespace\n",
    "from lib import common, ppo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cb5d3d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "HYPERPARAMS = {\n",
    "    'debug': SimpleNamespace(**{\n",
    "        'env_name':         \"CartPole-v0\",\n",
    "        'stop_reward':      None,\n",
    "        'stop_test_reward': 190.0,\n",
    "        'run_name':         'debug',\n",
    "        'actor_lr':         1e-4,\n",
    "        'critic_lr':        1e-4,\n",
    "        'gamma':            0.9,\n",
    "        'ppo_trajectory':   2049,\n",
    "        'ppo_epoches':      10,\n",
    "        'ppo_eps':          0.2,\n",
    "        'batch_size':       32,\n",
    "        'gae_lambda':       0.95,\n",
    "        'entropy_beta':     0.1,\n",
    "    }),\n",
    "    'ppo': SimpleNamespace(**{\n",
    "        'env_name':         \"MountainCar-v0\",\n",
    "        'stop_reward':      None,\n",
    "        'stop_test_reward': -130.0,\n",
    "        'run_name':         'ppo',\n",
    "        'actor_lr':         1e-4,\n",
    "        'critic_lr':        1e-4,\n",
    "        'gamma':            0.99,\n",
    "        'ppo_trajectory':   2049,\n",
    "        'ppo_epoches':      10,\n",
    "        'ppo_eps':          0.2,\n",
    "        'batch_size':       32,\n",
    "        'gae_lambda':       0.95,\n",
    "        'entropy_beta':     0.1,\n",
    "    }),\n",
    "    'noisynet': SimpleNamespace(**{\n",
    "        'env_name':         \"MountainCar-v0\",\n",
    "        'stop_reward':      None,\n",
    "        'stop_test_reward': -130.0,\n",
    "        'run_name':         'noisynet',\n",
    "        'actor_lr':         1e-4,\n",
    "        'critic_lr':        1e-4,\n",
    "        'gamma':            0.99,\n",
    "        'ppo_trajectory':   2049,\n",
    "        'ppo_epoches':      10,\n",
    "        'ppo_eps':          0.2,\n",
    "        'batch_size':       32,\n",
    "        'gae_lambda':       0.95,\n",
    "        'entropy_beta':     0.1,\n",
    "    }),\n",
    "    'counts': SimpleNamespace(**{\n",
    "        'env_name':         \"MountainCar-v0\",\n",
    "        'stop_reward':      None,\n",
    "        'stop_test_reward': -130.0,\n",
    "        'run_name':         'counts',\n",
    "        'actor_lr':         1e-4,\n",
    "        'critic_lr':        1e-4,\n",
    "        'gamma':            0.99,\n",
    "        'ppo_trajectory':   2049,\n",
    "        'ppo_epoches':      10,\n",
    "        'ppo_eps':          0.2,\n",
    "        'batch_size':       32,\n",
    "        'gae_lambda':       0.95,\n",
    "        'entropy_beta':     0.1,\n",
    "        'counts_reward_scale': 0.5,\n",
    "    }),\n",
    "\n",
    "    'distill': SimpleNamespace(**{\n",
    "        'env_name': \"MountainCar-v0\",\n",
    "        'stop_reward': None,\n",
    "        'stop_test_reward': -130.0,\n",
    "        'run_name': 'distill',\n",
    "        'actor_lr': 1e-4,\n",
    "        'critic_lr': 1e-4,\n",
    "        'gamma': 0.99,\n",
    "        'ppo_trajectory': 2049,\n",
    "        'ppo_epoches': 10,\n",
    "        'ppo_eps': 0.2,\n",
    "        'batch_size': 32,\n",
    "        'gae_lambda': 0.95,\n",
    "        'entropy_beta': 0.1,\n",
    "        'reward_scale': 100.0,\n",
    "        'distill_lr': 1e-5,\n",
    "    }),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8539ce6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def counts_hash(obs):\n",
    "    r = obs.tolist()\n",
    "    return tuple(map(lambda v: round(v, 3), r))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cc56d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    random.seed(common.SEED)\n",
    "    torch.manual_seed(common.SEED)\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"-n\", \"--name\", required=True, help=\"Run name\")\n",
    "    parser.add_argument(\"-p\", \"--params\", default='ppo', choices=list(HYPERPARAMS.keys()),\n",
    "                        help=\"Parameters, default=ppo\")\n",
    "    args = parser.parse_args()\n",
    "    params = HYPERPARAMS[args.params]\n",
    "\n",
    "    env = gym.make(params.env_name)\n",
    "    test_env = gym.make(params.env_name)\n",
    "    if args.params == 'counts':\n",
    "        env = common.PseudoCountRewardWrapper(env, reward_scale=params.counts_reward_scale,\n",
    "                                              hash_function=counts_hash)\n",
    "    net_distill = None\n",
    "    if args.params == 'distill':\n",
    "        net_distill = ppo.MountainCarNetDistillery(env.observation_space.shape[0])\n",
    "        env = common.NetworkDistillationRewardWrapper(env, net_distill.extra_reward, reward_scale=params.reward_scale)\n",
    "\n",
    "    if args.params == 'noisynet':\n",
    "        net = ppo.MountainCarNoisyNetsPPO(env.observation_space.shape[0], env.action_space.n)\n",
    "    else:\n",
    "        net = ppo.MountainCarBasePPO(env.observation_space.shape[0], env.action_space.n)\n",
    "    print(net)\n",
    "\n",
    "    agent = ptan.agent.PolicyAgent(net.actor, apply_softmax=True, preprocessor=ptan.agent.float32_preprocessor)\n",
    "    exp_source = ptan.experience.ExperienceSource(env, agent, steps_count=1, env_seed=common.SEED)\n",
    "    opt_actor = optim.Adam(net.actor.parameters(), lr=params.actor_lr)\n",
    "    opt_critic = optim.Adam(net.critic.parameters(), lr=params.critic_lr)\n",
    "    if net_distill is not None:\n",
    "        opt_distill = optim.Adam(net_distill.trn_net.parameters(), lr=params.distill_lr)\n",
    "\n",
    "    def process_batch(engine, batch):\n",
    "        states_t, actions_t, adv_t, ref_t, old_logprob_t = batch\n",
    "\n",
    "        opt_critic.zero_grad()\n",
    "        value_t = net.critic(states_t)\n",
    "        loss_value_t = F.mse_loss(value_t.squeeze(-1), ref_t)\n",
    "        loss_value_t.backward()\n",
    "        opt_critic.step()\n",
    "\n",
    "        opt_actor.zero_grad()\n",
    "        policy_t = net.actor(states_t)\n",
    "        logpolicy_t = F.log_softmax(policy_t, dim=1)\n",
    "\n",
    "        prob_t = F.softmax(policy_t, dim=1)\n",
    "        loss_entropy_t = (prob_t * logpolicy_t).sum(dim=1).mean()\n",
    "\n",
    "        logprob_t = logpolicy_t.gather(1, actions_t.unsqueeze(-1)).squeeze(-1)\n",
    "        ratio_t = torch.exp(logprob_t - old_logprob_t)\n",
    "        surr_obj_t = adv_t * ratio_t\n",
    "        clipped_surr_t = adv_t * torch.clamp(ratio_t, 1.0 - params.ppo_eps, 1.0 + params.ppo_eps)\n",
    "        loss_policy_t = -torch.min(surr_obj_t, clipped_surr_t).mean()\n",
    "        loss_polent_t = params.entropy_beta * loss_entropy_t + loss_policy_t\n",
    "        loss_polent_t.backward()\n",
    "        opt_actor.step()\n",
    "\n",
    "        res = {\n",
    "            \"loss\": loss_value_t.item() + loss_polent_t.item(),\n",
    "            \"loss_value\": loss_value_t.item(),\n",
    "            \"loss_policy\": loss_policy_t.item(),\n",
    "            \"adv\": adv_t.mean().item(),\n",
    "            \"ref\": ref_t.mean().item(),\n",
    "            \"loss_entropy\": loss_entropy_t.item(),\n",
    "        }\n",
    "\n",
    "        if net_distill is not None:\n",
    "            opt_distill.zero_grad()\n",
    "            loss_distill_t = net_distill.loss(states_t)\n",
    "            loss_distill_t.backward()\n",
    "            opt_distill.step()\n",
    "            res['loss_distill'] = loss_distill_t.item()\n",
    "\n",
    "        return res\n",
    "\n",
    "\n",
    "    engine = Engine(process_batch)\n",
    "    common.setup_ignite(engine, params, exp_source, args.name, extra_metrics=(\n",
    "        'test_reward', 'avg_test_reward', 'test_steps'))\n",
    "\n",
    "    @engine.on(ptan_ignite.PeriodEvents.ITERS_1000_COMPLETED)\n",
    "    def test_network(engine):\n",
    "        net.actor.train(False)\n",
    "        obs, _ = test_env.reset()\n",
    "        reward = 0.0\n",
    "        steps = 0\n",
    "\n",
    "        while True:\n",
    "            acts, _ = agent([obs])\n",
    "            obs, r, is_done, is_tr, _ = test_env.step(acts[0])\n",
    "            reward += r\n",
    "            steps += 1\n",
    "            if is_done or is_tr:\n",
    "                break\n",
    "        test_reward_avg = getattr(engine.state, \"test_reward_avg\", None)\n",
    "        if test_reward_avg is None:\n",
    "            test_reward_avg = reward\n",
    "        else:\n",
    "            test_reward_avg = test_reward_avg * 0.95 + 0.05 * reward\n",
    "        engine.state.test_reward_avg = test_reward_avg\n",
    "        print(\"Test done: got %.3f reward after %d steps, avg reward %.3f\" % (\n",
    "            reward, steps, test_reward_avg\n",
    "        ))\n",
    "        engine.state.metrics['test_reward'] = reward\n",
    "        engine.state.metrics['avg_test_reward'] = test_reward_avg\n",
    "        engine.state.metrics['test_steps'] = steps\n",
    "\n",
    "        if test_reward_avg > params.stop_test_reward:\n",
    "            print(\"Reward boundary has crossed, stopping training. Contgrats!\")\n",
    "            engine.should_terminate = True\n",
    "        net.actor.train(True)\n",
    "\n",
    "    def new_ppo_batch():\n",
    "        # In noisy networks we need to reset the noise\n",
    "        if args.params == 'noisynet':\n",
    "            net.reset_noise()\n",
    "\n",
    "    engine.run(ppo.batch_generator(exp_source, net, params.ppo_trajectory,\n",
    "                                   params.ppo_epoches, params.batch_size,\n",
    "                                   params.gamma, params.gae_lambda, new_batch_callable=new_ppo_batch))"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "executable": "/usr/bin/env python3",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
