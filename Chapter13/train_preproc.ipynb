{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80f66769",
   "metadata": {},
   "outputs": [],
   "source": [
    "from textworld import gym\n",
    "from textworld.gym import register_games\n",
    "import ptan\n",
    "import pathlib\n",
    "import argparse\n",
    "import itertools\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c36408cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from textworld import EnvInfos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c4f7fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib import preproc, model, common"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54084641",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "from ignite.engine import Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "503ceb72",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "EXTRA_GAME_INFO = {\n",
    "    \"inventory\": True,\n",
    "    \"description\": True,\n",
    "    \"intermediate_reward\": True,\n",
    "    \"admissible_commands\": True,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4b2313c",
   "metadata": {},
   "outputs": [],
   "source": [
    "GAMMA = 0.9\n",
    "LEARNING_RATE = 5e-5\n",
    "BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ec4fd2b",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# transformations we're applying. Below are default settings, you can switch them on with command line\n",
    "# track visited rooms in observations - LocationWrapper\n",
    "FLAGS_SEEN_ROOMS = False\n",
    "# convert actions into relative - RelativeDirectionWrapper\n",
    "FLAGS_RELATIVE_WRAPPER = False\n",
    "# add game objective to observation\n",
    "FLAGS_OBJECTIVE = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5831e7b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"-g\", \"--game\", default=\"simple\",\n",
    "                        help=\"Game prefix to be used during training, default=simple\")\n",
    "    parser.add_argument(\"--params\", choices=list(common.PARAMS.keys()), default='small',\n",
    "                        help=\"Training params, could be one of %s\" % (list(common.PARAMS.keys())))\n",
    "    parser.add_argument(\"-s\", \"--suffices\", type=int, default=1,\n",
    "                        help=\"Count of game indices to use during training, default=1\")\n",
    "    parser.add_argument(\"-v\", \"--validation\", default='-val',\n",
    "                        help=\"Suffix for game used for validation, default=-val\")\n",
    "    parser.add_argument(\"--dev\", default=\"cpu\",\n",
    "                        help=\"Device to use, default=cpu\")\n",
    "    parser.add_argument(\"--seen-rooms\", action='store_true',\n",
    "                        help=\"Track seen rooms\")\n",
    "    parser.add_argument(\"--relative-actions\", action='store_true',\n",
    "                        help=\"Enable relative actions\")\n",
    "    parser.add_argument(\"--objective\", action='store_true',\n",
    "                        help=\"Enable objective encoding\")\n",
    "    parser.add_argument(\"-r\", \"--run\", required=True, help=\"Run name\")\n",
    "    args = parser.parse_args()\n",
    "    device = torch.device(args.dev)\n",
    "    params = common.PARAMS[args.params]\n",
    "\n",
    "    FLAGS_SEEN_ROOMS |= bool(args.seen_rooms)\n",
    "    FLAGS_RELATIVE_WRAPPER |= bool(args.relative_actions)\n",
    "    FLAGS_OBJECTIVE |= bool(args.objective)\n",
    "\n",
    "    if FLAGS_OBJECTIVE:\n",
    "        EXTRA_GAME_INFO[\"objective\"] = True\n",
    "\n",
    "    game_files = [\n",
    "        \"games/%s%s.ulx\" % (args.game, s)\n",
    "        for s in range(1, args.suffices+1)\n",
    "    ]\n",
    "    val_game_file = \"games/%s%s.ulx\" % (args.game, args.validation)\n",
    "    if not all(map(lambda p: pathlib.Path(p).exists(), game_files)):\n",
    "        raise RuntimeError(f\"Some game files from {game_files} \"\n",
    "                           f\"not found! Please run make_games.sh\")\n",
    "    vocab, action_space, observation_space = \\\n",
    "        common.get_games_spaces(game_files + [val_game_file])\n",
    "    vocab_rev = common.build_rev_vocab(vocab)\n",
    "    env_id = register_games(\n",
    "        gamefiles=game_files,\n",
    "        request_infos=EnvInfos(**EXTRA_GAME_INFO),\n",
    "        name=args.game\n",
    "    )\n",
    "    print(f\"Registered env {env_id} for game files {game_files}\")\n",
    "    val_env_id = register_games(\n",
    "        gamefiles=[val_game_file],\n",
    "        request_infos=EnvInfos(**EXTRA_GAME_INFO),\n",
    "        name=args.game\n",
    "    )\n",
    "    print(f\"Game {val_env_id}, with file {val_game_file} \"\n",
    "          f\"will be used for validation\")\n",
    "\n",
    "    env = gym.make(env_id)\n",
    "    copy_extra_fields = ()\n",
    "    encode_extra_fields = (\"description\", \"inventory\")\n",
    "    if FLAGS_OBJECTIVE:\n",
    "        encode_extra_fields = encode_extra_fields + ('objective',)\n",
    "    if FLAGS_RELATIVE_WRAPPER:\n",
    "        env = preproc.RelativeDirectionWrapper(env)\n",
    "        preproc.RelativeDirectionWrapper.update_vocabs(vocab, vocab_rev)\n",
    "        copy_extra_fields = preproc.RelativeDirectionWrapper.HEADING_FIELDS\n",
    "    env_tw = preproc.TextWorldPreproc(\n",
    "        env, vocab_rev, copy_extra_fields=copy_extra_fields,\n",
    "        encode_extra_fields=encode_extra_fields,\n",
    "    )\n",
    "    if FLAGS_SEEN_ROOMS:\n",
    "        env = preproc.LocationWrapper(env_tw)\n",
    "    else:\n",
    "        env = env_tw\n",
    "    r = env.reset()\n",
    "\n",
    "    val_env = gym.make(val_env_id)\n",
    "    if FLAGS_RELATIVE_WRAPPER:\n",
    "        val_env = preproc.RelativeDirectionWrapper(val_env)\n",
    "    val_env = preproc.TextWorldPreproc(\n",
    "        val_env, vocab_rev, copy_extra_fields=copy_extra_fields,\n",
    "        encode_extra_fields=encode_extra_fields\n",
    "    )\n",
    "    if FLAGS_SEEN_ROOMS:\n",
    "        val_env = preproc.LocationWrapper(val_env)\n",
    "\n",
    "    prep = preproc.Preprocessor(\n",
    "        dict_size=len(vocab),\n",
    "        emb_size=params.embeddings, num_sequences=env_tw.num_fields,\n",
    "        enc_output_size=params.encoder_size,\n",
    "        extra_flags=(preproc.LocationWrapper.SEEN_LOCATION_FIELD, ) + copy_extra_fields\n",
    "    ).to(device)\n",
    "    tgt_prep = ptan.agent.TargetNet(prep)\n",
    "\n",
    "    net = model.DQNModel(\n",
    "        obs_size=prep.obs_enc_size, cmd_size=prep.cmd_enc_size\n",
    "    )\n",
    "    net = net.to(device)\n",
    "    tgt_net = ptan.agent.TargetNet(net)\n",
    "\n",
    "    agent = model.DQNAgent(net, prep, epsilon=1, device=device)\n",
    "    exp_source = ptan.experience.ExperienceSourceFirstLast(\n",
    "        env, agent, gamma=GAMMA, steps_count=1)\n",
    "    buffer = ptan.experience.ExperienceReplayBuffer(\n",
    "        exp_source, params.replay_size)\n",
    "\n",
    "    optimizer = optim.RMSprop(itertools.chain(net.parameters(), prep.parameters()),\n",
    "                              lr=LEARNING_RATE, eps=1e-5)\n",
    "\n",
    "    def process_batch(engine, batch):\n",
    "        optimizer.zero_grad()\n",
    "        loss_t = model.calc_loss_dqn(batch, prep, tgt_prep.target_model,\n",
    "                                     net, tgt_net.target_model, GAMMA, device=device)\n",
    "        loss_t.backward()\n",
    "        optimizer.step()\n",
    "        eps = 1 - engine.state.iteration / params.epsilon_steps\n",
    "        agent.epsilon = max(params.epsilon_final, eps)\n",
    "        if engine.state.iteration % params.sync_nets == 0:\n",
    "            tgt_net.sync()\n",
    "            tgt_prep.sync()\n",
    "        return {\n",
    "            \"loss\": loss_t.item(),\n",
    "            \"epsilon\": agent.epsilon,\n",
    "        }\n",
    "\n",
    "    engine = Engine(process_batch)\n",
    "    run_name = f\"preproc-{args.params}_{args.run}\"\n",
    "    save_path = pathlib.Path(\"saves\") / run_name\n",
    "    save_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    common.setup_ignite(engine, exp_source, run_name,\n",
    "                        extra_metrics=('val_reward', 'val_steps'))\n",
    "\n",
    "    @engine.on(ptan.ignite.PeriodEvents.ITERS_100_COMPLETED)\n",
    "    def validate(engine):\n",
    "        reward = 0.0\n",
    "        steps = 0\n",
    "\n",
    "        obs, extra = val_env.reset()\n",
    "\n",
    "        while True:\n",
    "            obs_t = prep.encode_observations([obs]).to(device)\n",
    "            cmd_t = prep.encode_commands(obs['admissible_commands']).to(device)\n",
    "            q_vals = net.q_values(obs_t, cmd_t)\n",
    "            act = np.argmax(q_vals)\n",
    "\n",
    "            obs, r, is_done, _, _ = val_env.step(act)\n",
    "            steps += 1\n",
    "            reward += r\n",
    "            if is_done:\n",
    "                break\n",
    "        engine.state.metrics['val_reward'] = reward\n",
    "        engine.state.metrics['val_steps'] = steps\n",
    "        print(\"Validation got %.3f reward in %d steps\" % (reward, steps))\n",
    "        best_val_reward = getattr(engine.state, \"best_val_reward\", None)\n",
    "        if best_val_reward is None:\n",
    "            engine.state.best_val_reward = reward\n",
    "        elif best_val_reward < reward:\n",
    "            print(\"Best validation reward updated: %s -> %s\" % (best_val_reward, reward))\n",
    "            save_prep_name = save_path / (\"best_val_%.3f_p.dat\" % reward)\n",
    "            save_net_name = save_path / (\"best_val_%.3f_n.dat\" % reward)\n",
    "            torch.save(prep.state_dict(), save_prep_name)\n",
    "            torch.save(net.state_dict(), save_net_name)\n",
    "            engine.state.best_val_reward = reward\n",
    "\n",
    "    @engine.on(ptan.ignite.EpisodeEvents.BEST_REWARD_REACHED)\n",
    "    def best_reward_updated(trainer: Engine):\n",
    "        reward = trainer.state.metrics['avg_reward']\n",
    "        if reward > 0:\n",
    "            save_prep_name = save_path / (\"best_train_%.3f_p.dat\" % reward)\n",
    "            save_net_name = save_path / (\"best_train_%.3f_n.dat\" % reward)\n",
    "            torch.save(prep.state_dict(), save_prep_name)\n",
    "            torch.save(net.state_dict(), save_net_name)\n",
    "            print(\"%d: best avg training reward: %.3f, saved\" % (\n",
    "                trainer.state.iteration, reward))\n",
    "\n",
    "    engine.run(common.batch_generator(buffer, params.replay_initial, BATCH_SIZE))"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "executable": "/usr/bin/env python3",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
