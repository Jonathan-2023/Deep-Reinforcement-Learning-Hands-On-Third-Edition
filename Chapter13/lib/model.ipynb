{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "662aea78",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import ptan\n",
    "import random\n",
    "import numpy as np\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cd9fd54",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.distributions as t_distr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83ad3de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from . import preproc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed6de4db",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNModel(nn.Module):\n",
    "    def __init__(self, obs_size: int, cmd_size: int, hid_size: int = 256):\n",
    "        super(DQNModel, self).__init__()\n",
    "\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(obs_size + cmd_size, hid_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hid_size, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, obs, cmd):\n",
    "        x = torch.cat((obs, cmd), dim=1)\n",
    "        return self.net(x)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def q_values(self, obs_t, commands_t):\n",
    "        \"\"\"\n",
    "        Calculate q-values for observation and tensor of commands\n",
    "        :param obs_t: preprocessed observation, need to be of [1, obs_size] shape\n",
    "        :param commands_t: commands to be evaluated, shape is [N, cmd_size]\n",
    "        :return: list of q-values for commands\n",
    "        \"\"\"\n",
    "        result = []\n",
    "        for cmd_t in commands_t:\n",
    "            qval = self(obs_t, cmd_t.unsqueeze(0))[0].cpu().item()\n",
    "            result.append(qval)\n",
    "        return result\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def q_values_cmd(self, obs_t, commands_t):\n",
    "        x = torch.cat(torch.broadcast_tensors(obs_t.unsqueeze(0), commands_t), dim=1)\n",
    "        q_vals = self.net(x)\n",
    "        return q_vals.cpu().numpy()[:, 0].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2aff76d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent(ptan.agent.BaseAgent):\n",
    "    def __init__(self, net: DQNModel,\n",
    "                 preprocessor: preproc.Preprocessor,\n",
    "                 epsilon: float = 0.0, device=\"cpu\"):\n",
    "        self.net = net\n",
    "        self._prepr = preprocessor\n",
    "        self._epsilon = epsilon\n",
    "        self.device = device\n",
    "\n",
    "    @property\n",
    "    def epsilon(self):\n",
    "        return self._epsilon\n",
    "\n",
    "    @epsilon.setter\n",
    "    def epsilon(self, value: float):\n",
    "        if 0.0 <= value <= 1.0:\n",
    "            self._epsilon = value\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def __call__(self, states, agent_states=None):\n",
    "        if agent_states is None:\n",
    "            agent_states = [None] * len(states)\n",
    "\n",
    "        # for every state in the batch, calculate\n",
    "        actions = []\n",
    "        for state in states:\n",
    "            commands = state['admissible_commands']\n",
    "            if random.random() <= self.epsilon:\n",
    "                actions.append(random.randrange(len(commands)))\n",
    "            else:\n",
    "                obs_t = self._prepr.encode_observations(\n",
    "                    [state]).to(self.device)\n",
    "                commands_t = self._prepr.encode_commands(commands)\n",
    "                commands_t = commands_t.to(self.device)\n",
    "                q_vals = self.net.q_values(obs_t, commands_t)\n",
    "                actions.append(np.argmax(q_vals))\n",
    "        return actions, agent_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ada5e5a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def unpack_batch(batch: List[ptan.experience.ExperienceFirstLast],\n",
    "                 preprocessor: preproc.Preprocessor,\n",
    "                 net: DQNModel, device=\"cpu\"):\n",
    "    \"\"\"\n",
    "    Convert batch to data needed for Bellman step\n",
    "    :param batch: list of ptan.Experience objects\n",
    "    :param preprocessor: emb.Preprocessor instance\n",
    "    :param net: network to be used for next state approximation\n",
    "    :param device: torch device\n",
    "    :return: tuple (list of states, list of taken commands,\n",
    "                    list of rewards, list of best Qs for the next s)\n",
    "    \"\"\"\n",
    "    # calculate Qs for next states\n",
    "    states, taken_commands, rewards, best_q = [], [], [], []\n",
    "    last_states, last_commands, last_offsets = [], [], []\n",
    "    for exp in batch:\n",
    "        states.append(exp.state)\n",
    "        taken_commands.append(exp.state['admissible_commands'][exp.action])\n",
    "        rewards.append(exp.reward)\n",
    "\n",
    "        # calculate best Q value for the next state\n",
    "        if exp.last_state is None:\n",
    "            # final state in the episode, Q=0\n",
    "            last_offsets.append(len(last_commands))\n",
    "        else:\n",
    "            assert isinstance(exp.last_state, dict)\n",
    "            last_states.append(exp.last_state)\n",
    "            last_commands.extend(exp.last_state['admissible_commands'])\n",
    "            last_offsets.append(len(last_commands))\n",
    "\n",
    "    obs_t = preprocessor.encode_observations(last_states).to(device)\n",
    "    commands_t = preprocessor.encode_commands(last_commands).to(device)\n",
    "\n",
    "    prev_ofs = 0\n",
    "    obs_ofs = 0\n",
    "    for ofs in last_offsets:\n",
    "        if prev_ofs == ofs:\n",
    "            best_q.append(0.0)\n",
    "        else:\n",
    "            q_vals = net.q_values(obs_t[obs_ofs:obs_ofs+1], commands_t[prev_ofs:ofs])\n",
    "            best_q.append(max(q_vals))\n",
    "            obs_ofs += 1\n",
    "        prev_ofs = ofs\n",
    "    return states, taken_commands, rewards, best_q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83006313",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_loss_dqn(batch, preprocessor, tgt_preprocessor, net,\n",
    "                  tgt_net, gamma, device=\"cpu\"):\n",
    "    states, taken_commands, rewards, next_best_qs = \\\n",
    "        unpack_batch(batch, tgt_preprocessor, tgt_net, device)\n",
    "\n",
    "    obs_t = preprocessor.encode_observations(states).to(device)\n",
    "    cmds_t = preprocessor.encode_commands(taken_commands).to(device)\n",
    "    q_values_t = net(obs_t, cmds_t)\n",
    "    tgt_q_t = torch.tensor(rewards) + gamma * torch.tensor(next_best_qs)\n",
    "    tgt_q_t = tgt_q_t.to(device)\n",
    "    return F.mse_loss(q_values_t.squeeze(-1), tgt_q_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaea0865",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CommandModel(nn.Module):\n",
    "    def __init__(self, obs_size: int, dict_size: int,\n",
    "                 embeddings: nn.Embedding, max_tokens: int,\n",
    "                 max_commands: int, start_token: int,\n",
    "                 sep_token: int):\n",
    "        super(CommandModel, self).__init__()\n",
    "\n",
    "        self.emb = embeddings\n",
    "        self.max_tokens = max_tokens\n",
    "        self.max_commands = max_commands\n",
    "        self.start_token = start_token\n",
    "        self.sep_token = sep_token\n",
    "\n",
    "        self.rnn = nn.LSTM(\n",
    "            input_size=embeddings.embedding_dim,\n",
    "            hidden_size=obs_size, batch_first=True)\n",
    "        self.out = nn.Linear(in_features=obs_size,\n",
    "                             out_features=dict_size)\n",
    "\n",
    "    def forward(self, input_seq, obs_t):\n",
    "        hid_t = obs_t.unsqueeze(0)\n",
    "        output, _ = self.rnn(input_seq, (hid_t, hid_t))\n",
    "        return self.out(output)\n",
    "\n",
    "    def commands(self, obs_batch):\n",
    "        \"\"\"\n",
    "        Generate commands from batch of encoded observations\n",
    "        :param obs_batch: tensor of (batch, obs_size)\n",
    "        :return: batch of commands lists\n",
    "        \"\"\"\n",
    "        batch_size = obs_batch.size(0)\n",
    "        # list of finalized commands and logits for every observation in batch\n",
    "        commands = [[] for _ in range(batch_size)]\n",
    "\n",
    "        # currently being constructed list\n",
    "        cur_commands = [[] for _ in range(batch_size)]\n",
    "\n",
    "        # preprare input tensor with start token embeddings\n",
    "        inp_t = torch.full((batch_size, ), self.start_token,\n",
    "                           dtype=torch.long)\n",
    "        inp_t = inp_t.to(obs_batch.device)\n",
    "        inp_t = self.emb(inp_t)\n",
    "        # adding time dimension (dim=1, as batch_first=True)\n",
    "        inp_t = inp_t.unsqueeze(1)\n",
    "        # hidden state is inserted on first dim\n",
    "        p_hid_t = obs_batch.unsqueeze(0)\n",
    "        hid = (p_hid_t, p_hid_t)\n",
    "\n",
    "        while True:\n",
    "            out, hid = self.rnn(inp_t, hid)\n",
    "            out = out.squeeze(1)\n",
    "            # output logits for batch at current time step\n",
    "            out_t = self.out(out)\n",
    "\n",
    "            cat = t_distr.Categorical(logits=out_t)\n",
    "            tokens = cat.sample()\n",
    "\n",
    "            for idx, token in enumerate(tokens):\n",
    "                token = token.item()\n",
    "                cur_commands[idx].append(token)\n",
    "                if token == self.sep_token or \\\n",
    "                        len(cur_commands[idx]) >= self.max_tokens:\n",
    "                    if cur_commands[idx]:\n",
    "                        l = len(commands[idx])\n",
    "                        if l < self.max_commands:\n",
    "                            commands[idx].append(cur_commands[idx])\n",
    "                        cur_commands[idx] = []\n",
    "                    if token != self.sep_token:\n",
    "                        tokens[idx] = self.sep_token\n",
    "            if min(map(len, commands)) == self.max_commands:\n",
    "                break\n",
    "            # convert tokens into input tensor\n",
    "            inp_t = self.emb(tokens)\n",
    "            inp_t = inp_t.unsqueeze(1)\n",
    "        return commands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47c862c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CmdAgent(ptan.agent.BaseAgent):\n",
    "    def __init__(self, env, cmd: CommandModel,\n",
    "                 preprocessor: preproc.Preprocessor,\n",
    "                 device = \"cpu\"):\n",
    "        self.env = env\n",
    "        self.cmd = cmd\n",
    "        self.prepr = preprocessor\n",
    "        self.device = device\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def __call__(self, states, agent_states=None):\n",
    "        if agent_states is None:\n",
    "            agent_states = [None] * len(states)\n",
    "\n",
    "        actions = []\n",
    "        for state in states:\n",
    "            obs_t = self.prepr.encode_observations(\n",
    "                [state]).to(self.device)\n",
    "            commands = self.cmd.commands(obs_t)[0]\n",
    "            cmd = random.choice(commands)\n",
    "            tokens = [\n",
    "                self.env.action_space.id2w[t]\n",
    "                for t in cmd\n",
    "                if t not in {self.cmd.sep_token,\n",
    "                             self.cmd.start_token}\n",
    "            ]\n",
    "            action = \" \".join(tokens)\n",
    "            actions.append(action)\n",
    "        return actions, agent_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "113890e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretrain_loss(cmd: CommandModel, commands: List,\n",
    "                  observations_t: torch.Tensor):\n",
    "    commands_batch = []\n",
    "    target_batch = []\n",
    "    min_length = None\n",
    "\n",
    "    for cmds in commands:\n",
    "        inp = [cmd.start_token]\n",
    "        for c in cmds:\n",
    "            inp.extend(c[1:])\n",
    "        commands_batch.append(inp[:-1])\n",
    "        target_batch.append(inp[1:])\n",
    "        if min_length is None or len(inp) < min_length:\n",
    "            min_length = len(inp)\n",
    "\n",
    "    commands_batch = [c[:min_length-1]\n",
    "                      for c in commands_batch]\n",
    "    target_batch = [c[:min_length-1]\n",
    "                    for c in target_batch]\n",
    "\n",
    "    commands_t = torch.tensor(commands_batch, dtype=torch.long)\n",
    "    commands_t = commands_t.to(observations_t.device)\n",
    "    target_t = torch.tensor(target_batch, dtype=torch.long)\n",
    "    target_t = target_t.to(observations_t.device)\n",
    "    input_t = cmd.emb(commands_t)\n",
    "    logits_t = cmd(input_t, observations_t)\n",
    "    logits_t = logits_t.view(-1, logits_t.size()[-1])\n",
    "    target_t = target_t.view(-1)\n",
    "    return F.cross_entropy(logits_t, target_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c81cda3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CmdDQNAgent(ptan.agent.BaseAgent):\n",
    "    def __init__(self, env, net: DQNModel, cmd: CommandModel,\n",
    "                 cmd_encoder: preproc.Encoder,\n",
    "                 preprocessor: preproc.Preprocessor,\n",
    "                 epsilon: float,\n",
    "                 device = \"cpu\"):\n",
    "        self.env = env\n",
    "        self.net = net\n",
    "        self.cmd = cmd\n",
    "        self.cmd_encoder = cmd_encoder\n",
    "        self.prepr = preprocessor\n",
    "        self.epsilon = epsilon\n",
    "        self.device = device\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def __call__(self, states, agent_states=None):\n",
    "        if agent_states is None:\n",
    "            agent_states = [None] * len(states)\n",
    "\n",
    "        actions = []\n",
    "        for state in states:\n",
    "            obs_t = self.prepr.encode_observations([state]).to(self.device)\n",
    "            commands = self.cmd.commands(obs_t)[0]\n",
    "            if random.random() <= self.epsilon:\n",
    "                act_index = random.randrange(len(commands))\n",
    "            else:\n",
    "                cmd_enc_t = self.prepr._apply_encoder(commands, self.cmd_encoder)\n",
    "                q_vals = self.net.q_values_cmd(obs_t[0], cmd_enc_t)\n",
    "                act_index = np.argmax(q_vals)\n",
    "\n",
    "            cmd = commands[act_index]\n",
    "            tokens = [\n",
    "                self.env.action_space.id2w[t]\n",
    "                for t in cmd\n",
    "                if t not in {self.cmd.sep_token, self.cmd.start_token}\n",
    "            ]\n",
    "            action = \" \".join(tokens)\n",
    "            actions.append(action)\n",
    "        return actions, agent_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fafcd7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def unpack_batch_dqncmd(batch, prep: preproc.Preprocessor,\n",
    "                        cmd: CommandModel,\n",
    "                        cmd_encoder: preproc.Encoder,\n",
    "                        net: DQNModel, env: gym.Env):\n",
    "    observations, taken_actions, rewards = [], [], []\n",
    "    not_done_indices, next_observations = [], []\n",
    "\n",
    "    for idx, exp in enumerate(batch):\n",
    "        observations.append(exp.state)\n",
    "        taken_actions.append(env.action_space.tokenize(exp.action))\n",
    "        rewards.append(exp.reward)\n",
    "        if exp.last_state is not None:\n",
    "            not_done_indices.append(idx)\n",
    "            next_observations.append(exp.last_state)\n",
    "\n",
    "    observations_t = prep.encode_observations(observations)\n",
    "    next_q_vals = [0.0] * len(batch)\n",
    "    if next_observations:\n",
    "        next_observations_t = \\\n",
    "            prep.encode_observations(next_observations)\n",
    "        next_commands = cmd.commands(next_observations_t)\n",
    "\n",
    "        for idx, next_obs_t, next_cmds in \\\n",
    "                zip(not_done_indices,\n",
    "                    next_observations_t,\n",
    "                    next_commands):\n",
    "            next_embs_t = prep._apply_encoder(\n",
    "                next_cmds, cmd_encoder)\n",
    "            q_vals = net.q_values_cmd(next_obs_t, next_embs_t)\n",
    "            next_q_vals[idx] = max(q_vals)\n",
    "\n",
    "    return observations_t, taken_actions, rewards, next_q_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ff99c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_loss_dqncmd(batch, preprocessor, cmd,\n",
    "                     cmd_encoder, tgt_cmd_encoder,\n",
    "                     net, tgt_net, gamma, env, device=\"cpu\"):\n",
    "    obs_t, commands, rewards, next_best_qs = unpack_batch_dqncmd(\n",
    "        batch, preprocessor, cmd, tgt_cmd_encoder, tgt_net, env)\n",
    "    cmds_t = preprocessor._apply_encoder(commands, cmd_encoder)\n",
    "    q_values_t = net(obs_t, cmds_t)\n",
    "    tgt_q_t = torch.tensor(rewards) + \\\n",
    "              gamma * torch.tensor(next_best_qs)\n",
    "    tgt_q_t = tgt_q_t.to(device)\n",
    "    return F.mse_loss(q_values_t.squeeze(-1), tgt_q_t)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
