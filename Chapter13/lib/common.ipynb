{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecabeaf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import textworld\n",
    "import re\n",
    "from textworld import text_utils\n",
    "import gymnasium as gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d471261",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import typing as tt\n",
    "from types import SimpleNamespace\n",
    "from datetime import timedelta, datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4349ea3",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "import ptan\n",
    "import ptan.ignite as ptan_ignite\n",
    "from ignite.engine import Engine\n",
    "from ignite.metrics import RunningAverage\n",
    "from ignite.contrib.handlers import tensorboard_logger as tb_logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4f61156",
   "metadata": {},
   "outputs": [],
   "source": [
    "PARAMS = {\n",
    "    'small': SimpleNamespace(**{\n",
    "        'encoder_size': 20,\n",
    "        'embeddings': 20,\n",
    "        'replay_size': 10000,\n",
    "        'replay_initial': 1000,\n",
    "        'sync_nets': 100,\n",
    "        'epsilon_steps': 1000,\n",
    "        'epsilon_final': 0.2,\n",
    "    }),\n",
    "\n",
    "    'medium': SimpleNamespace(**{\n",
    "        'encoder_size': 256,\n",
    "        'embeddings': 128,\n",
    "        'replay_size': 100000,\n",
    "        'replay_initial': 10000,\n",
    "        'sync_nets': 200,\n",
    "        'epsilon_steps': 10000,\n",
    "        'epsilon_final': 0.2,\n",
    "    })\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3517da9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_generator(buffer: ptan.experience.ExperienceReplayBuffer,\n",
    "                    initial: int, batch_size: int):\n",
    "    buffer.populate(initial)\n",
    "    while True:\n",
    "        buffer.populate(1)\n",
    "        yield buffer.sample(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5af1af06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_ignite(engine: Engine, exp_source, run_name: str,\n",
    "                 extra_metrics: tt.Iterable[str] = ()):\n",
    "    # get rid of missing metrics warning\n",
    "    warnings.simplefilter(\"ignore\", category=UserWarning)\n",
    "\n",
    "    handler = ptan_ignite.EndOfEpisodeHandler(exp_source)\n",
    "    handler.attach(engine)\n",
    "    ptan_ignite.EpisodeFPSHandler().attach(engine)\n",
    "\n",
    "    @engine.on(ptan_ignite.EpisodeEvents.EPISODE_COMPLETED)\n",
    "    def episode_completed(trainer: Engine):\n",
    "        passed = trainer.state.metrics.get('time_passed', 0)\n",
    "        avg_steps = trainer.state.metrics.get('avg_steps', 50)\n",
    "        avg_reward = trainer.state.metrics.get('avg_reward', 0.0)\n",
    "        print(\"Episode %d: reward=%.0f (avg %.2f), \"\n",
    "              \"steps=%s (avg %.2f), speed=%.1f f/s, \"\n",
    "              \"elapsed=%s\" % (\n",
    "            trainer.state.episode,\n",
    "            trainer.state.episode_reward, avg_reward,\n",
    "            trainer.state.episode_steps, avg_steps,\n",
    "            trainer.state.metrics.get('avg_fps', 0),\n",
    "            timedelta(seconds=int(passed))))\n",
    "\n",
    "        if avg_steps < 15 and trainer.state.episode > 100:\n",
    "            print(\"Average steps has fallen below 15, stop training\")\n",
    "            trainer.should_terminate = True\n",
    "\n",
    "    now = datetime.now().isoformat(timespec='minutes')\n",
    "    logdir = f\"runs/{now}-{run_name}\"\n",
    "    tb = tb_logger.TensorboardLogger(log_dir=logdir)\n",
    "    run_avg = RunningAverage(output_transform=lambda v: v['loss'])\n",
    "    run_avg.attach(engine, \"avg_loss\")\n",
    "\n",
    "    metrics = ['reward', 'steps', 'avg_reward', 'avg_steps']\n",
    "    handler = tb_logger.OutputHandler(\n",
    "        tag=\"episodes\", metric_names=metrics)\n",
    "    event = ptan_ignite.EpisodeEvents.EPISODE_COMPLETED\n",
    "    tb.attach(engine, log_handler=handler, event_name=event)\n",
    "\n",
    "    # write to tensorboard every 100 iterations\n",
    "    ptan_ignite.PeriodicEvents().attach(engine)\n",
    "    metrics = ['avg_loss', 'avg_fps']\n",
    "    metrics.extend(extra_metrics)\n",
    "    handler = tb_logger.OutputHandler(\n",
    "        tag=\"train\", metric_names=metrics,\n",
    "        output_transform=lambda a: a)\n",
    "    event = ptan_ignite.PeriodEvents.ITERS_100_COMPLETED\n",
    "    tb.attach(engine, log_handler=handler, event_name=event)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2718718d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_games_spaces(game_files: tt.List[str]) -> tt.Tuple[\n",
    "    tt.Dict[int, str],\n",
    "    gym.Space,\n",
    "    gym.Space,\n",
    "]:\n",
    "    \"\"\"\n",
    "    Get games vocabulary, action and observation spaces\n",
    "    :param game_files: game files to wrap\n",
    "    :return: tuple with dictionary, action and observation spaces\n",
    "    \"\"\"\n",
    "    vocab = text_utils.extract_vocab_from_gamefiles(game_files)\n",
    "    vocab_dict = {\n",
    "        idx: word\n",
    "        for idx, word in enumerate(sorted(vocab))\n",
    "    }\n",
    "    word_space = gym.spaces.Discrete(len(vocab))\n",
    "    action_space = gym.spaces.Sequence(word_space)\n",
    "    observation_space = gym.spaces.Sequence(word_space)\n",
    "    return vocab_dict, action_space, observation_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6585c90c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_rev_vocab(vocab: tt.Dict[int, str]) -> tt.Dict[str, int]:\n",
    "    \"\"\"\n",
    "    Build reverse vocabulary\n",
    "    :param vocab: forward vocab (int -> word)\n",
    "    :return: reverse vocabulary (word -> int)\n",
    "    \"\"\"\n",
    "    res = {word: idx for idx, word in vocab.items()}\n",
    "    assert len(res) == len(vocab)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7221421c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text: str, rev_vocab: tt.Dict[str, int]) -> tt.List[int]:\n",
    "    \"\"\"\n",
    "    Very simple tokeniser into fixed word set\n",
    "    :param text: text to tokenize\n",
    "    :param rev_vocab: reverse vocabulary\n",
    "    :return: list of tokens\n",
    "    \"\"\"\n",
    "    res = []\n",
    "    for word in re.split(r'\\W+', text.lower()):\n",
    "        token = rev_vocab.get(word)\n",
    "        if token is not None:\n",
    "            res.append(token)\n",
    "    return res"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
