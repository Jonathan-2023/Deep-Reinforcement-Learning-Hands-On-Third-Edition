{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ace6070d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import logging\n",
    "import typing as tt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fbbdc54",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "import textworld.gym.envs\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.utils.rnn as rnn_utils\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from . import common"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03a94490",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "KEY_ADM_COMMANDS = \"admissible_commands\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cbe0dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextWorldPreproc(gym.Wrapper):\n",
    "    \"\"\"\n",
    "    Simple wrapper to preprocess text_world game observation\n",
    "\n",
    "    Observation and action spaces are not handled, as it will\n",
    "    be wrapped into other preprocessors\n",
    "    \"\"\"\n",
    "    log = logging.getLogger(\"TextWorldPreproc\")\n",
    "\n",
    "    # field with observation\n",
    "    OBS_FIELD = \"obs\"\n",
    "\n",
    "    def __init__(\n",
    "            self, env: gym.Env, vocab_rev: tt.Optional[tt.Dict[str, int]],\n",
    "            encode_raw_text: bool = False,\n",
    "            encode_extra_fields: tt.Iterable[str] = ('description', 'inventory'),\n",
    "            copy_extra_fields: tt.Iterable[str] = (),\n",
    "            use_admissible_commands: bool = True, keep_admissible_commands: bool = False,\n",
    "            use_intermediate_reward: bool = True, tokens_limit: tt.Optional[int] = None,\n",
    "            reward_wrong_last_command: tt.Optional[float] = None\n",
    "    ):\n",
    "        \"\"\"\n",
    "        :param env: TextWorld env to be wrapped\n",
    "        :param vocab_ver: reverse vocabulary\n",
    "        :param encode_raw_text: flag to encode raw texts\n",
    "        :param encode_extra_fields: fields to be encoded\n",
    "        :param copy_extra_fields: fields to be copied into obs\n",
    "        :param use_admissible_commands: use list of commands\n",
    "        :param keep_admissible_commands: keep list of admissible commands in observations\n",
    "        :param use_intermediate_reward: intermediate reward\n",
    "        :param tokens_limit: limit tokens in encoded fields\n",
    "        :param reward_wrong_last_command: if given, this reward will be given if 'last_command' observation field is 'None'.\n",
    "        \"\"\"\n",
    "        super(TextWorldPreproc, self).__init__(env)\n",
    "        self._vocab_rev = vocab_rev\n",
    "        self._encode_raw_text = encode_raw_text\n",
    "        self._encode_extra_field = tuple(encode_extra_fields)\n",
    "        self._copy_extra_fields = tuple(copy_extra_fields)\n",
    "        self._use_admissible_commands = use_admissible_commands\n",
    "        self._keep_admissible_commands = keep_admissible_commands\n",
    "        self._use_intermedate_reward = use_intermediate_reward\n",
    "        self._num_fields = len(self._encode_extra_field) + int(self._encode_raw_text)\n",
    "        self._last_admissible_commands = None\n",
    "        self._last_extra_info = None\n",
    "        self._tokens_limit = tokens_limit\n",
    "        self._reward_wrong_last_command = reward_wrong_last_command\n",
    "        self._cmd_hist = []\n",
    "\n",
    "    @property\n",
    "    def num_fields(self):\n",
    "        return self._num_fields\n",
    "\n",
    "    def _maybe_tokenize(self, s: str) -> str | tt.List[int]:\n",
    "        \"\"\"\n",
    "        If dictionary is present, tokenise the string, otherwise keep intact\n",
    "        :param s: string to process\n",
    "        :return: tokenized string or original value\n",
    "        \"\"\"\n",
    "        if self._vocab_rev is None:\n",
    "            return s\n",
    "        tokens = common.tokenize(s, self._vocab_rev)\n",
    "        if self._tokens_limit is not None:\n",
    "            tokens = tokens[:self._tokens_limit]\n",
    "        return tokens\n",
    "\n",
    "    def _encode(self, obs: str, extra_info: dict) -> dict:\n",
    "        obs_result = []\n",
    "        if self._encode_raw_text:\n",
    "            obs_result.append(self._maybe_tokenize(obs))\n",
    "        for field in self._encode_extra_field:\n",
    "            extra = extra_info[field]\n",
    "            obs_result.append(self._maybe_tokenize(extra))\n",
    "        result = {self.OBS_FIELD: obs_result}\n",
    "        if self._use_admissible_commands:\n",
    "            result[KEY_ADM_COMMANDS] = [\n",
    "                self._maybe_tokenize(cmd) for cmd in extra_info[KEY_ADM_COMMANDS]\n",
    "            ]\n",
    "            self._last_admissible_commands = extra_info[KEY_ADM_COMMANDS]\n",
    "        if self._keep_admissible_commands:\n",
    "            result[KEY_ADM_COMMANDS] = extra_info[KEY_ADM_COMMANDS]\n",
    "            if 'policy_commands' in extra_info:\n",
    "                result['policy_commands'] = extra_info['policy_commands']\n",
    "        self._last_extra_info = extra_info\n",
    "        for field in self._copy_extra_fields:\n",
    "            if field in extra_info:\n",
    "                result[field] = extra_info[field]\n",
    "        return result\n",
    "\n",
    "    def reset(self, seed: tt.Optional[int] = None):\n",
    "        res, extra = self.env.reset()\n",
    "        self._cmd_hist = []\n",
    "        return self._encode(res, extra), extra\n",
    "\n",
    "    def step(self, action):\n",
    "        if self._use_admissible_commands:\n",
    "            action = self._last_admissible_commands[action]\n",
    "            self._cmd_hist.append(action)\n",
    "        obs, r, is_done, extra = self.env.step(action)\n",
    "        if self._use_intermedate_reward:\n",
    "            r += extra.get('intermediate_reward', 0)\n",
    "        if self._reward_wrong_last_command is not None:\n",
    "            if action not in self._last_extra_info[KEY_ADM_COMMANDS]:\n",
    "                r += self._reward_wrong_last_command\n",
    "        return self._encode(obs, extra), r, is_done, False, extra\n",
    "\n",
    "    @property\n",
    "    def last_admissible_commands(self):\n",
    "        if self._last_admissible_commands:\n",
    "            return tuple(self._last_admissible_commands)\n",
    "        return None\n",
    "\n",
    "    @property\n",
    "    def last_extra_info(self):\n",
    "        return self._last_extra_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2703d779",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LocationWrapper(gym.Wrapper):\n",
    "    \"\"\"\n",
    "    Wrapper which tracks list of locations we've already seen\n",
    "    \"\"\"\n",
    "    SEEN_LOCATION_FIELD = \"location_seen\"\n",
    "\n",
    "    def __init__(self, env: gym.Env):\n",
    "        super(LocationWrapper, self).__init__(env)\n",
    "        self._seen_locations = set()\n",
    "        self._cur_location = None\n",
    "\n",
    "    def reset(self, *, seed: tt.Optional[int] = None):\n",
    "        self._seen_locations.clear()\n",
    "        self._cur_location = None\n",
    "        obs, extra = self.env.reset(seed=seed)\n",
    "        self._track_location(extra)\n",
    "        obs[self.SEEN_LOCATION_FIELD] = int(self.location_was_seen)\n",
    "        return obs, extra\n",
    "\n",
    "    @property\n",
    "    def location_was_seen(self) -> bool:\n",
    "        return self._cur_location in self._seen_locations\n",
    "\n",
    "    def _track_location(self, extra_dict: dict):\n",
    "        if self._cur_location is not None:\n",
    "            self._seen_locations.add(self._cur_location)\n",
    "        descr = extra_dict.get('description')\n",
    "        if descr is None:\n",
    "            self._cur_location = None\n",
    "        else:\n",
    "            self._cur_location = descr.split(\"\\n\")[0]\n",
    "\n",
    "    def step(self, action):\n",
    "        obs, r, is_done, is_tr, extra = self.env.step(action)\n",
    "        self._track_location(extra)\n",
    "        obs[self.SEEN_LOCATION_FIELD] = int(self.location_was_seen)\n",
    "        return obs, r, is_done, is_tr, extra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a74f4088",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RelativeDirectionWrapper(gym.Wrapper):\n",
    "    \"\"\"\n",
    "    Wrapper which tracks our heading direction (NSWE) and adds support of\n",
    "    relative navigation (\"go right\", \"go straight\", etc).\n",
    "    We also add heading direction into observation.\n",
    "    \"\"\"\n",
    "    # Directions are made relative to each other, don't update them blindly\n",
    "    ABSOLUTE_DIRS = ('west', 'north', 'east', 'south')\n",
    "    RELATIVE_DIRS = ('right', 'forward', 'left', 'back')\n",
    "    HEADING_FIELDS = tuple(\"heading_\" + d for d in ABSOLUTE_DIRS)\n",
    "    NEW_ACTIONS = set(\"go \" + d for d in RELATIVE_DIRS)\n",
    "    OLD_ACTIONS = set(\"go \" + d for d in ABSOLUTE_DIRS)\n",
    "\n",
    "    OLD_ACTIONS_DIRS = {\n",
    "        \"go \" + d: idx\n",
    "        for idx, d in enumerate(ABSOLUTE_DIRS)\n",
    "    }\n",
    "\n",
    "    NEW_ACTIONS_DELTAS = {\n",
    "        \"go \" + d: delta\n",
    "        for d, delta in zip(RELATIVE_DIRS, (1, 0, 3, 2))\n",
    "    }\n",
    "\n",
    "    def __init__(self, env: textworld.gym.envs.TextworldGymEnv):\n",
    "        if not isinstance(env, textworld.gym.envs.TextworldGymEnv):\n",
    "            raise ValueError(f\"{self.class_name()} has to be applied to TextworldGymEnv\")\n",
    "        super(RelativeDirectionWrapper, self).__init__(env)\n",
    "        # look north\n",
    "        self._heading_idx = 1\n",
    "\n",
    "    @classmethod\n",
    "    def abs_to_rel(cls, abs_action: str, cur_dir: int) -> str:\n",
    "        act_dir = cls.OLD_ACTIONS_DIRS[abs_action]\n",
    "        delta = act_dir - cur_dir\n",
    "        if delta == 0:\n",
    "            return \"go forward\"\n",
    "        elif delta == 1 or delta == -3:\n",
    "            return \"go right\"\n",
    "        elif delta == -1 or delta == 3:\n",
    "            return \"go left\"\n",
    "        elif abs(delta) == 2:\n",
    "            return \"go back\"\n",
    "        else:\n",
    "            raise RuntimeError(\"Unhandled value of delta=\" + str(delta))\n",
    "\n",
    "    @classmethod\n",
    "    def rel_to_abs(cls, rel_action: str, cur_dir: int) -> str:\n",
    "        delta = cls.NEW_ACTIONS_DELTAS[rel_action]\n",
    "        new_dir = (cur_dir + delta) % 4\n",
    "        return \"go \" + cls.ABSOLUTE_DIRS[new_dir]\n",
    "\n",
    "    @classmethod\n",
    "    def rel_execute(cls, rel_action: str, cur_dir: int) -> int:\n",
    "        delta = cls.NEW_ACTIONS_DELTAS[rel_action]\n",
    "        new_dir = (cur_dir + delta) % 4\n",
    "        return new_dir\n",
    "\n",
    "    @classmethod\n",
    "    def update_vocabs(cls, vocab: tt.Dict[int, str], vocab_rev: tt.Dict[str, int]):\n",
    "        \"\"\"\n",
    "        Update vocabularies to include new action words\n",
    "        :param vocab: forward vocabulary\n",
    "        :param vocab_rev: reverse vocabulary\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        for word in cls.RELATIVE_DIRS:\n",
    "            if word not in vocab_rev.keys():\n",
    "                next_idx = len(vocab)\n",
    "                vocab[next_idx] = word\n",
    "                vocab_rev[word] = next_idx\n",
    "\n",
    "    def _update_info(self, info: dict) -> dict:\n",
    "        \"\"\"\n",
    "        Update information dict: replace admissible_commands into relative\n",
    "        :param info: dict with extra information\n",
    "        :return: updated dict (same object)\n",
    "        \"\"\"\n",
    "        new_commands = []\n",
    "        for cmd in info.get(KEY_ADM_COMMANDS, []):\n",
    "            if cmd in self.OLD_ACTIONS:\n",
    "                cmd = self.abs_to_rel(cmd, self._heading_idx)\n",
    "            new_commands.append(cmd)\n",
    "        if new_commands:\n",
    "            info[KEY_ADM_COMMANDS] = new_commands\n",
    "        for idx, field in enumerate(self.HEADING_FIELDS):\n",
    "            info[field] = int(idx == self._heading_idx)\n",
    "        return info\n",
    "\n",
    "    def reset(self):\n",
    "        # Look north initially\n",
    "        self._heading_idx = 1\n",
    "        obs, extra = self.env.reset()\n",
    "        return obs, self._update_info(extra)\n",
    "\n",
    "    def step(self, action: str):\n",
    "        if action in self.NEW_ACTIONS:\n",
    "            abs_action = self.rel_to_abs(action, self._heading_idx)\n",
    "            self._heading_idx = self.rel_execute(action, self._heading_idx)\n",
    "            action = abs_action\n",
    "        obs, r, is_done, extra = self.env.step(action)\n",
    "        return obs, r, is_done, self._update_info(extra)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ad8f63",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Takes input sequences (after embeddings) and returns\n",
    "    the hidden state from LSTM\n",
    "    \"\"\"\n",
    "    def __init__(self, emb_size: int, out_size: int):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.net = nn.LSTM(input_size=emb_size, hidden_size=out_size, batch_first=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.net.flatten_parameters()\n",
    "        _, hid_cell = self.net(x)\n",
    "        # Warn: if bidir=True or several layers,\n",
    "        # sequeeze has to be changed!\n",
    "        return hid_cell[0].squeeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0fa7e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preprocessor(nn.Module):\n",
    "    \"\"\"\n",
    "    Takes batch of several input sequences and outputs their\n",
    "    summary from one or many encoders\n",
    "    \"\"\"\n",
    "    def __init__(self, dict_size: int, emb_size: int, num_sequences: int,\n",
    "                 enc_output_size: int, extra_flags: tt.Sequence[str] = ()):\n",
    "        \"\"\"\n",
    "        :param dict_size: amount of words is our vocabulary\n",
    "        :param emb_size: dimensionality of embeddings\n",
    "        :param num_sequences: count of sequences\n",
    "        :param enc_output_size: output from single encoder\n",
    "        :param extra_flags: list of fields from observations\n",
    "        to encode as numbers\n",
    "        \"\"\"\n",
    "        super(Preprocessor, self).__init__()\n",
    "        self._extra_flags = extra_flags\n",
    "        self._enc_output_size = enc_output_size\n",
    "        self.emb = nn.Embedding(num_embeddings=dict_size, embedding_dim=emb_size)\n",
    "        self.encoders = []\n",
    "        for idx in range(num_sequences):\n",
    "            enc = Encoder(emb_size, enc_output_size)\n",
    "            self.encoders.append(enc)\n",
    "            self.add_module(f\"enc_{idx}\", enc)\n",
    "        self.enc_commands = Encoder(emb_size, enc_output_size)\n",
    "\n",
    "    @property\n",
    "    def obs_enc_size(self):\n",
    "        return self._enc_output_size * len(self.encoders) + \\\n",
    "            len(self._extra_flags)\n",
    "\n",
    "    @property\n",
    "    def cmd_enc_size(self):\n",
    "        return self._enc_output_size\n",
    "\n",
    "    def _apply_encoder(self, batch: tt.List[tt.List[int]], encoder: Encoder):\n",
    "        dev = self.emb.weight.device\n",
    "        batch_t = [self.emb(torch.tensor(sample).to(dev)) for sample in batch]\n",
    "        batch_seq = rnn_utils.pack_sequence(batch_t, enforce_sorted=False)\n",
    "        return encoder(batch_seq)\n",
    "\n",
    "    def encode_observations(self, observations: tt.List[dict]) -> torch.Tensor:\n",
    "        sequences = [obs[TextWorldPreproc.OBS_FIELD] for obs in observations ]\n",
    "        res_t = self.encode_sequences(sequences)\n",
    "        if not self._extra_flags:\n",
    "            return res_t\n",
    "        extra = [[obs[field] for field in self._extra_flags] for obs in observations]\n",
    "        extra_t = torch.Tensor(extra).to(res_t.device)\n",
    "        res_t = torch.cat([res_t, extra_t], dim=1)\n",
    "        return res_t\n",
    "\n",
    "    def encode_sequences(self, batches):\n",
    "        \"\"\"\n",
    "        Forward pass of Preprocessor\n",
    "        :param batches: list of tuples with variable-length sequences of word ids\n",
    "        :return: tensor with concatenated encoder outputs for every batch sample\n",
    "        \"\"\"\n",
    "        data = []\n",
    "        for enc, enc_batch in zip(self.encoders, zip(*batches)):\n",
    "            data.append(self._apply_encoder(enc_batch, enc))\n",
    "        res_t = torch.cat(data, dim=1)\n",
    "        return res_t\n",
    "\n",
    "    def encode_commands(self, batch):\n",
    "        \"\"\"\n",
    "        Apply encoder to list of commands sequence\n",
    "        :param batch: list of lists of idx\n",
    "        :return: tensor with encoded commands in original order\n",
    "        \"\"\"\n",
    "        return self._apply_encoder(batch, self.enc_commands)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dbaacbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerPreprocessor:\n",
    "    DEFAULT_MODEL = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "\n",
    "    def __init__(self, num_sequences: int,\n",
    "                 device: torch.device,\n",
    "                 extra_flags: tt.Sequence[str] = (),\n",
    "                 model_name: str = DEFAULT_MODEL):\n",
    "        self._device = device\n",
    "        self._transformer = SentenceTransformer(model_name, device=device.type)\n",
    "        self._emb_size = self._transformer.get_sentence_embedding_dimension()\n",
    "        self._extra_flags = extra_flags\n",
    "        self._num_sequences = num_sequences\n",
    "\n",
    "    @property\n",
    "    def obs_enc_size(self) -> int:\n",
    "        return self._emb_size * self._num_sequences + len(self._extra_flags)\n",
    "\n",
    "    @property\n",
    "    def cmd_enc_size(self) -> int:\n",
    "        return self._emb_size\n",
    "\n",
    "    def encode_observations(self, observations: tt.List[dict]) -> \\\n",
    "            torch.Tensor:\n",
    "        sequences = [\n",
    "            obs[TextWorldPreproc.OBS_FIELD]\n",
    "            for obs in observations\n",
    "        ]\n",
    "        res_t = self.encode_sequences(sequences)\n",
    "        if not self._extra_flags:\n",
    "            return res_t\n",
    "\n",
    "        extra = [\n",
    "            [obs[field] for field in self._extra_flags]\n",
    "            for obs in observations\n",
    "        ]\n",
    "        extra_t = torch.Tensor(extra).to(res_t.device)\n",
    "        res_t = torch.cat([res_t, extra_t], dim=1)\n",
    "        return res_t\n",
    "\n",
    "    def encode_sequences(self, batches: tt.List[tt.Tuple[str, ...]]) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass of Preprocessor\n",
    "        :param batches: list of tuples with strings\n",
    "        :return: tensor with concatenated encoder outputs for every batch sample\n",
    "        \"\"\"\n",
    "        data = []\n",
    "        for b in batches:\n",
    "            data.extend(b)\n",
    "        res_t = self._transformer.encode(\n",
    "            data, convert_to_tensor=True\n",
    "        )\n",
    "        res_t = res_t.reshape((len(batches), len(batches[0]) * self._emb_size))\n",
    "        return res_t\n",
    "\n",
    "    def encode_commands(self, batch: tt.List[str]) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Apply encoder to list of commands sequence\n",
    "        :param batch: list of string\n",
    "        :return: tensor with encoded commands in original order\n",
    "        \"\"\"\n",
    "        return self._transformer.encode(\n",
    "            batch, convert_to_tensor=True)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
