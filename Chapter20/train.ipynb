{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "804c3261",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import ptan\n",
    "import random\n",
    "import argparse\n",
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbf6a263",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib import game, model, mcts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c89c2a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard.writer import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89c79a1d",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e67ec069",
   "metadata": {},
   "outputs": [],
   "source": [
    "PLAY_EPISODES = 1  #25\n",
    "MCTS_SEARCHES = 10\n",
    "MCTS_BATCH_SIZE = 8\n",
    "REPLAY_BUFFER = 5000 # 30000\n",
    "LEARNING_RATE = 0.001\n",
    "BATCH_SIZE = 256\n",
    "TRAIN_ROUNDS = 10\n",
    "MIN_REPLAY_TO_TRAIN = 2000 #10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c056099",
   "metadata": {},
   "outputs": [],
   "source": [
    "BEST_NET_WIN_RATIO = 0.60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1da725ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "EVALUATE_EVERY_STEP = 100\n",
    "EVALUATION_ROUNDS = 20\n",
    "STEPS_BEFORE_TAU_0 = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e63e0b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(net1: model.Net, net2: model.Net,\n",
    "             rounds: int, device: torch.device):\n",
    "    n1_win, n2_win = 0, 0\n",
    "    mcts_stores = [mcts.MCTS(), mcts.MCTS()]\n",
    "\n",
    "    for r_idx in range(rounds):\n",
    "        r, _ = model.play_game(mcts_stores=mcts_stores, replay_buffer=None, net1=net1, net2=net2,\n",
    "                               steps_before_tau_0=0, mcts_searches=20, mcts_batch_size=16,\n",
    "                               device=device)\n",
    "        if r < -0.5:\n",
    "            n2_win += 1\n",
    "        elif r > 0.5:\n",
    "            n1_win += 1\n",
    "    return n1_win / (n1_win + n2_win)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbea1bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"-n\", \"--name\", required=True, help=\"Name of the run\")\n",
    "    parser.add_argument(\"--dev\", default=\"cpu\",\n",
    "                        help=\"Device to use, default=cpu\")\n",
    "    args = parser.parse_args()\n",
    "    device = torch.device(args.dev)\n",
    "\n",
    "    saves_path = os.path.join(\"saves\", args.name)\n",
    "    os.makedirs(saves_path, exist_ok=True)\n",
    "    writer = SummaryWriter(comment=\"-\" + args.name)\n",
    "\n",
    "    net = model.Net(input_shape=model.OBS_SHAPE, actions_n=game.GAME_COLS).to(device)\n",
    "    best_net = ptan.agent.TargetNet(net)\n",
    "    print(net)\n",
    "\n",
    "    optimizer = optim.SGD(net.parameters(), lr=LEARNING_RATE, momentum=0.9)\n",
    "\n",
    "    replay_buffer = collections.deque(maxlen=REPLAY_BUFFER)\n",
    "    mcts_store = mcts.MCTS()\n",
    "    step_idx = 0\n",
    "    best_idx = 0\n",
    "\n",
    "    with ptan.common.utils.TBMeanTracker(writer, batch_size=10) as tb_tracker:\n",
    "        while True:\n",
    "            t = time.time()\n",
    "            prev_nodes = len(mcts_store)\n",
    "            game_steps = 0\n",
    "            for _ in range(PLAY_EPISODES):\n",
    "                _, steps = model.play_game(mcts_store, replay_buffer, best_net.target_model, best_net.target_model,\n",
    "                                           steps_before_tau_0=STEPS_BEFORE_TAU_0, mcts_searches=MCTS_SEARCHES,\n",
    "                                           mcts_batch_size=MCTS_BATCH_SIZE, device=device)\n",
    "                game_steps += steps\n",
    "            game_nodes = len(mcts_store) - prev_nodes\n",
    "            dt = time.time() - t\n",
    "            speed_steps = game_steps / dt\n",
    "            speed_nodes = game_nodes / dt\n",
    "            tb_tracker.track(\"speed_steps\", speed_steps, step_idx)\n",
    "            tb_tracker.track(\"speed_nodes\", speed_nodes, step_idx)\n",
    "            print(\"Step %d, steps %3d, leaves %4d, steps/s %5.2f, leaves/s %6.2f, best_idx %d, replay %d\" % (\n",
    "                step_idx, game_steps, game_nodes, speed_steps, speed_nodes, best_idx, len(replay_buffer)))\n",
    "            step_idx += 1\n",
    "\n",
    "            if len(replay_buffer) < MIN_REPLAY_TO_TRAIN:\n",
    "                continue\n",
    "\n",
    "            # train\n",
    "            sum_loss = 0.0\n",
    "            sum_value_loss = 0.0\n",
    "            sum_policy_loss = 0.0\n",
    "\n",
    "            for _ in range(TRAIN_ROUNDS):\n",
    "                batch = random.sample(replay_buffer, BATCH_SIZE)\n",
    "                batch_states, batch_who_moves, batch_probs, batch_values = zip(*batch)\n",
    "                batch_states_lists = [game.decode_binary(state) for state in batch_states]\n",
    "                states_v = model.state_lists_to_batch(batch_states_lists, batch_who_moves, device)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                probs_v = torch.FloatTensor(batch_probs).to(device)\n",
    "                values_v = torch.FloatTensor(batch_values).to(device)\n",
    "                out_logits_v, out_values_v = net(states_v)\n",
    "\n",
    "                loss_value_v = F.mse_loss(out_values_v.squeeze(-1), values_v)\n",
    "                loss_policy_v = -F.log_softmax(out_logits_v, dim=1) * probs_v\n",
    "                loss_policy_v = loss_policy_v.sum(dim=1).mean()\n",
    "\n",
    "                loss_v = loss_policy_v + loss_value_v\n",
    "                loss_v.backward()\n",
    "                optimizer.step()\n",
    "                sum_loss += loss_v.item()\n",
    "                sum_value_loss += loss_value_v.item()\n",
    "                sum_policy_loss += loss_policy_v.item()\n",
    "\n",
    "            tb_tracker.track(\"loss_total\", sum_loss / TRAIN_ROUNDS, step_idx)\n",
    "            tb_tracker.track(\"loss_value\", sum_value_loss / TRAIN_ROUNDS, step_idx)\n",
    "            tb_tracker.track(\"loss_policy\", sum_policy_loss / TRAIN_ROUNDS, step_idx)\n",
    "\n",
    "            # evaluate net\n",
    "            if step_idx % EVALUATE_EVERY_STEP == 0:\n",
    "                win_ratio = evaluate(net, best_net.target_model, rounds=EVALUATION_ROUNDS, device=device)\n",
    "                print(\"Net evaluated, win ratio = %.2f\" % win_ratio)\n",
    "                writer.add_scalar(\"eval_win_ratio\", win_ratio, step_idx)\n",
    "                if win_ratio > BEST_NET_WIN_RATIO:\n",
    "                    print(\"Net is better than cur best, sync\")\n",
    "                    best_net.sync()\n",
    "                    best_idx += 1\n",
    "                    file_name = os.path.join(saves_path, \"best_%03d_%05d.dat\" % (best_idx, step_idx))\n",
    "                    torch.save(net.state_dict(), file_name)\n",
    "                    mcts_store.clear()"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "executable": "/usr/bin/env python3",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
