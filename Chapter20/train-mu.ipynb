{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39788c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import typing as tt\n",
    "from time import time\n",
    "import argparse\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import itertools\n",
    "import collections\n",
    "from torch.utils.tensorboard.writer import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7611831e",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "from lib import muzero as mu\n",
    "from lib import game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6373d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# count of recent episodes in the replay buffer\n",
    "REPLAY_BUFFER = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dcf5167",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this amount of states will be generated, samples are *5 of this size\n",
    "BATCH_SIZE = 256\n",
    "TRAIN_ROUNDS = 1\n",
    "LEARNING_RATE = 0.001\n",
    "TEMP_START = 10\n",
    "TEMP_STEP = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d5eea85",
   "metadata": {},
   "outputs": [],
   "source": [
    "BEST_NET_WIN_RATIO = 0.7\n",
    "EVALUATE_EVERY_STEP = 10\n",
    "EVALUATION_ROUNDS = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c50a5a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(net1: mu.MuZeroModels, net2: mu.MuZeroModels,\n",
    "             params: mu.MuZeroParams) -> tt.Tuple[float, float]:\n",
    "    n1_win, n2_win = 0, 0\n",
    "    rewards = []\n",
    "    sum_steps = 0\n",
    "\n",
    "    for r_idx in range(EVALUATION_ROUNDS):\n",
    "        r, e = mu.play_game(net1, net2, params, temperature=0)\n",
    "        sum_steps += len(e)\n",
    "        if r < -0.5:\n",
    "            n2_win += 1\n",
    "        elif r > 0.5:\n",
    "            n1_win += 1\n",
    "        rewards.append(r)\n",
    "    print(f\"Eval rewards: {rewards}, steps={sum_steps / EVALUATION_ROUNDS}\")\n",
    "    return n1_win / (n1_win + n2_win), sum_steps / EVALUATION_ROUNDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ccde2e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"-n\", \"--name\", required=True, help=\"Name of the run\")\n",
    "    parser.add_argument(\"--dev\", default=\"cpu\", help=\"Device to use, default=cpu\")\n",
    "    args = parser.parse_args()\n",
    "    print(args)\n",
    "    device = torch.device(args.dev)\n",
    "\n",
    "    saves_path = os.path.join(\"saves\", args.name)\n",
    "    os.makedirs(saves_path, exist_ok=True)\n",
    "    writer = SummaryWriter(comment=\"-\" + args.name)\n",
    "\n",
    "    net = mu.MuZeroModels(mu.OBS_SHAPE, game.GAME_COLS)\n",
    "    net.to(device)\n",
    "    best_net = mu.MuZeroModels(mu.OBS_SHAPE, game.GAME_COLS)\n",
    "    best_net.to(device)\n",
    "    params = mu.MuZeroParams(dev=args.dev)\n",
    "\n",
    "    optimizer = optim.SGD(itertools.chain(\n",
    "        net.repr.parameters(),\n",
    "        net.pred.parameters(),\n",
    "        net.dynamics.parameters(),\n",
    "    ), lr=LEARNING_RATE, momentum=0.9)\n",
    "\n",
    "    replay_buffer = collections.deque(maxlen=REPLAY_BUFFER)\n",
    "    temperature = TEMP_START\n",
    "    step_idx = 0\n",
    "    best_idx = 0\n",
    "\n",
    "    while True:\n",
    "        step_idx += 1\n",
    "        ts = time()\n",
    "        score, episode = mu.play_game(best_net, best_net, params, temperature=temperature)\n",
    "        print(f\"{step_idx}: result {score}, steps={len(episode)}\")\n",
    "        replay_buffer.append(episode)\n",
    "        writer.add_scalar(\"time_play\", time() - ts, step_idx)\n",
    "        writer.add_scalar(\"steps\", len(episode), step_idx)\n",
    "\n",
    "        # training\n",
    "        ts = time()\n",
    "        for _ in range(TRAIN_ROUNDS):\n",
    "            states_t, actions, policy_tgt, rewards_tgt, values_tgt = \\\n",
    "                mu.sample_batch(replay_buffer, BATCH_SIZE, params)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            h_t = net.repr(states_t)\n",
    "            loss_p_full_t = None\n",
    "            loss_v_full_t = None\n",
    "            loss_r_full_t = None\n",
    "            for step in range(params.unroll_steps):\n",
    "                policy_t, values_t = net.pred(h_t)\n",
    "                loss_p_t = F.cross_entropy(policy_t, policy_tgt[step])\n",
    "                loss_v_t = F.mse_loss(values_t, values_tgt[step])\n",
    "                # dynamic step\n",
    "                rewards_t, h_t = net.dynamics(h_t, actions[step])\n",
    "                loss_r_t = F.mse_loss(rewards_t, rewards_tgt[step])\n",
    "                if step == 0:\n",
    "                    loss_p_full_t = loss_p_t\n",
    "                    loss_v_full_t = loss_v_t\n",
    "                    loss_r_full_t = loss_r_t\n",
    "                else:\n",
    "                    loss_p_full_t += loss_p_t * 0.5\n",
    "                    loss_v_full_t += loss_v_t * 0.5\n",
    "                    loss_r_full_t += loss_r_t * 0.5\n",
    "            loss_full_t = loss_v_full_t + loss_p_full_t + loss_r_full_t\n",
    "            loss_full_t.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            writer.add_scalar(\"loss_p\", loss_p_full_t.item(), step_idx)\n",
    "            writer.add_scalar(\"loss_v\", loss_v_full_t.item(), step_idx)\n",
    "            writer.add_scalar(\"loss_r\", loss_r_full_t.item(), step_idx)\n",
    "\n",
    "\n",
    "        writer.add_scalar(\"time_train\", time() - ts, step_idx)\n",
    "        writer.add_scalar(\"temp\", temperature, step_idx)\n",
    "        temperature = max(0.0, temperature - TEMP_STEP)\n",
    "\n",
    "        # evaluate net\n",
    "        if step_idx % EVALUATE_EVERY_STEP == 0:\n",
    "            ts = time()\n",
    "            win_ratio, avg_steps = evaluate(net, best_net, params)\n",
    "            print(\"Net evaluated, win ratio = %.2f\" % win_ratio)\n",
    "            writer.add_scalar(\"eval_win_ratio\", win_ratio, step_idx)\n",
    "            writer.add_scalar(\"eval_steps\", avg_steps, step_idx)\n",
    "            if win_ratio > BEST_NET_WIN_RATIO:\n",
    "                print(\"Net is better than cur best, sync\")\n",
    "                best_net.sync(net)\n",
    "                best_idx += 1\n",
    "                file_name = os.path.join(saves_path, \"best_%03d_%05d.dat\" % (best_idx, step_idx))\n",
    "                torch.save(best_net.get_state_dict(), file_name)\n",
    "            writer.add_scalar(\"time_eval\", time() - ts, step_idx)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "executable": "/usr/bin/env python3",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
