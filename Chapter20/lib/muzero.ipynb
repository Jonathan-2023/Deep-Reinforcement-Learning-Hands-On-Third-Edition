{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d35f6ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "MuZero implementation.\n",
    "\"\"\"\n",
    "from dataclasses import dataclass\n",
    "import math as m\n",
    "import typing as tt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "864efc3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib import game\n",
    "# reuse function as net input matches\n",
    "from lib.model import state_lists_to_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "553ca5bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "OBS_SHAPE = (2, game.GAME_ROWS, game.GAME_COLS)\n",
    "HIDDEN_STATE_SIZE = 64\n",
    "NUM_FILTERS = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "847f994e",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "Action = int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d4afaf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class MuZeroParams:\n",
    "    actions_count: int = game.GAME_COLS\n",
    "    max_moves: int = game.GAME_COLS * game.GAME_ROWS >> 2 + 1\n",
    "    dirichlet_alpha: float = 0.3\n",
    "    discount: float = 1.0\n",
    "    # how long hidden states dynamics are unrolled\n",
    "    unroll_steps: int = 5\n",
    "\n",
    "    # UCB formula\n",
    "    pb_c_base: int = 19652\n",
    "    pb_c_init: float = 1.25\n",
    "\n",
    "    dev: torch.device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f126ecaf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a12b5a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MinMaxStats:\n",
    "    \"\"\"A class that holds the min-max values of the tree.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.maximum = -1.0\n",
    "        self.minimum = 1.0\n",
    "\n",
    "    def update(self, value: float):\n",
    "        self.maximum = max(self.maximum, value)\n",
    "        self.minimum = min(self.minimum, value)\n",
    "\n",
    "    def normalize(self, value: float) -> float:\n",
    "        return (value - self.minimum) / (self.maximum - self.minimum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0732a09b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MCTSNode:\n",
    "    def __init__(self, prior: float, first_plays: bool):\n",
    "        self.first_plays: bool = first_plays\n",
    "        self.visit_count = 0\n",
    "        self.value_sum = 0.0\n",
    "        self.prior = prior\n",
    "        self.children: tt.Dict[Action, MCTSNode] = {}\n",
    "        # node is not expanded, so has no hidden state\n",
    "        self.h = None\n",
    "        # predicted reward\n",
    "        self.r = 0.0\n",
    "\n",
    "    @property\n",
    "    def is_expanded(self) -> bool:\n",
    "        return bool(self.children)\n",
    "\n",
    "    @property\n",
    "    def value(self) -> float:\n",
    "        return 0 if not self.visit_count else self.value_sum / self.visit_count\n",
    "\n",
    "    def select_child(self, params: MuZeroParams, min_max: MinMaxStats) -> \\\n",
    "            tt.Tuple[Action, \"MCTSNode\"]:\n",
    "        max_ucb, best_action, best_node = None, None, None\n",
    "        for action, node in self.children.items():\n",
    "            ucb = ucb_value(params, self, node, min_max)\n",
    "            if max_ucb is None or max_ucb < ucb:\n",
    "                max_ucb = ucb\n",
    "                best_action = action\n",
    "                best_node = node\n",
    "        return best_action, best_node\n",
    "\n",
    "    def get_act_probs(self, t: float = 1) -> tt.List[float]:\n",
    "        child_visits = sum(map(lambda n: n.visit_count, self.children.values()))\n",
    "        p = np.array([(child.visit_count / child_visits) ** (1 / t)\n",
    "                      for _, child in sorted(self.children.items())])\n",
    "        p /= sum(p)\n",
    "        return list(p)\n",
    "\n",
    "    def select_action(self, t: float, params: MuZeroParams) -> Action:\n",
    "        \"\"\"\n",
    "        Select action from visit counts using softmax with temperature.\n",
    "        :param t: temperature to be used (from 0.00001 to inf)\n",
    "        :return: sampled action\n",
    "        \"\"\"\n",
    "        act_vals = list(sorted(self.children.keys()))\n",
    "\n",
    "        if not act_vals:\n",
    "            res = np.random.choice(params.actions_count)\n",
    "        elif t < 0.0001:\n",
    "            res, _ = max(self.children.items(), key=lambda p: p[1].visit_count)\n",
    "        else:\n",
    "            p = self.get_act_probs(t)\n",
    "            res = int(np.random.choice(act_vals, p=p))\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6c88b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReprModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Representation model, maps observations into the hidden state\n",
    "    \"\"\"\n",
    "    def __init__(self, input_shape: tt.Tuple[int, ...]):\n",
    "        super(ReprModel, self).__init__()\n",
    "        self.conv_in = nn.Sequential(\n",
    "            nn.Conv2d(input_shape[0], NUM_FILTERS, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(NUM_FILTERS),\n",
    "            nn.LeakyReLU()\n",
    "        )\n",
    "        # layers with residual\n",
    "        self.conv_1 = nn.Sequential(\n",
    "            nn.Conv2d(NUM_FILTERS, NUM_FILTERS, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(NUM_FILTERS),\n",
    "            nn.LeakyReLU()\n",
    "        )\n",
    "        self.conv_2 = nn.Sequential(\n",
    "            nn.Conv2d(NUM_FILTERS, NUM_FILTERS, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(NUM_FILTERS),\n",
    "            nn.LeakyReLU()\n",
    "        )\n",
    "        self.conv_3 = nn.Sequential(\n",
    "            nn.Conv2d(NUM_FILTERS, NUM_FILTERS, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(NUM_FILTERS),\n",
    "            nn.LeakyReLU()\n",
    "        )\n",
    "        self.conv_4 = nn.Sequential(\n",
    "            nn.Conv2d(NUM_FILTERS, NUM_FILTERS, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(NUM_FILTERS),\n",
    "            nn.LeakyReLU()\n",
    "        )\n",
    "        self.conv_5 = nn.Sequential(\n",
    "            nn.Conv2d(NUM_FILTERS, NUM_FILTERS, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(NUM_FILTERS),\n",
    "            nn.LeakyReLU(),\n",
    "        )\n",
    "        self.conv_out = nn.Sequential(\n",
    "            nn.Conv2d(NUM_FILTERS, 16, kernel_size=1),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Flatten()\n",
    "        )\n",
    "\n",
    "        body_shape = (NUM_FILTERS,) + input_shape[1:]\n",
    "        size = self.conv_out(torch.zeros(1, *body_shape)).size()[-1]\n",
    "        self.out = nn.Sequential(\n",
    "            nn.Linear(size, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, HIDDEN_STATE_SIZE),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        v = self.conv_in(x)\n",
    "        v = v + self.conv_1(v)\n",
    "        v = v + self.conv_2(v)\n",
    "        v = v + self.conv_3(v)\n",
    "        v = v + self.conv_4(v)\n",
    "        v = v + self.conv_5(v)\n",
    "        c_out = self.conv_out(v)\n",
    "        out = self.out(c_out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffb99a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PredModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Prediction model, maps hidden state into policy and value\n",
    "    \"\"\"\n",
    "    def __init__(self, actions: int):\n",
    "        super(PredModel, self).__init__()\n",
    "        self.policy = nn.Sequential(\n",
    "            nn.Linear(HIDDEN_STATE_SIZE, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, actions),\n",
    "        )\n",
    "\n",
    "        self.value = nn.Sequential(\n",
    "            nn.Linear(HIDDEN_STATE_SIZE, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x) -> tt.Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Returns policy and value from the hidden state\n",
    "        \"\"\"\n",
    "        return self.policy(x), self.value(x).squeeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9e04ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DynamicsModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Dynamics model, maps hidden state and action into\n",
    "    reward and new hidden state.\n",
    "    \"\"\"\n",
    "    def __init__(self, actions: int):\n",
    "        super(DynamicsModel, self).__init__()\n",
    "        self.reward = nn.Sequential(\n",
    "            nn.Linear(HIDDEN_STATE_SIZE + actions, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 1),\n",
    "        )\n",
    "\n",
    "        self.hidden = nn.Sequential(\n",
    "            nn.Linear(HIDDEN_STATE_SIZE + actions, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, HIDDEN_STATE_SIZE),\n",
    "        )\n",
    "\n",
    "    def forward(self, h: torch.Tensor, a: torch.Tensor) -> \\\n",
    "            tt.Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Forward pass of dynamics model\n",
    "        :param h: batch of hidden states\n",
    "        :param a: batch of one-hot actions\n",
    "        :return: predicted rewards and new hidden states\n",
    "        \"\"\"\n",
    "        x = torch.hstack((h, a))\n",
    "        return self.reward(x).squeeze(1), self.hidden(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b91f7e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MuZeroModels:\n",
    "    def __init__(self, input_shape: tt.Tuple[int, ...], actions: int):\n",
    "        self.repr = ReprModel(input_shape)\n",
    "        self.pred = PredModel(actions)\n",
    "        self.dynamics = DynamicsModel(actions)\n",
    "\n",
    "    def to(self, dev: torch.device):\n",
    "        self.repr.to(dev)\n",
    "        self.pred.to(dev)\n",
    "        self.dynamics.to(dev)\n",
    "\n",
    "    def sync(self, src: \"MuZeroModels\"):\n",
    "        self.repr.load_state_dict(src.repr.state_dict())\n",
    "        self.pred.load_state_dict(src.pred.state_dict())\n",
    "        self.dynamics.load_state_dict(src.dynamics.state_dict())\n",
    "\n",
    "    def get_state_dict(self) -> tt.Dict[str, dict]:\n",
    "        return {\n",
    "            \"repr\": self.repr.state_dict(),\n",
    "            \"pred\": self.pred.state_dict(),\n",
    "            \"dynamics\": self.dynamics.state_dict(),\n",
    "        }\n",
    "\n",
    "    def set_state_dict(self, d: dict):\n",
    "        self.repr.load_state_dict(d['repr'])\n",
    "        self.pred.load_state_dict(d['pred'])\n",
    "        self.dynamics.load_state_dict(d['dynamics'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7835d267",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class EpisodeStep:\n",
    "    state: int\n",
    "    player_idx: int\n",
    "    action: int\n",
    "    reward: int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04aabe8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Episode:\n",
    "    def __init__(self):\n",
    "        self.steps: tt.List[EpisodeStep] = []\n",
    "        self.action_probs: tt.List[tt.List[float]] = []\n",
    "        self.root_values: tt.List[float] = []\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.steps)\n",
    "\n",
    "    def add_step(self, step: EpisodeStep, node: MCTSNode):\n",
    "        self.steps.append(step)\n",
    "        self.action_probs.append(node.get_act_probs())\n",
    "        self.root_values.append(node.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4ea35b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ucb_value(params: MuZeroParams, parent: MCTSNode, child: MCTSNode,\n",
    "              min_max: MinMaxStats) -> float:\n",
    "    pb_c = m.log((parent.visit_count + params.pb_c_base + 1) /\n",
    "                 params.pb_c_base) + params.pb_c_init\n",
    "    pb_c *= m.sqrt(parent.visit_count) / (child.visit_count + 1)\n",
    "    prior_score = pb_c * child.prior\n",
    "    value_score = 0.0\n",
    "    if child.visit_count > 0:\n",
    "        value_score = min_max.normalize(child.value + child.r)\n",
    "    return prior_score + value_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a46420b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_expanded_root(player_idx: int, game_state_int: int, params: MuZeroParams,\n",
    "                       models: MuZeroModels, min_max: MinMaxStats) -> MCTSNode:\n",
    "    root = MCTSNode(1.0, player_idx == 0)\n",
    "    state_list = game.decode_binary(game_state_int)\n",
    "    state_t = state_lists_to_batch([state_list], [player_idx], device=params.dev)\n",
    "    h_t = models.repr(state_t)\n",
    "    root.h = h_t[0].cpu().numpy()\n",
    "\n",
    "    p_t, v_t = models.pred(h_t)\n",
    "    # logits to probs\n",
    "    p_t.exp_()\n",
    "    probs_t = p_t.squeeze(0) / p_t.sum()\n",
    "    probs = probs_t.cpu().numpy()\n",
    "    # add dirichlet noise\n",
    "    noises = np.random.dirichlet([params.dirichlet_alpha] * params.actions_count)\n",
    "    probs = probs * 0.75 + noises * 0.25\n",
    "    for a, prob in enumerate(probs):\n",
    "        root.children[a] = MCTSNode(prob, not root.first_plays)\n",
    "    # backpropagate value\n",
    "    v = v_t.cpu().item()\n",
    "    backpropagate([root], v, root.first_plays, params, min_max)\n",
    "    return root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17ceed02",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2012fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_node(parent: MCTSNode, node: MCTSNode, last_action: Action,\n",
    "                params: MuZeroParams, models: MuZeroModels) -> float:\n",
    "    \"\"\"\n",
    "    Performs node expansion using models.\n",
    "    Return predicted value for the node's state.\n",
    "    :param parent: parent's node\n",
    "    :param node: node to be expanded\n",
    "    :param action: action from the parent's node\n",
    "    :param params: hyperparams\n",
    "    :param models: models\n",
    "    :return: predicted value to be backpropagated\n",
    "    \"\"\"\n",
    "    h_t = torch.as_tensor(parent.h, dtype=torch.float32, device=params.dev)\n",
    "    h_t.unsqueeze_(0)\n",
    "    p_t, v_t = models.pred(h_t)\n",
    "    # one-hot of actions\n",
    "    a_t = torch.zeros(params.actions_count, dtype=torch.float32, device=params.dev)\n",
    "    a_t[last_action] = 1.0\n",
    "    a_t.unsqueeze_(0)\n",
    "    # predict the reward and the next hidden state\n",
    "    r_t, h_next_t = models.dynamics(h_t, a_t)\n",
    "    node.h = h_next_t[0].cpu().numpy()\n",
    "    node.r = float(r_t[0].cpu().item())\n",
    "\n",
    "    # convert logits to probs\n",
    "    p_t.squeeze_(0)\n",
    "    p_t.exp_()\n",
    "    probs_t = p_t / p_t.sum()\n",
    "    probs = probs_t.cpu().numpy()\n",
    "    for a, prob in enumerate(probs):\n",
    "        node.children[a] = MCTSNode(prob, not node.first_plays)\n",
    "    return float(v_t.cpu().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59291c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backpropagate(search_path: tt.List[MCTSNode], value: float, first_plays: bool,\n",
    "                  params: MuZeroParams, min_max: MinMaxStats):\n",
    "    for node in reversed(search_path):\n",
    "        node.value_sum += value if node.first_plays == first_plays else -value\n",
    "        node.visit_count += 1\n",
    "        value = node.r + params.discount * value\n",
    "        min_max.update(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d174dc91",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def run_mcts(player_idx: int, root_state_int: int, params: MuZeroParams,\n",
    "             models: MuZeroModels, min_max: MinMaxStats,\n",
    "             search_rounds: int = 800) -> MCTSNode:\n",
    "    # prepare root node\n",
    "    root = make_expanded_root(player_idx, root_state_int, params, models, min_max)\n",
    "    for _ in range(search_rounds):\n",
    "        search_path = [root]\n",
    "        parent_node = None\n",
    "        last_action = 0             # to make type checker happy\n",
    "        node = root\n",
    "        while node.is_expanded:\n",
    "            action, new_node = node.select_child(params, min_max)\n",
    "            last_action = action\n",
    "            parent_node = node\n",
    "            node = new_node\n",
    "            search_path.append(new_node)\n",
    "        value = expand_node(parent_node, node, last_action, params, models)\n",
    "        backpropagate(search_path, value, node.first_plays, params, min_max)\n",
    "    return root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87cfa145",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def play_game(\n",
    "        player1: MuZeroModels, player2: MuZeroModels, params: MuZeroParams,\n",
    "        temperature: float, init_state: tt.Optional[int] = None\n",
    ") -> tt.Tuple[int, Episode]:\n",
    "    episode = Episode()\n",
    "    state = game.INITIAL_STATE if init_state is None else init_state\n",
    "    players = [player1, player2]\n",
    "    player_idx = 0\n",
    "    reward = 0\n",
    "    min_max = MinMaxStats()\n",
    "\n",
    "    while True:\n",
    "        possible_actions = game.possible_moves(state)\n",
    "        # we have a draw situation\n",
    "        if not possible_actions:\n",
    "            break\n",
    "\n",
    "        root_node = run_mcts(player_idx, state, params, players[player_idx], min_max)\n",
    "        # run_mcts(node, action, params, players[player_idx])\n",
    "        action = root_node.select_action(temperature, params)\n",
    "\n",
    "        # act randomly on wrong move\n",
    "        if action not in possible_actions:\n",
    "            action = int(np.random.choice(possible_actions))\n",
    "\n",
    "        new_state, won = game.move(state, action, player_idx)\n",
    "        if won:\n",
    "            if player_idx == 0:\n",
    "                reward = 1\n",
    "            else:\n",
    "                reward = -1\n",
    "        step = EpisodeStep(state, player_idx, action, reward)\n",
    "        episode.add_step(step, root_node)\n",
    "        if won:\n",
    "            break\n",
    "        player_idx = (player_idx + 1) % 2\n",
    "        state = new_state\n",
    "    return reward, episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84668a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_batch(\n",
    "        episode_buffer: tt.Deque[Episode], batch_size: int, params: MuZeroParams,\n",
    ") -> tt.Tuple[\n",
    "    torch.Tensor, tt.Tuple[torch.Tensor, ...], tt.Tuple[torch.Tensor, ...],\n",
    "    tt.Tuple[torch.Tensor, ...], tt.Tuple[torch.Tensor, ...],\n",
    "]:\n",
    "    \"\"\"\n",
    "    Sample training batch from episode buffer\n",
    "    :param episode_buffer: buffer with episodes\n",
    "    :param batch_size: size of the batch\n",
    "    :param params: hyperparameters\n",
    "    :return: tensor with encoded states,\n",
    "        tuple with one-hot actions,\n",
    "        tuple with unrolled policy targets,\n",
    "        tuple with unrolled rewards,\n",
    "        tuple with unrolled values\n",
    "    \"\"\"\n",
    "    states = []\n",
    "    player_indices = []\n",
    "    actions = [[] for _ in range(params.unroll_steps)]\n",
    "    policy_targets = [[] for _ in range(params.unroll_steps)]\n",
    "    rewards = [[] for _ in range(params.unroll_steps)]\n",
    "    values = [[] for _ in range(params.unroll_steps)]\n",
    "\n",
    "    for episode in np.random.choice(episode_buffer, batch_size):\n",
    "        assert isinstance(episode, Episode)\n",
    "        ofs = np.random.choice(len(episode) - params.unroll_steps)\n",
    "        state = game.decode_binary(episode.steps[ofs].state)\n",
    "        states.append(state)\n",
    "        player_indices.append(episode.steps[ofs].player_idx)\n",
    "\n",
    "        for s in range(params.unroll_steps):\n",
    "            full_ofs = ofs + s\n",
    "            actions[s].append(episode.steps[full_ofs].action)\n",
    "            rewards[s].append(episode.steps[full_ofs].reward)\n",
    "            policy_targets[s].append(episode.action_probs[full_ofs])\n",
    "\n",
    "            # compute discounted value target till the end of episode\n",
    "            value = 0.0\n",
    "            for step in reversed(episode.steps[full_ofs:]):\n",
    "                value *= params.discount\n",
    "                value += step.reward\n",
    "            values[s].append(value)\n",
    "    states_t = state_lists_to_batch(states, player_indices, device=params.dev)\n",
    "    res_actions = tuple(\n",
    "        torch.as_tensor(np.eye(params.actions_count)[a],\n",
    "                        dtype=torch.float32, device=params.dev)\n",
    "        for a in actions\n",
    "    )\n",
    "    res_policies = tuple(\n",
    "        torch.as_tensor(p, dtype=torch.float32, device=params.dev)\n",
    "        for p in policy_targets\n",
    "    )\n",
    "    res_rewards = tuple(\n",
    "        torch.as_tensor(r, dtype=torch.float32, device=params.dev)\n",
    "        for r in rewards\n",
    "    )\n",
    "    res_values = tuple(\n",
    "        torch.as_tensor(v, dtype=torch.float32, device=params.dev)\n",
    "        for v in values\n",
    "    )\n",
    "    return states_t, res_actions, res_policies, res_rewards, res_values"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
