{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fd352b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import gymnasium as gym\n",
    "import ptan\n",
    "from ptan.experience import ExperienceFirstLast\n",
    "import typing as tt\n",
    "import numpy as np\n",
    "from ray import tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5846f3aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6d257e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ignite.engine import Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f03d560",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib import common, dqn_extra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39421e02",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "mpl.use(\"Agg\")\n",
    "import matplotlib.pylab as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a735559f",
   "metadata": {},
   "outputs": [],
   "source": [
    "NAME = \"07_distrib\"\n",
    "STATES_TO_EVALUATE = 64\n",
    "EVAL_EVERY_GAME = 10\n",
    "IMG_EVERY_GAME = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65871dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "BEST_PONG = common.GAME_PARAMS['pong']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "659bdaf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_values_of_states(\n",
    "        states: np.ndarray, net: dqn_extra.DistributionalDQN,\n",
    "        device: torch.device) -> float:\n",
    "    mean_vals = []\n",
    "    for batch in np.array_split(states, 64):\n",
    "        states_v = torch.as_tensor(batch).to(device)\n",
    "        action_values_v = net.qvals(states_v)\n",
    "        best_action_values_v = action_values_v.max(1)[0]\n",
    "        mean_vals.append(best_action_values_v.mean().item())\n",
    "    return float(np.mean(mean_vals))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb116770",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_state_images(\n",
    "        path_prefix: str, game_idx: int, states: np.ndarray,\n",
    "        net: dqn_extra.DistributionalDQN,\n",
    "        device: torch.device\n",
    "):\n",
    "    p = np.arange(dqn_extra.Vmin, dqn_extra.Vmax +\n",
    "                  dqn_extra.DELTA_Z, dqn_extra.DELTA_Z)\n",
    "    states_v = torch.as_tensor(states).to(device)\n",
    "    action_prob = net.apply_softmax(net(states_v)).data.cpu().numpy()\n",
    "    batch_size, num_actions, _ = action_prob.shape\n",
    "    for batch_idx in range(batch_size):\n",
    "        plt.clf()\n",
    "        for action_idx in range(num_actions):\n",
    "            plt.subplot(num_actions, 1, action_idx+1)\n",
    "            plt.bar(p, action_prob[batch_idx, action_idx], width=0.5)\n",
    "        plt.savefig(\"%s/%05d_%08d.png\" % (\n",
    "            path_prefix, batch_idx, game_idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8da31a02",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_loss(batch: tt.List[ExperienceFirstLast], net: dqn_extra.DistributionalDQN,\n",
    "              tgt_net: dqn_extra.DistributionalDQN, gamma: float,\n",
    "              device: torch.device) -> torch.Tensor:\n",
    "    states, actions, rewards, dones, next_states = common.unpack_batch(batch)\n",
    "    batch_size = len(batch)\n",
    "\n",
    "    states_v = torch.as_tensor(states).to(device)\n",
    "    actions_v = torch.tensor(actions).to(device)\n",
    "    next_states_v = torch.as_tensor(next_states).to(device)\n",
    "\n",
    "    # next state distribution\n",
    "    next_distr_v, next_qvals_v = tgt_net.both(next_states_v)\n",
    "    next_acts = next_qvals_v.max(1)[1].data.cpu().numpy()\n",
    "    next_distr = tgt_net.apply_softmax(next_distr_v)\n",
    "    next_distr = next_distr.data.cpu().numpy()\n",
    "\n",
    "    next_best_distr = next_distr[range(batch_size), next_acts]\n",
    "    proj_distr = dqn_extra.distr_projection(next_best_distr, rewards, dones, gamma)\n",
    "\n",
    "    distr_v = net(states_v)\n",
    "    sa_vals = distr_v[range(batch_size), actions_v.data]\n",
    "    state_log_sm_v = F.log_softmax(sa_vals, dim=1)\n",
    "    proj_distr_v = torch.tensor(proj_distr).to(device)\n",
    "\n",
    "    loss_v = -state_log_sm_v * proj_distr_v\n",
    "    return loss_v.sum(dim=1).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9042e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(params: common.Hyperparams,\n",
    "          device: torch.device, extra: dict) -> tt.Optional[int]:\n",
    "    img_path = extra.get(\"img_path\")\n",
    "    env = gym.make(params.env_name)\n",
    "    env = ptan.common.wrappers.wrap_dqn(env)\n",
    "\n",
    "    net = dqn_extra.DistributionalDQN(env.observation_space.shape, env.action_space.n).to(device)\n",
    "    print(net)\n",
    "    tgt_net = ptan.agent.TargetNet(net)\n",
    "    selector = ptan.actions.EpsilonGreedyActionSelector(epsilon=params.epsilon_start)\n",
    "    epsilon_tracker = common.EpsilonTracker(selector, params)\n",
    "    agent = ptan.agent.DQNAgent(lambda x: net.qvals(x), selector, device=device)\n",
    "\n",
    "    exp_source = ptan.experience.ExperienceSourceFirstLast(\n",
    "        env, agent, gamma=params.gamma, env_seed=common.SEED)\n",
    "    buffer = ptan.experience.ExperienceReplayBuffer(\n",
    "        exp_source, buffer_size=params.replay_size)\n",
    "    optimizer = optim.Adam(net.parameters(), lr=params.learning_rate)\n",
    "\n",
    "    def process_batch(engine, batch):\n",
    "        optimizer.zero_grad()\n",
    "        loss_v = calc_loss(batch, net, tgt_net.target_model,\n",
    "                           gamma=params.gamma, device=device)\n",
    "        loss_v.backward()\n",
    "        optimizer.step()\n",
    "        epsilon_tracker.frame(engine.state.iteration)\n",
    "        if engine.state.iteration % params.target_net_sync == 0:\n",
    "            tgt_net.sync()\n",
    "\n",
    "        if img_path is not None:\n",
    "            eval_states = getattr(engine.state, \"eval_states\", None)\n",
    "            if eval_states is None:\n",
    "                eval_states = buffer.sample(STATES_TO_EVALUATE)\n",
    "                eval_states = [np.asarray(transition.state) for transition in eval_states]\n",
    "                engine.state.eval_states = np.asarray(eval_states)\n",
    "\n",
    "            if engine.state.episode % EVAL_EVERY_GAME == 0:\n",
    "                engine.state.metrics[\"values\"] = \\\n",
    "                    calc_values_of_states(eval_states, net, device=device)\n",
    "\n",
    "            if engine.state.episode % IMG_EVERY_GAME == 0:\n",
    "                save_state_images(img_path, engine.state.episode,\n",
    "                                  eval_states, net, device=device)\n",
    "\n",
    "        return {\n",
    "            \"loss\": loss_v.item(),\n",
    "            \"epsilon\": selector.epsilon,\n",
    "        }\n",
    "\n",
    "    engine = Engine(process_batch)\n",
    "    common.setup_ignite(engine, params, exp_source, NAME,\n",
    "                        tuner_reward_episode=200)\n",
    "    r = engine.run(common.batch_generator(\n",
    "        buffer, params.replay_initial, params.batch_size))\n",
    "    if r.solved:\n",
    "        return r.episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f80e240",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    parser = common.argparser()\n",
    "    parser.add_argument(\"--img-path\", help=\"Set image path\")\n",
    "    args = parser.parse_args()\n",
    "    if args.img_path is not None:\n",
    "        pathlib.Path(args.img_path).mkdir(parents=True, exist_ok=True)\n",
    "    common.train_or_tune(\n",
    "        args, train, BEST_PONG,\n",
    "        extra_params={\n",
    "            \"img_path\": args.img_path,\n",
    "        },\n",
    "        extra_space={\n",
    "            \"learning_rate\": tune.loguniform(5e-5, 1e-3)\n",
    "        }\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "executable": "/usr/bin/env python3",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
