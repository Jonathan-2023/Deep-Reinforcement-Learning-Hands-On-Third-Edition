{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50256ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from ptan.experience import ExperienceReplayBuffer, ExperienceSource, ExperienceFirstLast\n",
    "import torch\n",
    "from torch import nn as nn\n",
    "from torchrl.modules import NoisyLinear\n",
    "import typing as tt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cdd96f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# replay buffer params\n",
    "BETA_START = 0.4\n",
    "BETA_FRAMES = 100000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa7acfd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# distributional DQN params\n",
    "Vmax = 10\n",
    "Vmin = -10\n",
    "N_ATOMS = 51\n",
    "DELTA_Z = (Vmax - Vmin) / (N_ATOMS - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fc8713a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NoisyDQN(nn.Module):\n",
    "    def __init__(self, input_shape: tt.Tuple[int, ...],\n",
    "                 n_actions: int):\n",
    "        super(NoisyDQN, self).__init__()\n",
    "\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(input_shape[0], 32, kernel_size=8, stride=4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "        )\n",
    "        size = self.conv(torch.zeros(1, *input_shape)).size()[-1]\n",
    "        self.noisy_layers = [\n",
    "            NoisyLinear(size, 512),\n",
    "            NoisyLinear(512, n_actions)\n",
    "        ]\n",
    "        self.fc = nn.Sequential(\n",
    "            self.noisy_layers[0],\n",
    "            nn.ReLU(),\n",
    "            self.noisy_layers[1],\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.ByteTensor):\n",
    "        xx = x / 255.0\n",
    "        return self.fc(self.conv(xx))\n",
    "\n",
    "    def reset_noise(self):\n",
    "        for n in self.noisy_layers:\n",
    "            n.reset_noise()\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def noisy_layers_sigma_snr(self) -> tt.List[float]:\n",
    "        return [\n",
    "            ((layer.weight_mu ** 2).mean().sqrt() /\n",
    "             (layer.weight_sigma ** 2).mean().sqrt()).item()\n",
    "            for layer in self.noisy_layers\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a146ecbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrioReplayBuffer(ExperienceReplayBuffer):\n",
    "    def __init__(self, exp_source: ExperienceSource, buf_size: int,\n",
    "                 prob_alpha: float = 0.6):\n",
    "        super().__init__(exp_source, buf_size)\n",
    "        self.experience_source_iter = iter(exp_source)\n",
    "        self.capacity = buf_size\n",
    "        self.pos = 0\n",
    "        self.buffer = []\n",
    "        self.prob_alpha = prob_alpha\n",
    "        self.priorities = np.zeros((buf_size, ), dtype=np.float32)\n",
    "        self.beta = BETA_START\n",
    "\n",
    "    def update_beta(self, idx: int) -> float:\n",
    "        v = BETA_START + idx * (1.0 - BETA_START) / BETA_FRAMES\n",
    "        self.beta = min(1.0, v)\n",
    "        return self.beta\n",
    "\n",
    "    def populate(self, count: int):\n",
    "        max_prio = self.priorities.max(initial=1.0)\n",
    "        for _ in range(count):\n",
    "            sample = next(self.experience_source_iter)\n",
    "            if len(self.buffer) < self.capacity:\n",
    "                self.buffer.append(sample)\n",
    "            else:\n",
    "                self.buffer[self.pos] = sample\n",
    "            self.priorities[self.pos] = max_prio\n",
    "            self.pos = (self.pos + 1) % self.capacity\n",
    "\n",
    "    def sample(self, batch_size: int) -> tt.Tuple[\n",
    "        tt.List[ExperienceFirstLast], np.ndarray, np.ndarray\n",
    "    ]:\n",
    "        if len(self.buffer) == self.capacity:\n",
    "            prios = self.priorities\n",
    "        else:\n",
    "            prios = self.priorities[:self.pos]\n",
    "        probs = prios ** self.prob_alpha\n",
    "        probs /= probs.sum()\n",
    "\n",
    "        indices = np.random.choice(len(self.buffer), batch_size, p=probs)\n",
    "        samples = [self.buffer[idx] for idx in indices]\n",
    "        total = len(self.buffer)\n",
    "        weights = (total * probs[indices]) ** (-self.beta)\n",
    "        weights /= weights.max()\n",
    "        return samples, indices, np.array(weights, dtype=np.float32)\n",
    "\n",
    "    def update_priorities(self, batch_indices: np.ndarray, batch_priorities: np.ndarray):\n",
    "        for idx, prio in zip(batch_indices, batch_priorities):\n",
    "            self.priorities[idx] = prio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0b90f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DuelingDQN(nn.Module):\n",
    "    def __init__(self, input_shape: tt.Tuple[int, ...], n_actions: int):\n",
    "        super(DuelingDQN, self).__init__()\n",
    "\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(input_shape[0], 32, kernel_size=8, stride=4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten()\n",
    "        )\n",
    "        size = self.conv(torch.zeros(1, *input_shape)).size()[-1]\n",
    "        self.fc_adv = nn.Sequential(\n",
    "            nn.Linear(size, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, n_actions)\n",
    "        )\n",
    "        self.fc_val = nn.Sequential(\n",
    "            nn.Linear(size, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.ByteTensor):\n",
    "        adv, val = self.adv_val(x)\n",
    "        return val + (adv - adv.mean(dim=1, keepdim=True))\n",
    "\n",
    "    def adv_val(self, x: torch.ByteTensor):\n",
    "        xx = x / 255.0\n",
    "        conv_out = self.conv(xx)\n",
    "        return self.fc_adv(conv_out), self.fc_val(conv_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c239a132",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DistributionalDQN(nn.Module):\n",
    "    def __init__(self, input_shape: tt.Tuple[int, ...], n_actions: int):\n",
    "        super(DistributionalDQN, self).__init__()\n",
    "\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(input_shape[0], 32, kernel_size=8, stride=4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten()\n",
    "        )\n",
    "        size = self.conv(torch.zeros(1, *input_shape)).size()[-1]\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(size, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, n_actions * N_ATOMS)\n",
    "        )\n",
    "\n",
    "        sups = torch.arange(Vmin, Vmax + DELTA_Z, DELTA_Z)\n",
    "        self.register_buffer(\"supports\", sups)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x: torch.ByteTensor) -> torch.Tensor:\n",
    "        batch_size = x.size()[0]\n",
    "        xx = x / 255\n",
    "        fc_out = self.fc(self.conv(xx))\n",
    "        return fc_out.view(batch_size, -1, N_ATOMS)\n",
    "\n",
    "    def both(self, x: torch.ByteTensor) -> tt.Tuple[torch.Tensor, torch.Tensor]:\n",
    "        cat_out = self(x)\n",
    "        probs = self.apply_softmax(cat_out)\n",
    "        weights = probs * self.supports\n",
    "        res = weights.sum(dim=2)\n",
    "        return cat_out, res\n",
    "\n",
    "    def qvals(self, x: torch.ByteTensor) -> torch.Tensor:\n",
    "        return self.both(x)[1]\n",
    "\n",
    "    def apply_softmax(self, t: torch.Tensor) -> torch.Tensor:\n",
    "        return self.softmax(t.view(-1, N_ATOMS)).view(t.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c28c6b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def distr_projection(next_distr: np.ndarray, rewards: np.ndarray,\n",
    "                     dones: np.ndarray, gamma: float):\n",
    "    \"\"\"\n",
    "    Perform distribution projection aka Catergorical Algorithm from the\n",
    "    \"A Distributional Perspective on RL\" paper\n",
    "    \"\"\"\n",
    "    batch_size = len(rewards)\n",
    "    proj_distr = np.zeros((batch_size, N_ATOMS), dtype=np.float32)\n",
    "    delta_z = (Vmax - Vmin) / (N_ATOMS - 1)\n",
    "    for atom in range(N_ATOMS):\n",
    "        v = rewards + (Vmin + atom * delta_z) * gamma\n",
    "        tz_j = np.minimum(Vmax, np.maximum(Vmin, v))\n",
    "        b_j = (tz_j - Vmin) / delta_z\n",
    "        l = np.floor(b_j).astype(np.int64)\n",
    "        u = np.ceil(b_j).astype(np.int64)\n",
    "        eq_mask = u == l\n",
    "        proj_distr[eq_mask, l[eq_mask]] += next_distr[eq_mask, atom]\n",
    "        ne_mask = u != l\n",
    "        proj_distr[ne_mask, l[ne_mask]] += next_distr[ne_mask, atom] * (u - b_j)[ne_mask]\n",
    "        proj_distr[ne_mask, u[ne_mask]] += next_distr[ne_mask, atom] * (b_j - l)[ne_mask]\n",
    "    if dones.any():\n",
    "        proj_distr[dones] = 0.0\n",
    "        tz_j = np.minimum(Vmax, np.maximum(Vmin, rewards[dones]))\n",
    "        b_j = (tz_j - Vmin) / delta_z\n",
    "        l = np.floor(b_j).astype(np.int64)\n",
    "        u = np.ceil(b_j).astype(np.int64)\n",
    "        eq_mask = u == l\n",
    "        eq_dones = dones.copy()\n",
    "        eq_dones[dones] = eq_mask\n",
    "        if eq_dones.any():\n",
    "            proj_distr[eq_dones, l[eq_mask]] = 1.0\n",
    "        ne_mask = u != l\n",
    "        ne_dones = dones.copy()\n",
    "        ne_dones[dones] = ne_mask\n",
    "        if ne_dones.any():\n",
    "            proj_distr[ne_dones, l[ne_mask]] = (u - b_j)[ne_mask]\n",
    "            proj_distr[ne_dones, u[ne_mask]] = (b_j - l)[ne_mask]\n",
    "    return proj_distr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a6bec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RainbowDQN(nn.Module):\n",
    "    def __init__(self, input_shape: tt.Tuple[int, ...],\n",
    "                 n_actions: int):\n",
    "        super(RainbowDQN, self).__init__()\n",
    "\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(input_shape[0], 32, kernel_size=8, stride=4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten()\n",
    "        )\n",
    "\n",
    "        size = self.conv(torch.zeros(1, *input_shape)).size()[-1]\n",
    "        self.noisy_layers = [\n",
    "            NoisyLinear(size, 256),\n",
    "            NoisyLinear(256, n_actions)\n",
    "        ]\n",
    "        self.fc_adv = nn.Sequential(\n",
    "            self.noisy_layers[0],\n",
    "            nn.ReLU(),\n",
    "            self.noisy_layers[1],\n",
    "        )\n",
    "        self.fc_val = nn.Sequential(\n",
    "            nn.Linear(size, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 1)\n",
    "        )\n",
    "\n",
    "    def reset_noise(self):\n",
    "        for n in self.noisy_layers:\n",
    "            n.reset_noise()\n",
    "\n",
    "    def forward(self, x: torch.ByteTensor):\n",
    "        adv, val = self.adv_val(x)\n",
    "        return val + (adv - adv.mean(dim=1, keepdim=True))\n",
    "\n",
    "    def adv_val(self, x: torch.ByteTensor):\n",
    "        xx = x / 255.0\n",
    "        conv_out = self.conv(xx)\n",
    "        return self.fc_adv(conv_out), self.fc_val(conv_out)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
