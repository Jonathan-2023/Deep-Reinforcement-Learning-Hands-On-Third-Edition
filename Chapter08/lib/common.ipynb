{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e215a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import argparse\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import warnings\n",
    "import dataclasses\n",
    "from datetime import timedelta, datetime\n",
    "import typing as tt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8216a822",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ptan.ignite as ptan_ignite\n",
    "from ptan.actions import EpsilonGreedyActionSelector\n",
    "from ptan.experience import ExperienceFirstLast, \\\n",
    "    ExperienceSourceFirstLast, ExperienceReplayBuffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f4bbe58",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ignite.engine import Engine\n",
    "from ignite.metrics import RunningAverage\n",
    "from ignite.contrib.handlers import tensorboard_logger as tb_logger\n",
    "from ray import tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a6c848d",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 123"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1005b461",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclasses.dataclass\n",
    "class Hyperparams:\n",
    "    env_name: str\n",
    "    stop_reward: float\n",
    "    run_name: str\n",
    "    replay_size: int\n",
    "    replay_initial: int\n",
    "    target_net_sync: int\n",
    "    epsilon_frames: int\n",
    "\n",
    "    learning_rate: float = 0.0001\n",
    "    batch_size: int = 32\n",
    "    gamma: float = 0.99\n",
    "    epsilon_start: float = 1.0\n",
    "    epsilon_final: float = 0.1\n",
    "\n",
    "    tuner_mode: bool = False\n",
    "    episodes_to_solve: int = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3690de3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "GAME_PARAMS = {\n",
    "    'pong': Hyperparams(\n",
    "        env_name=\"PongNoFrameskip-v4\",\n",
    "        stop_reward=18.0,\n",
    "        run_name=\"pong\",\n",
    "        replay_size=100_000,\n",
    "        replay_initial=10_000,\n",
    "        target_net_sync=1000,\n",
    "        epsilon_frames=100_000,\n",
    "        epsilon_final=0.02,\n",
    "    ),\n",
    "    'breakout-small': Hyperparams(\n",
    "        env_name=\"BreakoutNoFrameskip-v4\",\n",
    "        stop_reward=500.0,\n",
    "        run_name=\"breakout-small\",\n",
    "        replay_size=300_000,\n",
    "        replay_initial=20_000,\n",
    "        target_net_sync=1000,\n",
    "        epsilon_frames=1_000_000,\n",
    "        batch_size=64,\n",
    "    ),\n",
    "    'breakout': Hyperparams(\n",
    "        env_name=\"BreakoutNoFrameskip-v4\",\n",
    "        stop_reward=500.0,\n",
    "        run_name='breakout',\n",
    "        replay_size=1_000_000,\n",
    "        replay_initial=50_000,\n",
    "        target_net_sync=10_000,\n",
    "        epsilon_frames=10_000_000,\n",
    "        learning_rate=0.00025,\n",
    "    ),\n",
    "    'invaders': Hyperparams(\n",
    "        env_name=\"SpaceInvadersNoFrameskip-v4\",\n",
    "        stop_reward=500.0,\n",
    "        run_name='invaders',\n",
    "        replay_size=10_000_000,\n",
    "        replay_initial=50_000,\n",
    "        target_net_sync=10_000,\n",
    "        epsilon_frames=10_000_000,\n",
    "        learning_rate=0.00025,\n",
    "    ),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c81cd4f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unpack_batch(batch: tt.List[ExperienceFirstLast]):\n",
    "    states, actions, rewards, dones, last_states = [],[],[],[],[]\n",
    "    for exp in batch:\n",
    "        states.append(exp.state)\n",
    "        actions.append(exp.action)\n",
    "        rewards.append(exp.reward)\n",
    "        dones.append(exp.last_state is None)\n",
    "        if exp.last_state is None:\n",
    "            lstate = exp.state  # the result will be masked anyway\n",
    "        else:\n",
    "            lstate = exp.last_state\n",
    "        last_states.append(lstate)\n",
    "    return np.asarray(states), np.array(actions), np.array(rewards, dtype=np.float32), \\\n",
    "        np.array(dones, dtype=bool), np.asarray(last_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c43dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_loss_dqn(\n",
    "        batch: tt.List[ExperienceFirstLast], net: nn.Module, tgt_net: nn.Module,\n",
    "        gamma: float, device: torch.device) -> torch.Tensor:\n",
    "    states, actions, rewards, dones, next_states = unpack_batch(batch)\n",
    "\n",
    "    states_v = torch.as_tensor(states).to(device)\n",
    "    next_states_v = torch.as_tensor(next_states).to(device)\n",
    "    actions_v = torch.LongTensor(actions).to(device)\n",
    "    rewards_v = torch.FloatTensor(rewards).to(device)\n",
    "    done_mask = torch.BoolTensor(dones).to(device)\n",
    "\n",
    "    actions_v = actions_v.unsqueeze(-1)\n",
    "    state_action_vals = net(states_v).gather(1, actions_v)\n",
    "    state_action_vals = state_action_vals.squeeze(-1)\n",
    "    with torch.no_grad():\n",
    "        next_state_vals = tgt_net(next_states_v).max(1)[0]\n",
    "        next_state_vals[done_mask] = 0.0\n",
    "\n",
    "    bellman_vals = next_state_vals.detach() * gamma + rewards_v\n",
    "    return nn.MSELoss()(state_action_vals, bellman_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c04dd99",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EpsilonTracker:\n",
    "    def __init__(self, selector: EpsilonGreedyActionSelector, params: Hyperparams):\n",
    "        self.selector = selector\n",
    "        self.params = params\n",
    "        self.frame(0)\n",
    "\n",
    "    def frame(self, frame_idx: int):\n",
    "        eps = self.params.epsilon_start - frame_idx / self.params.epsilon_frames\n",
    "        self.selector.epsilon = max(self.params.epsilon_final, eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "783901d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_generator(buffer: ExperienceReplayBuffer, initial: int, batch_size: int) -> \\\n",
    "        tt.Generator[tt.List[ExperienceFirstLast], None, None]:\n",
    "    buffer.populate(initial)\n",
    "    while True:\n",
    "        buffer.populate(1)\n",
    "        yield buffer.sample(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8844edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def calc_values_of_states(states: np.ndarray, net: nn.Module, device: torch.device):\n",
    "    mean_vals = []\n",
    "    for batch in np.array_split(states, 64):\n",
    "        states_v = torch.tensor(batch).to(device)\n",
    "        action_values_v = net(states_v)\n",
    "        best_action_values_v = action_values_v.max(1)[0]\n",
    "        mean_vals.append(best_action_values_v.mean().item())\n",
    "    return np.mean(mean_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a06efc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_ignite(\n",
    "        engine: Engine, params: Hyperparams, exp_source: ExperienceSourceFirstLast,\n",
    "        run_name: str, extra_metrics: tt.Iterable[str] = (),\n",
    "        tuner_reward_episode: int = 100, tuner_reward_min: float = -19,\n",
    "):\n",
    "    handler = ptan_ignite.EndOfEpisodeHandler(\n",
    "        exp_source, bound_avg_reward=params.stop_reward)\n",
    "    handler.attach(engine)\n",
    "    ptan_ignite.EpisodeFPSHandler().attach(engine)\n",
    "\n",
    "    @engine.on(ptan_ignite.EpisodeEvents.EPISODE_COMPLETED)\n",
    "    def episode_completed(trainer: Engine):\n",
    "        passed = trainer.state.metrics.get('time_passed', 0)\n",
    "        print(\"Episode %d: reward=%.0f, steps=%s, speed=%.1f f/s, elapsed=%s\" % (\n",
    "            trainer.state.episode, trainer.state.episode_reward,\n",
    "            trainer.state.episode_steps, trainer.state.metrics.get('avg_fps', 0),\n",
    "            timedelta(seconds=int(passed))))\n",
    "\n",
    "    @engine.on(ptan_ignite.EpisodeEvents.BOUND_REWARD_REACHED)\n",
    "    def game_solved(trainer: Engine):\n",
    "        passed = trainer.state.metrics['time_passed']\n",
    "        print(\"Game solved in %s, after %d episodes and %d iterations!\" % (\n",
    "            timedelta(seconds=int(passed)), trainer.state.episode,\n",
    "            trainer.state.iteration))\n",
    "        trainer.should_terminate = True\n",
    "        trainer.state.solved = True\n",
    "\n",
    "    now = datetime.now().isoformat(timespec='minutes').replace(':', '')\n",
    "    logdir = f\"runs/{now}-{params.run_name}-{run_name}\"\n",
    "    tb = tb_logger.TensorboardLogger(log_dir=logdir)\n",
    "    run_avg = RunningAverage(output_transform=lambda v: v['loss'])\n",
    "    run_avg.attach(engine, \"avg_loss\")\n",
    "\n",
    "    metrics = ['reward', 'steps', 'avg_reward']\n",
    "    handler = tb_logger.OutputHandler(tag=\"episodes\", metric_names=metrics)\n",
    "    event = ptan_ignite.EpisodeEvents.EPISODE_COMPLETED\n",
    "    tb.attach(engine, log_handler=handler, event_name=event)\n",
    "\n",
    "    # write to tensorboard every 100 iterations\n",
    "    ptan_ignite.PeriodicEvents().attach(engine)\n",
    "    metrics = ['avg_loss', 'avg_fps']\n",
    "    metrics.extend(extra_metrics)\n",
    "    handler = tb_logger.OutputHandler(tag=\"train\", metric_names=metrics,\n",
    "                                      output_transform=lambda a: a)\n",
    "    event = ptan_ignite.PeriodEvents.ITERS_100_COMPLETED\n",
    "    tb.attach(engine, log_handler=handler, event_name=event)\n",
    "\n",
    "    if params.tuner_mode:\n",
    "        @engine.on(ptan_ignite.EpisodeEvents.EPISODE_COMPLETED)\n",
    "        def episode_completed(trainer: Engine):\n",
    "            avg_reward = trainer.state.metrics.get('avg_reward')\n",
    "            max_episodes = params.episodes_to_solve * 1.1\n",
    "            if trainer.state.episode > tuner_reward_episode and \\\n",
    "                    avg_reward < tuner_reward_min:\n",
    "                trainer.should_terminate = True\n",
    "                trainer.state.solved = False\n",
    "            elif trainer.state.episode > max_episodes:\n",
    "                trainer.should_terminate = True\n",
    "                trainer.state.solved = False\n",
    "            if trainer.should_terminate:\n",
    "                print(f\"Episode {trainer.state.episode}, \"\n",
    "                      f\"avg_reward {avg_reward:.2f}, terminating\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf718f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparams tuner\n",
    "TrainFunc = tt.Callable[\n",
    "    [Hyperparams, torch.device, dict],\n",
    "    tt.Optional[int]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72dd5555",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "BASE_SPACE = {\n",
    "    \"learning_rate\": tune.loguniform(1e-5, 1e-4),\n",
    "    \"gamma\": tune.choice([0.9, 0.92, 0.95, 0.98, 0.99, 0.995]),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4ed981a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune_params(\n",
    "        base_params: Hyperparams, train_func: TrainFunc, device: torch.device,\n",
    "        samples: int = 10, extra_space: tt.Optional[tt.Dict[str, tt.Any]] = None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Perform hyperparameters tune.\n",
    "    :param train_func: Train function, has to return \"episodes\" key with metric\n",
    "    :param device: torch device\n",
    "    :param samples: count of samples to perform\n",
    "    :param extra_space: additional search space\n",
    "    \"\"\"\n",
    "    search_space = dict(BASE_SPACE)\n",
    "    if extra_space is not None:\n",
    "        search_space.update(extra_space)\n",
    "    config = tune.TuneConfig(num_samples=samples)\n",
    "\n",
    "    def objective(config: dict, device: torch.device) -> dict:\n",
    "        keys = dataclasses.asdict(base_params).keys()\n",
    "        upd = {\"tuner_mode\": True}\n",
    "        for k, v in config.items():\n",
    "            if k in keys:\n",
    "                upd[k] = v\n",
    "        params = dataclasses.replace(base_params, **upd)\n",
    "        res = train_func(params, device, config)\n",
    "        return {\"episodes\": res if res is not None else 10**6}\n",
    "\n",
    "    obj = tune.with_parameters(objective, device=device)\n",
    "    if device.type == \"cuda\":\n",
    "        obj = tune.with_resources(obj, {\"gpu\": 1})\n",
    "    tuner = tune.Tuner(obj, param_space=search_space, tune_config=config)\n",
    "    results = tuner.fit()\n",
    "    best = results.get_best_result(metric=\"episodes\", mode=\"min\")\n",
    "    print(best.config)\n",
    "    print(best.metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "766d8d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "def argparser():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\n",
    "        \"--dev\", default=\"cpu\", help=\"Device to use, default=cpu\")\n",
    "    parser.add_argument(\n",
    "        \"--params\", choices=('common', 'best'), default=\"best\",\n",
    "        help=\"Params to use for training or tuning, default=best\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--tune\", type=int, help=\"Steps of params tune\")\n",
    "    return parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53e820fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_or_tune(\n",
    "        args: argparse.Namespace,\n",
    "        train_func: TrainFunc,\n",
    "        best_params: Hyperparams,\n",
    "        extra_params: tt.Optional[dict] = None,\n",
    "        extra_space: tt.Optional[dict] = None,\n",
    "):\n",
    "    random.seed(SEED)\n",
    "    torch.manual_seed(SEED)\n",
    "    device = torch.device(args.dev)\n",
    "\n",
    "    if args.params == \"common\":\n",
    "        params = GAME_PARAMS['pong']\n",
    "    else:\n",
    "        params = best_params\n",
    "\n",
    "    if extra_params is None:\n",
    "        extra_params = {}\n",
    "    if args.tune is None:\n",
    "        train_func(params, device, extra_params)\n",
    "    else:\n",
    "        tune_params(params, train_func, device, samples=args.tune,\n",
    "                    extra_space=extra_space)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
